{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Tweet2Vec:Character-Based Distributed Representations for Social Media\n",
    "\n",
    "#### Kumara Prasanna Jayaraju / Rinaldo Sonia Joseph Santhana Raj\n",
    "\n",
    "####  Emails: kjayaraju@ryerson.ca / rinaldo.joseph@ryerson.ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "#### Problem Description:\n",
    "\n",
    "Text from social media provides a set of challenges that can cause traditional NLP approaches to fail. Informal language, spelling errors, abbreviations, and special characters are all commonplace in these posts. \n",
    "\n",
    "#### Context of the Problem:\n",
    "\n",
    "This is leading to a prohibitively large vocabulary size for word-level approaches. In any natural language corpus a majority of the vocabulary word types will either be absent or occur in low frequency. Estimating the statistical properties of these rare word types is naturally a difficult task.This is analogous to the curse of dimensionality when we deal with sequences of tokens, most sequences will occur only once in the training data.\n",
    "\n",
    "#### Limitation About other Approaches:\n",
    "\n",
    "Traditional Neural Network Language Models (NNLMs) treat words as the basic units of language and assign independent vectors to each word type. This paper is motivated by bi-directional Long Short Term Memory (LSTM )for composing word vectors\n",
    "\n",
    "#### Solution:\n",
    "\n",
    "In this project, the authors explore a similar approach to learning distributed representations of social media posts by composing them from their constituent characters, with the goal of generalizing to out-of-vocabulary words as well as sequences at test time. The paper propose a character composition model, tweet2vec, which finds vector-space representations of whole tweets by learning complex, non-local dependencies in character sequences. The proposed model outperforms a word-level baseline at predicting user-annotated hashtags associated with the posts, doing significantly better when the input contains many out-of-vocabulary words or unusual character sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "| Reference |Explanation |  Dataset/Input |Weakness\n",
    "| --- | --- | --- | --- |\n",
    "| Bengio et al., 2003 [1] | Using neural networks to learn distributed representations of words dates back leased word2vec, a collection of word vectors trained using a recurrent neural network.| word2Vev, sentences, documents and paragraphs | Require storing an extremely large table of vectors for all word types and cannot be easily generalized to unseen words at test time.\n",
    "| Ling et al., 2015 [2] | the authors present a compositional character model based on bidirectional LSTMs as a potential solution to these problems.| Large data sets, sentences, paragraphs| generate word embeddings from character-level representations only, less accuracy. \n",
    "| Luong et al., 2013 [3] | Dealt with the problem of estimating rare word representations by building them from their constituent morphemes | Large data sets, sentences, paragraphs| approach requires a morpheme parser for preprocessing which may not perform well on noisy text like Twitter.\n",
    "| Dhingra et al., 2016 [4] | A character composition model, tweet2vec, which finds vector-space representations of whole tweets by learning complex, non-local dependencies in character sequences. | Feeds, Twitter| The paper is limited only to english language but can be extended for other languages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "Bi-GRU Encoder: Figure below shows our model for encoding tweets. It uses a similar structure to the C2W model in (Ling et al., 2015), with LSTM units replaced with GRU units.\n",
    "\n",
    "The input to the network is defined by an alphabet of characters C (this may include the entire unicode character set). The input tweet is broken into a stream of characters c1 , c2...cm each of which is represented by a 1-by-|C| encoding. These one-hot vectors are then projected to a character space by multiplying with the matrix PC ∈ R|C|×dc, where dc is the dimension of the character vector space.\n",
    "\n",
    "The encoder consists of a forward- GRU and a backward-GRU. Both have the same architecture, except the backward-GRU processes the sequence in reverse order. Each of the GRU units process these vectors sequentially, and start- ing with the initial state h0 compute the sequence h1, h2, ...hm. \n",
    "\n",
    "Finally, the tweet embedding is passed through a linear layer whose output is the same size as the number of hashtags L in the data set. We use a softmax layer to compute the posterior hashtag probabilities and the objective function is to optimize the categorical cross-entropy loss between predicted and true hashtags.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"> Bi-GRU Encoder </div>\n",
    "\n",
    "![Tweet2Vec_Encoder](Tweet2vec_encoder.png \"Bi-GRU Encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation:\n",
    "\n",
    "We didnt have any dataset associated with the paper due to confidentiality. So we planned to collect tweets on our own based on the following common life oriented keywords such as '#life', '#motivation', '#happy', '#emotions', '#friends', '#babies', '#dogs' to implement and test out the project. \n",
    "\n",
    "We have divided the paper into two parts: \n",
    "\n",
    "Part 1: Comparing the correctly predicted hashtags by Word Model baseline with Tweet2Vec\n",
    "\n",
    "Part 2: Training and testing the dataset to calculate Precision, Recall and Mean Rank.\n",
    "\n",
    "In this notebook, we are going show case the implemention of second part which is training and testing the dataset to compare the performace between word model baseline and tweet2Vec Encoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the neccessary libraries:\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import json\n",
    "from textblob import TextBlob, Word, Blobber\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from textblob.taggers import NLTKTagger\n",
    "from decimal import *\n",
    "import re\n",
    "import sys\n",
    "import io\n",
    "import preprocessor as p\n",
    "import numpy as np\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import sys\n",
    "import batch_word as batch\n",
    "import pickle as pkl\n",
    "import os\n",
    "from w2v import tweet2vec, load_params\n",
    "from settings_word import N_BATCH, N_WORD, MAX_CLASSES\n",
    "\n",
    "#setting up conditions for Theano:\n",
    "theano.config.gcc.cxxflags = \"-Wno-c++11-narrowing\"\n",
    "os.environ['THEANO_FLAGS'] = \"device=cpu,floatX=float32\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word baseline classifier trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Data...\n",
      "Preparing Model...\n",
      "Building network...\n",
      "Computing updates...\n",
      "Compiling theano functions...\n",
      "Training...\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Testing on Validation set...\n",
      "Epoch 29 Training Cost 5.025040453130549 Validation Precision 3.6 Regularization Cost 0.5672546625137329 Max Precision 3.6\n",
      "Seen 110 samples.\n",
      "Saving...\n",
      "Done.\n",
      "Total training time = 0.2481698989868164\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import random\n",
    "import sys\n",
    "import batch_word as batch\n",
    "import time\n",
    "import pickle as pkl\n",
    "import io\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "from collections import OrderedDict\n",
    "from w2v import tweet2vec, init_params, load_params_shared\n",
    "from settings_word import NUM_EPOCHS, N_BATCH, N_WORD, SCALE, WDIM, MAX_CLASSES, LEARNING_RATE, DISPF, SAVEF, REGULARIZATION, RELOAD_MODEL, MOMENTUM, SCHEDULE\n",
    "from evaluate_w2v import precision\n",
    "\n",
    "#setting up conditions for Theano:\n",
    "theano.config.gcc.cxxflags = \"-Wno-c++11-narrowing\"\n",
    "os.environ['THEANO_FLAGS'] = \"device=cpu,floatX=float32\"\n",
    "\n",
    "T1 = 0.01\n",
    "T2 = 0.0001\n",
    "\n",
    "train_path = \"/NLP_Final_Project/Method 2/data/train_DS.txt\"\n",
    "val_path = \"/NLP_Final_Project/Method 2/data/Val_DS.txt\"\n",
    "save_path = \"/NLP_Final_Project/Method 2/data\"\n",
    "\n",
    "def schedule(lr, mu):\n",
    "    print(\"Updating Schedule...\")\n",
    "    lr = max(1e-5,lr/2)\n",
    "    return lr, mu\n",
    "\n",
    "def tnorm(tens):\n",
    "    '''\n",
    "    Tensor Norm\n",
    "    '''\n",
    "    return T.sqrt(T.sum(T.sqr(tens),axis=1))\n",
    "\n",
    "def classify(tweet, t_mask, params, n_classes, n_tokens):\n",
    "    # tweet embedding\n",
    "    emb_layer = tweet2vec(tweet, t_mask, params, n_tokens)\n",
    "\n",
    "    # Dense layer for classes\n",
    "    l_dense = lasagne.layers.DenseLayer(emb_layer, n_classes, W=params['W_cl'], b=params['b_cl'], nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return lasagne.layers.get_output(l_dense), l_dense, lasagne.layers.get_output(emb_layer)\n",
    "\n",
    "def main(train_path,val_path,save_path,num_epochs=NUM_EPOCHS):\n",
    "    global T1\n",
    "\n",
    "    # save settings\n",
    "    shutil.copyfile('settings_word.py','%s/settings_word.txt'%save_path)\n",
    "\n",
    "    print(\"Preparing Data...\")\n",
    "    # Training data\n",
    "    Xt = []\n",
    "    yt = []\n",
    "    with io.open(train_path,'r') as f:\n",
    "        for line in f:\n",
    "            (yc, Xc) = line.rstrip('\\n').split('\\t')\n",
    "            Xt.append(Xc)\n",
    "            yt.append(yc)\n",
    "    # Validation data\n",
    "    Xv = []\n",
    "    yv = []\n",
    "    with io.open(val_path,'r') as f:\n",
    "        for line in f:\n",
    "            (yc, Xc) = line.rstrip('\\n').split('\\t')\n",
    "            Xv.append(Xc)\n",
    "            yv.append(yc.split(','))\n",
    "\n",
    "    print(\"Preparing Model...\")\n",
    "    if not RELOAD_MODEL:\n",
    "        # Build dictionaries from training data\n",
    "        tokendict, tokencount = batch.build_dictionary(Xt)\n",
    "        n_token = min(len(tokendict.keys()) + 1, N_WORD)\n",
    "        batch.save_dictionary(tokendict,tokencount,'%s/dict-nlp-w2v-p2.pkl' % save_path)\n",
    "        \n",
    "        # params\n",
    "        params = init_params(n_chars=n_token)\n",
    "        \n",
    "        labeldict, labelcount = batch.build_label_dictionary(yt)\n",
    "        batch.save_dictionary(labeldict, labelcount, '%s/label_dict-nlp-w2v-p2.pkl' % save_path)\n",
    "\n",
    "        n_classes = min(len(labeldict.keys()) + 1, MAX_CLASSES)\n",
    "\n",
    "        # classification params\n",
    "        params['W_cl'] = theano.shared(np.random.normal(loc=0., scale=SCALE, size=(WDIM,n_classes)).astype('float32'), name='W_cl')\n",
    "        params['b_cl'] = theano.shared(np.zeros((n_classes)).astype('float32'), name='b_cl')\n",
    "\n",
    "    else:\n",
    "        print(\"Loading model params...\")\n",
    "        params = load_params_shared('%s/best_model-nlp-w2v-p2.npz' % save_path)\n",
    "\n",
    "        print(\"Loading dictionaries...\")\n",
    "        with open('%s/dict-nlp-w2v-p2.pkl' % save_path, 'rb') as f:\n",
    "            tokendict = pkl.load(f)\n",
    "        with open('%s/label_dict-nlp-w2v-p2.pkl' % save_path, 'rb') as f:\n",
    "            labeldict = pkl.load(f)\n",
    "        n_token = min(len(tokendict.keys()) + 1, N_WORD)\n",
    "        n_classes = min(len(labeldict.keys()) + 1, MAX_CLASSES)\n",
    "\n",
    "    # iterators\n",
    "    train_iter = batch.BatchTweets(Xt, yt, labeldict, batch_size=N_BATCH, max_classes=MAX_CLASSES)\n",
    "    val_iter = batch.BatchTweets(Xv, yv, labeldict, batch_size=N_BATCH, max_classes=MAX_CLASSES, test=True)\n",
    "\n",
    "    print(\"Building network...\")\n",
    "    # Tweet variables\n",
    "    tweet = T.itensor3()\n",
    "    targets = T.ivector()\n",
    "\n",
    "    # masks\n",
    "    t_mask = T.fmatrix()\n",
    "\n",
    "    # network for prediction\n",
    "    predictions, net, emb = classify(tweet, t_mask, params, n_classes, n_token)\n",
    "\n",
    "    # batch loss\n",
    "    loss = lasagne.objectives.categorical_crossentropy(predictions, targets)\n",
    "    cost = T.mean(loss) + REGULARIZATION*lasagne.regularization.regularize_network_params(net, lasagne.regularization.l2)\n",
    "    cost_only = T.mean(loss)\n",
    "    reg_only = REGULARIZATION*lasagne.regularization.regularize_network_params(net, lasagne.regularization.l2)\n",
    "\n",
    "    # params and updates\n",
    "    print(\"Computing updates...\")\n",
    "    lr = LEARNING_RATE\n",
    "    mu = MOMENTUM\n",
    "    updates = lasagne.updates.nesterov_momentum(cost, lasagne.layers.get_all_params(net), lr, momentum=mu)\n",
    "\n",
    "    # Theano function\n",
    "    print(\"Compiling theano functions...\")\n",
    "    inps = [tweet,t_mask,targets]\n",
    "    predict = theano.function([tweet,t_mask],predictions)\n",
    "    encode = theano.function([tweet,t_mask],emb)\n",
    "    cost_val = theano.function(inps,[cost_only,emb])\n",
    "    train = theano.function(inps,cost,updates=updates)\n",
    "    reg_val = theano.function([],reg_only)\n",
    "\n",
    "    # Training\n",
    "    print(\"Training...\")\n",
    "    uidx = 0\n",
    "    maxp = 0.\n",
    "    start = time.time()\n",
    "    valcosts = []\n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            n_samples = 0\n",
    "            train_cost = 0.\n",
    "            print(\"Epoch {}\".format(epoch))\n",
    "\n",
    "            # learning schedule\n",
    "            if len(valcosts) > 1 and SCHEDULE:\n",
    "                change = (valcosts[-1]-valcosts[-2])/abs(valcosts[-2])\n",
    "                if change < T1:\n",
    "                    lr, mu = schedule(lr, mu)\n",
    "                    updates = lasagne.updates.nesterov_momentum(cost, lasagne.layers.get_all_params(net), lr, momentum=mu)\n",
    "                    train = theano.function(inps,cost,updates=updates)\n",
    "                    T1 = T1/2\n",
    "\n",
    "            # stopping criterion\n",
    "            if len(valcosts) > 6:\n",
    "                deltas = []\n",
    "                for i in range(5):\n",
    "                    deltas.append((valcosts[-i-1]-valcosts[-i-2])/abs(valcosts[-i-2]))\n",
    "                if sum(deltas)/len(deltas) < T2:\n",
    "                    break\n",
    "\n",
    "            ud_start = time.time()\n",
    "        for xr,y in train_iter:\n",
    "            n_samples +=len(xr)\n",
    "            uidx += 1\n",
    "            x, x_m = batch.prepare_data(xr, tokendict, n_tokens=n_token)\n",
    "            if x.any()==None:\n",
    "                print(\"Minibatch with zero samples under maxlength.\")\n",
    "                uidx -= 1\n",
    "                continue\n",
    "\n",
    "            curr_cost = train(x,x_m,y)\n",
    "            train_cost += curr_cost*len(xr)\n",
    "            ud = time.time() - ud_start\n",
    "\n",
    "            if np.isnan(curr_cost) or np.isinf(curr_cost):\n",
    "                print(\"Nan detected.\")\n",
    "                return\n",
    "\n",
    "            if np.mod(uidx, DISPF) == 0:\n",
    "                print(\"Epoch {} Update {} Cost {} Time {}\".format(epoch,uidx,curr_cost,ud))\n",
    "\n",
    "            if np.mod(uidx,SAVEF) == 0:\n",
    "                print(\"Saving...\")\n",
    "                saveparams = OrderedDict()\n",
    "                for kk,vv in params.items():\n",
    "                    saveparams[kk] = vv.get_value()\n",
    "                    np.savez('%s/model-nlp-w2v-p2.npz' % save_path,**saveparams)\n",
    "                    print(\"Done.\")\n",
    "\n",
    "        print(\"Testing on Validation set...\")\n",
    "        preds = []\n",
    "        targs = []\n",
    "        for xr,y in val_iter:\n",
    "            x, x_m = batch.prepare_data(xr, tokendict, n_tokens=n_token)\n",
    "            if x.any()==None:\n",
    "                print(\"Validation: Minibatch with zero samples under maxlength.\")\n",
    "                continue\n",
    "\n",
    "            vp = predict(x,x_m)\n",
    "            ranks = np.argsort(vp)[:,::-1]\n",
    "            for idx,item in enumerate(xr):\n",
    "                preds.append(ranks[idx,:])\n",
    "                targs.append(y[idx])\n",
    "\n",
    "        validation_cost = precision(np.asarray(preds),targs,1)\n",
    "        regularization_cost = reg_val()\n",
    "\n",
    "        if validation_cost > maxp:\n",
    "            maxp = validation_cost\n",
    "            saveparams = OrderedDict()\n",
    "            for kk,vv in params.items():\n",
    "                saveparams[kk] = vv.get_value()\n",
    "            np.savez('%s/best_model-nlp-w2v-p2.npz' % (save_path),**saveparams)\n",
    "\n",
    "        print(\"Epoch {} Training Cost {} Validation Precision {} Regularization Cost {} Max Precision {}\".format(epoch, train_cost/n_samples, validation_cost, regularization_cost, maxp))\n",
    "        print(\"Seen {} samples.\".format(n_samples))\n",
    "        valcosts.append(validation_cost)\n",
    "\n",
    "        print(\"Saving...\")\n",
    "        saveparams = OrderedDict()\n",
    "        for kk,vv in params.items():\n",
    "            saveparams[kk] = vv.get_value()\n",
    "        np.savez('%s/model-nlp-w2v-p2_%d.npz' % (save_path,epoch),**saveparams)\n",
    "        print(\"Done.\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    print(\"Total training time = {}\".format(time.time()-start))\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(train_path,val_path,save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word baseline classifier tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Data...\n",
      "Loading model params...\n",
      "Loading dictionaries...\n",
      "Building network...\n",
      "Compiling theano functions...\n",
      "Testing...\n",
      "Saving...\n",
      "Precision @ 1 = 3.4347826086956523\n",
      "Recall @ 10 = 0.42748917748917753\n",
      "Mean rank = 43\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import random\n",
    "import sys\n",
    "import batch_word as batch\n",
    "import time\n",
    "import pickle as pkl\n",
    "import io\n",
    "import os\n",
    "import evaluate_w2v as evaluate\n",
    "\n",
    "from collections import OrderedDict\n",
    "from w2v import tweet2vec, load_params\n",
    "from settings_word import N_BATCH, N_WORD, MAX_CLASSES\n",
    "\n",
    "\n",
    "#setting up conditions for Theano:\n",
    "theano.config.gcc.cxxflags = \"-Wno-c++11-narrowing\"\n",
    "os.environ['THEANO_FLAGS'] = \"device=cpu,floatX=float32\"\n",
    "\n",
    "\n",
    "data_path = \"/NLP_Final_Project/Method 2/data/test_DS.txt\"\n",
    "save_path = \"/NLP_Final_Project/Method 2/data\"\n",
    "\n",
    "\n",
    "def classify(tweet, t_mask, params, n_classes, n_chars):\n",
    "    # tweet embedding\n",
    "    emb_layer = tweet2vec(tweet, t_mask, params, n_chars)\n",
    "    # Dense layer for classes\n",
    "    l_dense = lasagne.layers.DenseLayer(emb_layer, n_classes, W=params['W_cl'], b=params['b_cl'], nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return lasagne.layers.get_output(l_dense), lasagne.layers.get_output(emb_layer)\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    if len(args)>3:\n",
    "        m_num = int(args[3])\n",
    "\n",
    "    print(\"Preparing Data...\")\n",
    "    # Test data\n",
    "    Xt = []\n",
    "    yt = []\n",
    "    with io.open(data_path,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            (yc, Xc) = line.rstrip('\\n').split('\\t')\n",
    "            Xt.append(Xc)\n",
    "            yt.append(yc.split(','))\n",
    "\n",
    "    # Model\n",
    "    print(\"Loading model params...\")\n",
    "    if len(args)>3:\n",
    "        #print('Loading %s/model_%d.npz' % (save_path,m_num))\n",
    "        params = load_params('%s/model-nlp-w2v-p2_%d.npz' % (save_path,m_num))\n",
    "    else:\n",
    "        #print('Loading %s/best_model.npz' % save_path)\n",
    "        params = load_params('%s/best_model-nlp-w2v-p2.npz' % save_path)\n",
    "\n",
    "    print(\"Loading dictionaries...\")\n",
    "    with open('%s/dict-nlp-w2v-p2.pkl' % save_path, 'rb') as f:\n",
    "        chardict = pkl.load(f)\n",
    "    with open('%s/label_dict-nlp-w2v-p2.pkl' % save_path, 'rb') as f:\n",
    "        labeldict = pkl.load(f)\n",
    "    n_char = min(len(chardict.keys()) + 1, N_WORD)\n",
    "    n_classes = min(len(labeldict.keys()) + 1, MAX_CLASSES)\n",
    "\n",
    "    # iterators\n",
    "    test_iter = batch.BatchTweets(Xt, yt, labeldict, batch_size=N_BATCH, max_classes=MAX_CLASSES, test=True)\n",
    "\n",
    "    print(\"Building network...\")\n",
    "    # Tweet variables\n",
    "    tweet = T.itensor3()\n",
    "    targets = T.imatrix()\n",
    "    # masks\n",
    "    t_mask = T.fmatrix()\n",
    "\n",
    "    # network for prediction\n",
    "    predictions, embeddings = classify(tweet, t_mask, params, n_classes, n_char)\n",
    "\n",
    "    # Theano function\n",
    "    print(\"Compiling theano functions...\")\n",
    "    predict = theano.function([tweet,t_mask],predictions)\n",
    "    encode = theano.function([tweet,t_mask],embeddings)\n",
    "\n",
    "    # Test\n",
    "    print(\"Testing...\")\n",
    "    out_data = []\n",
    "    out_pred = []\n",
    "    out_emb = []\n",
    "    out_target = []\n",
    "    for xr,y in test_iter:\n",
    "        x, x_m = batch.prepare_data(xr, chardict, n_tokens=n_char)\n",
    "        p = predict(x,x_m)\n",
    "        e = encode(x,x_m)\n",
    "        ranks = np.argsort(p)[:,::-1]\n",
    "\n",
    "        for idx, item in enumerate(xr):\n",
    "            out_data.append(item)\n",
    "            out_pred.append(ranks[idx,:])\n",
    "            out_emb.append(e[idx,:])\n",
    "            out_target.append(y[idx])\n",
    "\n",
    "    # Save\n",
    "    print(\"Saving...\")\n",
    "    with open('%s/data-nlp-w2v-p2.pkl'%save_path,'wb') as f:\n",
    "        pkl.dump(out_data,f)\n",
    "    with open('%s/predictions-nlp-w2v-p2.npy'%save_path,'wb') as f:\n",
    "        np.save(f,np.asarray(out_pred))\n",
    "    with open('%s/embeddings-nlp-w2v-p2.npy'%save_path,'wb') as f:\n",
    "        np.save(f,np.asarray(out_emb))\n",
    "    with open('%s/targets-nlp-w2v-p2.pkl'%save_path,'wb') as f:\n",
    "        pkl.dump(out_target,f)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main(sys.argv[1:])\n",
    "    evaluate.main(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tweet2Vec classifier trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Data...\n",
      "Building Model...\n",
      "OrderedDict()\n",
      "Building network...\n",
      "Computing updates...\n",
      "Compiling theano functions...\n",
      "Training...\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Testing on Validation set...\n",
      "Done\n",
      "Epoch 29 Training Cost 0.0 Validation Precision 4.0 Regularization Cost 2.498483419418335 Max Precision 4.0\n",
      "Seen 110 samples.\n",
      "Saving...\n",
      "Done\n",
      "Total training time = 0.23064899444580078\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import random\n",
    "import sys\n",
    "import batch_char as batch\n",
    "import time\n",
    "import pickle as pkl\n",
    "import io\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from collections import OrderedDict\n",
    "from t2v import tweet2vec, init_params, load_params_shared\n",
    "from settings_char import NUM_EPOCHS, N_BATCH, MAX_LENGTH, SCALE, WDIM, MAX_CLASSES, LEARNING_RATE, DISPF, SAVEF, REGULARIZATION, RELOAD_MODEL, MOMENTUM, SCHEDULE\n",
    "from evaluate_t2v import precision\n",
    "\n",
    "#setting up conditions for Theano:\n",
    "theano.config.gcc.cxxflags = \"-Wno-c++11-narrowing\"\n",
    "os.environ['THEANO_FLAGS'] = \"device=cpu,floatX=float32\"\n",
    "\n",
    "\n",
    "T1 = 0.01\n",
    "T2 = 0.0001\n",
    "\n",
    "train_path = \"/NLP_Final_Project/Method 2/data/train_DS.txt\"\n",
    "val_path = \"/NLP_Final_Project/Method 2/data/Val_DS.txt\"\n",
    "save_path = \"/NLP_Final_Project/Method 2/data\"\n",
    "\n",
    "def schedule(lr, mu):\n",
    "    print(\"Updating Schedule...\")\n",
    "    lr = max(1e-5,lr/2)\n",
    "    return lr, mu\n",
    "\n",
    "def tnorm(tens):\n",
    "    '''\n",
    "    Tensor Norm\n",
    "    '''\n",
    "    return T.sqrt(T.sum(T.sqr(tens),axis=1))\n",
    "\n",
    "def classify(tweet, t_mask, params, n_classes, n_chars):\n",
    "    # tweet embedding\n",
    "    emb_layer = tweet2vec(tweet, t_mask, params, n_chars)\n",
    "    # Dense layer for classes\n",
    "    l_dense = lasagne.layers.DenseLayer(emb_layer, n_classes, W=params['W_cl'], b=params['b_cl'], nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return lasagne.layers.get_output(l_dense), l_dense, lasagne.layers.get_output(emb_layer)\n",
    "\n",
    "def main(train_path,val_path,save_path,num_epochs=NUM_EPOCHS):\n",
    "    global T1\n",
    "\n",
    "    # save settings\n",
    "    shutil.copyfile('settings_char.py','%s/settings_char.txt'%save_path)\n",
    "\n",
    "    print(\"Preparing Data...\")\n",
    "    # Training data\n",
    "    Xt = []\n",
    "    yt = []\n",
    "    with io.open(train_path,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            (yc, Xc) = line.rstrip('\\n').split('\\t')\n",
    "            Xt.append(Xc[:MAX_LENGTH])\n",
    "            yt.append(yc)\n",
    "    # Validation data\n",
    "    Xv = []\n",
    "    yv = []\n",
    "    with io.open(val_path,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            (yc, Xc) = line.rstrip('\\n').split('\\t')\n",
    "            Xv.append(Xc[:MAX_LENGTH])\n",
    "            yv.append(yc.split(','))\n",
    "\n",
    "    print(\"Building Model...\")\n",
    "    if not RELOAD_MODEL:\n",
    "        # Build dictionaries from training data\n",
    "        chardict, charcount = batch.build_dictionary(Xt)\n",
    "        n_char = len(chardict.keys()) + 1\n",
    "        batch.save_dictionary(chardict,charcount,'%s/dict-nlp-t2v-p2.pkl' % save_path)\n",
    "        \n",
    "        # params\n",
    "        params = init_params(n_chars=n_char)\n",
    "        \n",
    "        labeldict, labelcount = batch.build_label_dictionary(yt)\n",
    "        batch.save_dictionary(labeldict, labelcount, '%s/label_dict-nlp-t2v-p2.pkl' % save_path)\n",
    "\n",
    "        n_classes = min(len(labeldict.keys()) + 1, MAX_CLASSES)\n",
    "\n",
    "        # classification params\n",
    "        params['W_cl'] = theano.shared(np.random.normal(loc=0., scale=SCALE, size=(WDIM,n_classes)).astype('float32'), name='W_cl')\n",
    "        params['b_cl'] = theano.shared(np.zeros((n_classes)).astype('float32'), name='b_cl')\n",
    "\n",
    "    else:\n",
    "        print(\"Loading model params...\")\n",
    "        params = load_params_shared('%s/model-nlp-t2v-p2.npz' % save_path)\n",
    "\n",
    "        print(\"Loading dictionaries...\")\n",
    "        with open('%s/dict-nlp-t2v-p2.pkl' % save_path, 'rb') as f:\n",
    "            chardict = pkl.load(f)\n",
    "        with open('%s/label_dict-nlp-t2v-p2.pkl' % save_path, 'rb') as f:\n",
    "            labeldict = pkl.load(f)\n",
    "        n_char = len(chardict.keys()) + 1\n",
    "        n_classes = min(len(labeldict.keys()) + 1, MAX_CLASSES)\n",
    "\n",
    "    # iterators\n",
    "    train_iter = batch.BatchTweets(Xt, yt, labeldict, batch_size=N_BATCH, max_classes=MAX_CLASSES)\n",
    "    val_iter = batch.BatchTweets(Xv, yv, labeldict, batch_size=N_BATCH, max_classes=MAX_CLASSES, test=True)\n",
    "\n",
    "    print(\"Building network...\")\n",
    "    # Tweet variables\n",
    "    tweet = T.itensor3()\n",
    "    targets = T.ivector()\n",
    "    \n",
    "    # masks\n",
    "    t_mask = T.fmatrix()\n",
    "\n",
    "    # network for prediction\n",
    "    predictions, net, emb = classify(tweet, t_mask, params, n_classes, n_char)\n",
    "\n",
    "    # batch loss\n",
    "    loss = lasagne.objectives.categorical_crossentropy(predictions, targets)\n",
    "    cost = T.mean(loss) + REGULARIZATION*lasagne.regularization.regularize_network_params(net, lasagne.regularization.l2)\n",
    "    cost_only = T.mean(loss)\n",
    "    reg_only = REGULARIZATION*lasagne.regularization.regularize_network_params(net, lasagne.regularization.l2)\n",
    "\n",
    "    # params and updates\n",
    "    print(\"Computing updates...\")\n",
    "    lr = LEARNING_RATE\n",
    "    mu = MOMENTUM\n",
    "    updates = lasagne.updates.nesterov_momentum(cost, lasagne.layers.get_all_params(net), lr, momentum=mu)\n",
    "\n",
    "    # Theano function\n",
    "    print(\"Compiling theano functions...\")\n",
    "    inps = [tweet,t_mask,targets]\n",
    "    predict = theano.function([tweet,t_mask],predictions)\n",
    "    cost_val = theano.function(inps,[cost_only,emb])\n",
    "    train = theano.function(inps,cost,updates=updates)\n",
    "    reg_val = theano.function([],reg_only)\n",
    "\n",
    "    # Training\n",
    "    print(\"Training...\")\n",
    "    uidx = 0\n",
    "    maxp = 0.\n",
    "    start = time.time()\n",
    "    valcosts = []\n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            n_samples = 0\n",
    "            train_cost = 0.\n",
    "            print(\"Epoch {}\".format(epoch))\n",
    "\n",
    "            # learning schedule\n",
    "            if len(valcosts) > 1 and SCHEDULE:\n",
    "                change = (valcosts[-1]-valcosts[-2])/abs(valcosts[-2])\n",
    "                if change < T1:\n",
    "                    lr, mu = schedule(lr, mu)\n",
    "                    updates = lasagne.updates.nesterov_momentum(cost, lasagne.layers.get_all_params(net), lr, momentum=mu)\n",
    "                    train = theano.function(inps,cost,updates=updates)\n",
    "                    T1 = T1/2\n",
    "\n",
    "            # stopping criterion\n",
    "            if len(valcosts) > 6:\n",
    "                deltas = []\n",
    "                for i in range(5):\n",
    "                    deltas.append((valcosts[-i-1]-valcosts[-i-2])/abs(valcosts[-i-2]))\n",
    "                if sum(deltas)/len(deltas) < T2:\n",
    "                    break\n",
    "\n",
    "            ud_start = time.time()\n",
    "        for xr,y in train_iter:\n",
    "            n_samples +=len(xr)\n",
    "            uidx += 1\n",
    "            x, x_m = batch.prepare_data(xr, chardict, n_chars=n_char)\n",
    "            if x is None:\n",
    "                print(\"Minibatch with zero samples under maxlength.\")\n",
    "                uidx -= 1\n",
    "                continue\n",
    "\n",
    "                curr_cost = train(x,x_m,y)\n",
    "                train_cost += curr_cost*len(xr)\n",
    "                ud = time.time() - ud_start\n",
    "\n",
    "                if np.isnan(curr_cost) or np.isinf(curr_cost):\n",
    "                    print(\"Nan detected.\")\n",
    "                    return\n",
    "\n",
    "                if np.mod(uidx, DISPF) == 0:\n",
    "                    print(\"Epoch {} Update {} Cost {} Time {}\".format(epoch,uidx,curr_cost,ud))\n",
    "\n",
    "                if np.mod(uidx,SAVEF) == 0:\n",
    "                    print(\"Saving...\")\n",
    "                    saveparams = OrderedDict()\n",
    "                    for kk,vv in params.items():\n",
    "                        saveparams[kk] = vv.get_value()\n",
    "                    np.savez('%s/model-nlp-t2v-p2.npz' % save_path,**saveparams)\n",
    "                    print(\"Done.\")\n",
    "\n",
    "        print(\"Testing on Validation set...\")\n",
    "        preds = []\n",
    "        targs = []\n",
    "        for xr,y in val_iter:\n",
    "            x, x_m = batch.prepare_data(xr, chardict, n_chars=n_char)\n",
    "            if x is None:\n",
    "                print(\"Validation: Minibatch with zero samples under maxlength.\")\n",
    "                continue\n",
    "\n",
    "            vp = predict(x,x_m)\n",
    "            ranks = np.argsort(vp)[:,::-1]\n",
    "            for idx,item in enumerate(xr):\n",
    "                preds.append(ranks[idx,:])\n",
    "                targs.append(y[idx])\n",
    "\n",
    "        validation_cost = precision(np.asarray(preds),targs,1)\n",
    "        regularization_cost = reg_val()\n",
    "\n",
    "        if validation_cost > maxp:\n",
    "            maxp = validation_cost\n",
    "            saveparams = OrderedDict()\n",
    "            for kk,vv in params.items():\n",
    "                saveparams[kk] = vv.get_value()\n",
    "            np.savez('%s/best_model-nlp-t2v-p2.npz' % (save_path),**saveparams)\n",
    "            print(\"Done\")\n",
    "\n",
    "        print(\"Epoch {} Training Cost {} Validation Precision {} Regularization Cost {} Max Precision {}\".format(epoch, train_cost/n_samples, validation_cost, regularization_cost, maxp))\n",
    "        print(\"Seen {} samples.\".format(n_samples))\n",
    "        valcosts.append(validation_cost)\n",
    "\n",
    "        print(\"Saving...\")\n",
    "        saveparams = OrderedDict()\n",
    "        for kk,vv in params.items():\n",
    "            saveparams[kk] = vv.get_value()\n",
    "        np.savez('%s/model-nlp-t2v-p2_%d.npz' % (save_path,epoch),**saveparams)\n",
    "        print(\"Done\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    print(\"Total training time = {}\".format(time.time()-start))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main(train_path,val_path,save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tweet2Vec classifier trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Data...\n",
      "Loading model params...\n",
      "Loading best_model\n",
      "Loading dictionaries...\n",
      "Building network...\n",
      "Compiling theano functions...\n",
      "Testing...\n",
      "Saving...\n",
      "Precision @ 1 = 3.590909090909091\n",
      "Recall @ 10 = 0.42748917748917753\n",
      "Mean rank = 26\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import random\n",
    "import sys\n",
    "import batch_char as batch\n",
    "import time\n",
    "import pickle as pkl\n",
    "import io\n",
    "import os\n",
    "import evaluate_t2v as evaluate\n",
    "\n",
    "from collections import OrderedDict\n",
    "from t2v import tweet2vec, init_params, load_params\n",
    "from settings_char import N_BATCH, MAX_LENGTH, MAX_CLASSES\n",
    "\n",
    "#setting up conditions for Theano:\n",
    "theano.config.gcc.cxxflags = \"-Wno-c++11-narrowing\"\n",
    "os.environ['THEANO_FLAGS'] = \"device=cpu,floatX=float32\"\n",
    "\n",
    "data_path = \"/NLP_Final_Project/Method 2/data/test_DS.txt\"\n",
    "save_path = \"/NLP_Final_Project/Method 2/data\"\n",
    "\n",
    "def classify(tweet, t_mask, params, n_classes, n_chars):\n",
    "    # tweet embedding\n",
    "    emb_layer = tweet2vec(tweet, t_mask, params, n_chars)\n",
    "    # Dense layer for classes\n",
    "    l_dense = lasagne.layers.DenseLayer(emb_layer, n_classes, W=params['W_cl'], b=params['b_cl'], nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return lasagne.layers.get_output(l_dense), lasagne.layers.get_output(emb_layer)\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    if len(args)>3:\n",
    "        m_num = int(args[3])\n",
    "\n",
    "    print(\"Preparing Data...\")\n",
    "    # Test data\n",
    "    Xt = []\n",
    "    yt = []\n",
    "    with io.open(data_path,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            (yc, Xc) = line.rstrip('\\n').split('\\t')\n",
    "            Xt.append(Xc[:MAX_LENGTH])\n",
    "            yt.append(yc.split(','))\n",
    "\n",
    "    # Model\n",
    "    print(\"Loading model params...\")\n",
    "    if len(args)>3:\n",
    "        print(\"model\")\n",
    "        params = load_params('%s/model-nlp-t2v-p2_%d.npz' % (save_path,m_num))\n",
    "    else:\n",
    "        print(\"Loading best_model\")\n",
    "        params = load_params('%s/best_model-nlp-t2v-p2.npz' % save_path)\n",
    "\n",
    "    print(\"Loading dictionaries...\")\n",
    "    with open('%s/dict-nlp-t2v-p2.pkl' % save_path, 'rb') as f:\n",
    "        chardict = pkl.load(f)\n",
    "    with open('%s/label_dict-nlp-t2v-p2.pkl' % save_path, 'rb') as f:\n",
    "        labeldict = pkl.load(f)\n",
    "    n_char = len(chardict.keys()) + 1\n",
    "    n_classes = min(len(labeldict.keys()) + 1, MAX_CLASSES)\n",
    "\n",
    "    # iterators\n",
    "    test_iter = batch.BatchTweets(Xt, yt, labeldict, batch_size=N_BATCH, max_classes=MAX_CLASSES, test=True)\n",
    "\n",
    "    print(\"Building network...\")\n",
    "    # Tweet variables\n",
    "    tweet = T.itensor3()\n",
    "    targets = T.imatrix()\n",
    "\n",
    "    # masks\n",
    "    t_mask = T.fmatrix()\n",
    "\n",
    "    predictions, embeddings = classify(tweet, t_mask, params, n_classes, n_char)\n",
    "\n",
    "    # Theano function\n",
    "    print(\"Compiling theano functions...\")\n",
    "    predict = theano.function([tweet,t_mask],predictions)\n",
    "    encode = theano.function([tweet,t_mask],embeddings)\n",
    "\n",
    "    # Test\n",
    "    print(\"Testing...\")\n",
    "    out_data = []\n",
    "    out_pred = []\n",
    "    out_emb = []\n",
    "    out_target = []\n",
    "    for xr,y in test_iter:\n",
    "        x, x_m = batch.prepare_data(xr, chardict, n_chars=n_char)\n",
    "        p = predict(x,x_m)\n",
    "        e = encode(x,x_m)\n",
    "        ranks = np.argsort(p)[:,::-1]\n",
    "\n",
    "        for idx, item in enumerate(xr):\n",
    "            out_data.append(item)\n",
    "            out_pred.append(ranks[idx,:])\n",
    "            out_emb.append(e[idx,:])\n",
    "            out_target.append(y[idx])\n",
    "\n",
    "    # Save\n",
    "    print(\"Saving...\")\n",
    "    with open('%s/data-nlp-t2v-p2.pkl'%save_path,'wb') as f:\n",
    "        pkl.dump(out_data,f)\n",
    "    with open('%s/predictions-nlp-t2v-p2.npy'%save_path,'wb') as f:\n",
    "        np.save(f,np.asarray(out_pred))\n",
    "    with open('%s/embeddings-nlp-t2v-p2.npy'%save_path,'wb') as f:\n",
    "        np.save(f,np.asarray(out_emb))\n",
    "    with open('%s/targets-nlp-t2v-p2.pkl'%save_path,'wb') as f:\n",
    "        pkl.dump(out_target,f)\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(sys.argv[1:])\n",
    "    evaluate.main(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performace output:\n",
    "\n",
    "\n",
    "| Model |Precision @1 |  Recall @10 |MeanRank\n",
    "| --- | --- | --- | --- |\n",
    "| Word Level Baseline | 34.34% | 42.74% | 43 |\n",
    "| Tweet2Vec | 35.90% | 42.74% | 26 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Future Direction\n",
    "\n",
    "Our learning from this project is that, tweet2vec encoder performs better than word baseline for social media posts trained using supervision from associated hashtags. However, based on our observation there were few tweets were words baseline had better prediction of hastags as well. With respect to performance, without doubt tweet2vec outperforms the word baseline. This paper was limited to English language however the model can be extended to other languages as well. Future direction of the project will focus on how the model can be used for domains specific classification such as news feeds, social media and any content based platforms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "[1]: [Dhingra1 et al.2016] Bhuwan Dhingra1, Zhong Zhou2, Dylan Fitzpatrick1,2\n",
    "Michael Muehl1 and William W. Cohen1, Tweet2Vec: Character-Based Distributed Representations for Social Media, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),2016\n",
    "\n",
    "\n",
    "[2]: [Bengio et al.2003] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neu- ral probabilistic language model. The Journal of Machine Learning Research, 3:1137–1155.\n",
    "\n",
    "[3]: [Godin et al.2013] Fréderic Godin, Viktor Slavkovikj, Wesley De Neve, Benjamin Schrauwen, and Rik Van de Walle. 2013. Using topic models for twit- ter hashtag recommendation. In Proceedings of the 22nd international conference on World Wide Web companion, pages 593–596. International World Wide Web Conferences Steering Committee.\n",
    "\n",
    "[4]: [Zhangetal.2015] XiangZhang,JunboZhao,andYann LeCun. 2015. Character-level convolutional net- works for text classification. In Advances in Neural Information Processing Systems, pages 649–657."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
