{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unknown-kelly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Data...\n",
      "Building Model...\n",
      "OrderedDict()\n",
      "Building network...\n",
      "Computing updates...\n",
      "Compiling theano functions...\n",
      "Training...\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Testing on Validation set...\n",
      "Epoch 29 Training Cost 0.0 Validation Precision 4.0 Regularization Cost 2.498483419418335 Max Precision 4.0\n",
      "Seen 110 samples.\n",
      "Saving...\n",
      "Done\n",
      "Total training time = 0.1954360008239746\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Tweet2Vec classifier trainer\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import random\n",
    "import sys\n",
    "import batch_char as batch\n",
    "import time\n",
    "import pickle as pkl\n",
    "import io\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from collections import OrderedDict\n",
    "from t2v import tweet2vec, init_params, load_params_shared\n",
    "from settings_char import NUM_EPOCHS, N_BATCH, MAX_LENGTH, SCALE, WDIM, MAX_CLASSES, LEARNING_RATE, DISPF, SAVEF, REGULARIZATION, RELOAD_MODEL, MOMENTUM, SCHEDULE\n",
    "from evaluate_t2v import precision\n",
    "\n",
    "#setting up conditions for Theano:\n",
    "theano.config.gcc.cxxflags = \"-Wno-c++11-narrowing\"\n",
    "os.environ['THEANO_FLAGS'] = \"device=cpu,floatX=float32\"\n",
    "\n",
    "\n",
    "T1 = 0.01\n",
    "T2 = 0.0001\n",
    "\n",
    "train_path = \"/users/kumaraprasannajayaraju/Downloads/NLP_Final_Project/Method 2/data/train_DS.txt\"\n",
    "val_path = \"/users/kumaraprasannajayaraju/Downloads/NLP_Final_Project/Method 2/data/Val_DS.txt\"\n",
    "save_path = \"/users/kumaraprasannajayaraju/Downloads/NLP_Final_Project/Method 2/data\"\n",
    "\n",
    "def schedule(lr, mu):\n",
    "    print(\"Updating Schedule...\")\n",
    "    lr = max(1e-5,lr/2)\n",
    "    return lr, mu\n",
    "\n",
    "def tnorm(tens):\n",
    "    '''\n",
    "    Tensor Norm\n",
    "    '''\n",
    "    return T.sqrt(T.sum(T.sqr(tens),axis=1))\n",
    "\n",
    "def classify(tweet, t_mask, params, n_classes, n_chars):\n",
    "    # tweet embedding\n",
    "    emb_layer = tweet2vec(tweet, t_mask, params, n_chars)\n",
    "    # Dense layer for classes\n",
    "    l_dense = lasagne.layers.DenseLayer(emb_layer, n_classes, W=params['W_cl'], b=params['b_cl'], nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return lasagne.layers.get_output(l_dense), l_dense, lasagne.layers.get_output(emb_layer)\n",
    "\n",
    "def main(train_path,val_path,save_path,num_epochs=NUM_EPOCHS):\n",
    "    global T1\n",
    "\n",
    "    # save settings\n",
    "    shutil.copyfile('settings_char.py','%s/settings_char.txt'%save_path)\n",
    "\n",
    "    print(\"Preparing Data...\")\n",
    "    # Training data\n",
    "    Xt = []\n",
    "    yt = []\n",
    "    with io.open(train_path,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            (yc, Xc) = line.rstrip('\\n').split('\\t')\n",
    "            Xt.append(Xc[:MAX_LENGTH])\n",
    "            yt.append(yc)\n",
    "    # Validation data\n",
    "    Xv = []\n",
    "    yv = []\n",
    "    with io.open(val_path,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            (yc, Xc) = line.rstrip('\\n').split('\\t')\n",
    "            Xv.append(Xc[:MAX_LENGTH])\n",
    "            yv.append(yc.split(','))\n",
    "\n",
    "    print(\"Building Model...\")\n",
    "    if not RELOAD_MODEL:\n",
    "        #print(\"inside reload model\")\n",
    "        # Build dictionaries from training data\n",
    "        chardict, charcount = batch.build_dictionary(Xt)\n",
    "        n_char = len(chardict.keys()) + 1\n",
    "        #print(\"n_char\", n_char)\n",
    "        batch.save_dictionary(chardict,charcount,'%s/dict-nlp-t2v-p2.pkl' % save_path)\n",
    "        # params\n",
    "        params = init_params(n_chars=n_char)\n",
    "        \n",
    "        labeldict, labelcount = batch.build_label_dictionary(yt)\n",
    "        batch.save_dictionary(labeldict, labelcount, '%s/label_dict-nlp-t2v-p2.pkl' % save_path)\n",
    "\n",
    "        n_classes = min(len(labeldict.keys()) + 1, MAX_CLASSES)\n",
    "\n",
    "        # classification params\n",
    "        params['W_cl'] = theano.shared(np.random.normal(loc=0., scale=SCALE, size=(WDIM,n_classes)).astype('float32'), name='W_cl')\n",
    "        params['b_cl'] = theano.shared(np.zeros((n_classes)).astype('float32'), name='b_cl')\n",
    "\n",
    "    else:\n",
    "        print(\"Loading model params...\")\n",
    "        params = load_params_shared('%s/model-nlp-t2v-p2.npz' % save_path)\n",
    "\n",
    "        print(\"Loading dictionaries...\")\n",
    "        with open('%s/dict-nlp-t2v-p2.pkl' % save_path, 'rb') as f:\n",
    "            chardict = pkl.load(f)\n",
    "        with open('%s/label_dict-nlp-t2v-p2.pkl' % save_path, 'rb') as f:\n",
    "            labeldict = pkl.load(f)\n",
    "        n_char = len(chardict.keys()) + 1\n",
    "        n_classes = min(len(labeldict.keys()) + 1, MAX_CLASSES)\n",
    "\n",
    "    # iterators\n",
    "    train_iter = batch.BatchTweets(Xt, yt, labeldict, batch_size=N_BATCH, max_classes=MAX_CLASSES)\n",
    "    val_iter = batch.BatchTweets(Xv, yv, labeldict, batch_size=N_BATCH, max_classes=MAX_CLASSES, test=True)\n",
    "\n",
    "    print(\"Building network...\")\n",
    "    # Tweet variables\n",
    "    tweet = T.itensor3()\n",
    "    targets = T.ivector()\n",
    "    # masks\n",
    "    t_mask = T.fmatrix()\n",
    "\n",
    "    # network for prediction\n",
    "    predictions, net, emb = classify(tweet, t_mask, params, n_classes, n_char)\n",
    "\n",
    "    # batch loss\n",
    "    loss = lasagne.objectives.categorical_crossentropy(predictions, targets)\n",
    "    cost = T.mean(loss) + REGULARIZATION*lasagne.regularization.regularize_network_params(net, lasagne.regularization.l2)\n",
    "    cost_only = T.mean(loss)\n",
    "    reg_only = REGULARIZATION*lasagne.regularization.regularize_network_params(net, lasagne.regularization.l2)\n",
    "\n",
    "    # params and updates\n",
    "    print(\"Computing updates...\")\n",
    "    lr = LEARNING_RATE\n",
    "    mu = MOMENTUM\n",
    "    updates = lasagne.updates.nesterov_momentum(cost, lasagne.layers.get_all_params(net), lr, momentum=mu)\n",
    "\n",
    "    # Theano function\n",
    "    print(\"Compiling theano functions...\")\n",
    "    inps = [tweet,t_mask,targets]\n",
    "    predict = theano.function([tweet,t_mask],predictions)\n",
    "    cost_val = theano.function(inps,[cost_only,emb])\n",
    "    train = theano.function(inps,cost,updates=updates)\n",
    "    reg_val = theano.function([],reg_only)\n",
    "\n",
    "    # Training\n",
    "    print(\"Training...\")\n",
    "    uidx = 0\n",
    "    maxp = 0.\n",
    "    start = time.time()\n",
    "    valcosts = []\n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            n_samples = 0\n",
    "            train_cost = 0.\n",
    "            print(\"Epoch {}\".format(epoch))\n",
    "\n",
    "            # learning schedule\n",
    "            if len(valcosts) > 1 and SCHEDULE:\n",
    "                change = (valcosts[-1]-valcosts[-2])/abs(valcosts[-2])\n",
    "                if change < T1:\n",
    "                    lr, mu = schedule(lr, mu)\n",
    "                    updates = lasagne.updates.nesterov_momentum(cost, lasagne.layers.get_all_params(net), lr, momentum=mu)\n",
    "                    train = theano.function(inps,cost,updates=updates)\n",
    "                    T1 = T1/2\n",
    "\n",
    "            # stopping criterion\n",
    "            if len(valcosts) > 6:\n",
    "                deltas = []\n",
    "                for i in range(5):\n",
    "                    deltas.append((valcosts[-i-1]-valcosts[-i-2])/abs(valcosts[-i-2]))\n",
    "                if sum(deltas)/len(deltas) < T2:\n",
    "                    break\n",
    "\n",
    "            ud_start = time.time()\n",
    "        for xr,y in train_iter:\n",
    "            n_samples +=len(xr)\n",
    "            uidx += 1\n",
    "            x, x_m = batch.prepare_data(xr, chardict, n_chars=n_char)\n",
    "            if x is None:\n",
    "                print(\"Minibatch with zero samples under maxlength.\")\n",
    "                uidx -= 1\n",
    "                continue\n",
    "\n",
    "                curr_cost = train(x,x_m,y)\n",
    "                train_cost += curr_cost*len(xr)\n",
    "                ud = time.time() - ud_start\n",
    "\n",
    "                if np.isnan(curr_cost) or np.isinf(curr_cost):\n",
    "                    print(\"Nan detected.\")\n",
    "                    return\n",
    "\n",
    "                if np.mod(uidx, DISPF) == 0:\n",
    "                    print(\"Epoch {} Update {} Cost {} Time {}\".format(epoch,uidx,curr_cost,ud))\n",
    "\n",
    "                if np.mod(uidx,SAVEF) == 0:\n",
    "                    print(\"Saving...\")\n",
    "                    saveparams = OrderedDict()\n",
    "                    for kk,vv in params.items():\n",
    "                        saveparams[kk] = vv.get_value()\n",
    "                    np.savez('%s/model-nlp-t2v-p2.npz' % save_path,**saveparams)\n",
    "                    #print(\"Done. for model.npz\")\n",
    "\n",
    "        print(\"Testing on Validation set...\")\n",
    "        preds = []\n",
    "        targs = []\n",
    "        for xr,y in val_iter:\n",
    "            x, x_m = batch.prepare_data(xr, chardict, n_chars=n_char)\n",
    "            if x is None:\n",
    "                print(\"Validation: Minibatch with zero samples under maxlength.\")\n",
    "                continue\n",
    "\n",
    "            vp = predict(x,x_m)\n",
    "            ranks = np.argsort(vp)[:,::-1]\n",
    "            for idx,item in enumerate(xr):\n",
    "                preds.append(ranks[idx,:])\n",
    "                targs.append(y[idx])\n",
    "\n",
    "        validation_cost = precision(np.asarray(preds),targs,1)\n",
    "        regularization_cost = reg_val()\n",
    "\n",
    "        if validation_cost > maxp:\n",
    "            maxp = validation_cost\n",
    "            saveparams = OrderedDict()\n",
    "            for kk,vv in params.items():\n",
    "                saveparams[kk] = vv.get_value()\n",
    "            np.savez('%s/best_model-nlp-t2v-p2.npz' % (save_path),**saveparams)\n",
    "            #print(\"Done for best_model\")\n",
    "\n",
    "        print(\"Epoch {} Training Cost {} Validation Precision {} Regularization Cost {} Max Precision {}\".format(epoch, train_cost/n_samples, validation_cost, regularization_cost, maxp))\n",
    "        print(\"Seen {} samples.\".format(n_samples))\n",
    "        valcosts.append(validation_cost)\n",
    "\n",
    "        print(\"Saving...\")\n",
    "        saveparams = OrderedDict()\n",
    "        for kk,vv in params.items():\n",
    "            saveparams[kk] = vv.get_value()\n",
    "        np.savez('%s/model-nlp-t2v-p2_%d.npz' % (save_path,epoch),**saveparams)\n",
    "        #print(\"%s/model-nlp-t2v-p2_%d.npz\" % (save_path,epoch))\n",
    "        print(\"Done\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    print(\"Total training time = {}\".format(time.time()-start))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main(train_path,val_path,save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-craps",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
