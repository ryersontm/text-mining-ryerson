{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-cheese",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import random\n",
    "import sys\n",
    "import batch_word as batch\n",
    "import time\n",
    "import pickle as pkl\n",
    "import io\n",
    "import os\n",
    "import evaluate_w2v as evaluate\n",
    "\n",
    "from collections import OrderedDict\n",
    "from w2v import tweet2vec, load_params\n",
    "from settings_word import N_BATCH, N_WORD, MAX_CLASSES\n",
    "\n",
    "\n",
    "#setting up conditions for Theano:\n",
    "theano.config.gcc.cxxflags = \"-Wno-c++11-narrowing\"\n",
    "os.environ['THEANO_FLAGS'] = \"device=cpu,floatX=float32\"\n",
    "\n",
    "\n",
    "data_path = \"/users/kumaraprasannajayaraju/Downloads/NLP_Final_Project/Method 2/data/test_DS.txt\"\n",
    "save_path = \"/users/kumaraprasannajayaraju/Downloads/NLP_Final_Project/Method 2/data\"\n",
    "\n",
    "\n",
    "def classify(tweet, t_mask, params, n_classes, n_chars):\n",
    "    # tweet embedding\n",
    "    emb_layer = tweet2vec(tweet, t_mask, params, n_chars)\n",
    "    # Dense layer for classes\n",
    "    l_dense = lasagne.layers.DenseLayer(emb_layer, n_classes, W=params['W_cl'], b=params['b_cl'], nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return lasagne.layers.get_output(l_dense), lasagne.layers.get_output(emb_layer)\n",
    "\n",
    "def main(args):\n",
    "    \n",
    "#     data_path = \"/users/kumaraprasannajayaraju/Downloads/test_DS.txt\"\n",
    "#     model_path = \"/users/kumaraprasannajayaraju\"\n",
    "#     save_path = \"/users/kumaraprasannajayaraju/Downloads/\"\n",
    "\n",
    "#     data_path = args[0]\n",
    "#     model_path = args[1]\n",
    "#     save_path = args[2]\n",
    "    if len(args)>3:\n",
    "        m_num = int(args[3])\n",
    "\n",
    "    print(\"Preparing Data...\")\n",
    "    # Test data\n",
    "    Xt = []\n",
    "    yt = []\n",
    "    with io.open(data_path,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            (yc, Xc) = line.rstrip('\\n').split('\\t')\n",
    "            Xt.append(Xc)\n",
    "            yt.append(yc.split(','))\n",
    "\n",
    "    # Model\n",
    "    print(\"Loading model params...\")\n",
    "    if len(args)>3:\n",
    "        #print('Loading %s/model_%d.npz' % (model_path,m_num))\n",
    "        params = load_params('%s/model-nlp-w2v-p2_%d.npz' % (save_path,m_num))\n",
    "    else:\n",
    "        #print('Loading %s/best_model.npz' % model_path)\n",
    "        params = load_params('%s/best_model-nlp-w2v-p2.npz' % save_path)\n",
    "\n",
    "    print(\"Loading dictionaries...\")\n",
    "    with open('%s/dict-nlp-w2v-p2.pkl' % save_path, 'rb') as f:\n",
    "        chardict = pkl.load(f)\n",
    "    with open('%s/label_dict-nlp-w2v-p2.pkl' % save_path, 'rb') as f:\n",
    "        labeldict = pkl.load(f)\n",
    "    n_char = min(len(chardict.keys()) + 1, N_WORD)\n",
    "    n_classes = min(len(labeldict.keys()) + 1, MAX_CLASSES)\n",
    "\n",
    "    # iterators\n",
    "    test_iter = batch.BatchTweets(Xt, yt, labeldict, batch_size=N_BATCH, max_classes=MAX_CLASSES, test=True)\n",
    "\n",
    "    print(\"Building network...\")\n",
    "    # Tweet variables\n",
    "    tweet = T.itensor3()\n",
    "    targets = T.imatrix()\n",
    "    # masks\n",
    "    t_mask = T.fmatrix()\n",
    "\n",
    "    # network for prediction\n",
    "    predictions, embeddings = classify(tweet, t_mask, params, n_classes, n_char)\n",
    "\n",
    "    # Theano function\n",
    "    print(\"Compiling theano functions...\")\n",
    "    predict = theano.function([tweet,t_mask],predictions)\n",
    "    encode = theano.function([tweet,t_mask],embeddings)\n",
    "\n",
    "    # Test\n",
    "    print(\"Testing...\")\n",
    "    out_data = []\n",
    "    out_pred = []\n",
    "    out_emb = []\n",
    "    out_target = []\n",
    "    for xr,y in test_iter:\n",
    "        x, x_m = batch.prepare_data(xr, chardict, n_tokens=n_char)\n",
    "        p = predict(x,x_m)\n",
    "        e = encode(x,x_m)\n",
    "        ranks = np.argsort(p)[:,::-1]\n",
    "\n",
    "        for idx, item in enumerate(xr):\n",
    "            out_data.append(item)\n",
    "            out_pred.append(ranks[idx,:])\n",
    "            out_emb.append(e[idx,:])\n",
    "            out_target.append(y[idx])\n",
    "\n",
    "    # Save\n",
    "    print(\"Saving...\")\n",
    "    with open('%s/data-nlp-w2v-p2.pkl'%save_path,'wb') as f:\n",
    "        pkl.dump(out_data,f)\n",
    "    with open('%s/predictions-nlp-w2v-p2.npy'%save_path,'wb') as f:\n",
    "        np.save(f,np.asarray(out_pred))\n",
    "    with open('%s/embeddings-nlp-w2v-p2.npy'%save_path,'wb') as f:\n",
    "        np.save(f,np.asarray(out_emb))\n",
    "    with open('%s/targets-nlp-w2v-p2.pkl'%save_path,'wb') as f:\n",
    "        pkl.dump(out_target,f)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main(sys.argv[1:])\n",
    "    evaluate.main(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-discovery",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
