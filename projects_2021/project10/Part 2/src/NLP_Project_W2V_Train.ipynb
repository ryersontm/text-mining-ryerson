{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "enormous-symposium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Data...\n",
      "Preparing Model...\n",
      "Building network...\n",
      "Computing updates...\n",
      "Compiling theano functions...\n",
      "Training...\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Testing on Validation set...\n",
      "Epoch 29 Training Cost 5.0215349197387695 Validation Precision 4.0 Regularization Cost 0.5672622919082642 Max Precision 4.0\n",
      "Seen 64 samples.\n",
      "Saving...\n",
      "Done.\n",
      "Testing on Validation set...\n",
      "Epoch 29 Training Cost 5.025040453130549 Validation Precision 4.0 Regularization Cost 0.5672546625137329 Max Precision 4.0\n",
      "Seen 110 samples.\n",
      "Saving...\n",
      "Done.\n",
      "Total training time = 0.28714799880981445\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Tweet2Vec classifier trainer\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import random\n",
    "import sys\n",
    "import batch_word as batch\n",
    "import time\n",
    "import pickle as pkl\n",
    "import io\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "from collections import OrderedDict\n",
    "from w2v import tweet2vec, init_params, load_params_shared\n",
    "from settings_word import NUM_EPOCHS, N_BATCH, N_WORD, SCALE, WDIM, MAX_CLASSES, LEARNING_RATE, DISPF, SAVEF, REGULARIZATION, RELOAD_MODEL, MOMENTUM, SCHEDULE\n",
    "from evaluate_w2v import precision\n",
    "\n",
    "#setting up conditions for Theano:\n",
    "theano.config.gcc.cxxflags = \"-Wno-c++11-narrowing\"\n",
    "os.environ['THEANO_FLAGS'] = \"device=cpu,floatX=float32\"\n",
    "\n",
    "T1 = 0.01\n",
    "T2 = 0.0001\n",
    "\n",
    "train_path = \"/users/kumaraprasannajayaraju/Downloads/NLP_Final_Project/Method 2/data/train_DS.txt\"\n",
    "val_path = \"/users/kumaraprasannajayaraju/Downloads/NLP_Final_Project/Method 2/data/Val_DS.txt\"\n",
    "save_path = \"/users/kumaraprasannajayaraju/Downloads/NLP_Final_Project/Method 2/data\"\n",
    "\n",
    "def schedule(lr, mu):\n",
    "    print(\"Updating Schedule...\")\n",
    "    lr = max(1e-5,lr/2)\n",
    "    return lr, mu\n",
    "\n",
    "def tnorm(tens):\n",
    "    '''\n",
    "    Tensor Norm\n",
    "    '''\n",
    "    return T.sqrt(T.sum(T.sqr(tens),axis=1))\n",
    "\n",
    "def classify(tweet, t_mask, params, n_classes, n_tokens):\n",
    "    # tweet embedding\n",
    "    emb_layer = tweet2vec(tweet, t_mask, params, n_tokens)\n",
    "\n",
    "    # Dense layer for classes\n",
    "    l_dense = lasagne.layers.DenseLayer(emb_layer, n_classes, W=params['W_cl'], b=params['b_cl'], nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return lasagne.layers.get_output(l_dense), l_dense, lasagne.layers.get_output(emb_layer)\n",
    "\n",
    "def main(train_path,val_path,save_path,num_epochs=NUM_EPOCHS):\n",
    "    global T1\n",
    "\n",
    "    # save settings\n",
    "    shutil.copyfile('settings_word.py','%s/settings_word.txt'%save_path)\n",
    "\n",
    "    print(\"Preparing Data...\")\n",
    "    # Training data\n",
    "    Xt = []\n",
    "    yt = []\n",
    "    with io.open(train_path,'r') as f:\n",
    "        for line in f:\n",
    "            (yc, Xc) = line.rstrip('\\n').split('\\t')\n",
    "            Xt.append(Xc)\n",
    "            yt.append(yc)\n",
    "    # Validation data\n",
    "    Xv = []\n",
    "    yv = []\n",
    "    with io.open(val_path,'r') as f:\n",
    "        for line in f:\n",
    "            (yc, Xc) = line.rstrip('\\n').split('\\t')\n",
    "            Xv.append(Xc)\n",
    "            yv.append(yc.split(','))\n",
    "\n",
    "    print(\"Preparing Model...\")\n",
    "    if not RELOAD_MODEL:\n",
    "        # Build dictionaries from training data\n",
    "        tokendict, tokencount = batch.build_dictionary(Xt)\n",
    "        n_token = min(len(tokendict.keys()) + 1, N_WORD)\n",
    "        batch.save_dictionary(tokendict,tokencount,'%s/dict-nlp-w2v-p2.pkl' % save_path)\n",
    "        \n",
    "        # params\n",
    "        params = init_params(n_chars=n_token)\n",
    "        \n",
    "        labeldict, labelcount = batch.build_label_dictionary(yt)\n",
    "        batch.save_dictionary(labeldict, labelcount, '%s/label_dict-nlp-w2v-p2.pkl' % save_path)\n",
    "\n",
    "        n_classes = min(len(labeldict.keys()) + 1, MAX_CLASSES)\n",
    "\n",
    "        # classification params\n",
    "        params['W_cl'] = theano.shared(np.random.normal(loc=0., scale=SCALE, size=(WDIM,n_classes)).astype('float32'), name='W_cl')\n",
    "        params['b_cl'] = theano.shared(np.zeros((n_classes)).astype('float32'), name='b_cl')\n",
    "\n",
    "    else:\n",
    "        print(\"Loading model params...\")\n",
    "        params = load_params_shared('%s/best_model-nlp-w2v-p2.npz' % save_path)\n",
    "\n",
    "        print(\"Loading dictionaries...\")\n",
    "        with open('%s/dict-nlp-w2v-p2.pkl' % save_path, 'rb') as f:\n",
    "            tokendict = pkl.load(f)\n",
    "        with open('%s/label_dict-nlp-w2v-p2.pkl' % save_path, 'rb') as f:\n",
    "            labeldict = pkl.load(f)\n",
    "        n_token = min(len(tokendict.keys()) + 1, N_WORD)\n",
    "        n_classes = min(len(labeldict.keys()) + 1, MAX_CLASSES)\n",
    "\n",
    "    # iterators\n",
    "    train_iter = batch.BatchTweets(Xt, yt, labeldict, batch_size=N_BATCH, max_classes=MAX_CLASSES)\n",
    "    val_iter = batch.BatchTweets(Xv, yv, labeldict, batch_size=N_BATCH, max_classes=MAX_CLASSES, test=True)\n",
    "\n",
    "    print(\"Building network...\")\n",
    "    # Tweet variables\n",
    "    tweet = T.itensor3()\n",
    "    targets = T.ivector()\n",
    "\n",
    "    # masks\n",
    "    t_mask = T.fmatrix()\n",
    "\n",
    "    # network for prediction\n",
    "    predictions, net, emb = classify(tweet, t_mask, params, n_classes, n_token)\n",
    "\n",
    "    # batch loss\n",
    "    loss = lasagne.objectives.categorical_crossentropy(predictions, targets)\n",
    "    cost = T.mean(loss) + REGULARIZATION*lasagne.regularization.regularize_network_params(net, lasagne.regularization.l2)\n",
    "    cost_only = T.mean(loss)\n",
    "    reg_only = REGULARIZATION*lasagne.regularization.regularize_network_params(net, lasagne.regularization.l2)\n",
    "\n",
    "    # params and updates\n",
    "    print(\"Computing updates...\")\n",
    "    lr = LEARNING_RATE\n",
    "    mu = MOMENTUM\n",
    "    updates = lasagne.updates.nesterov_momentum(cost, lasagne.layers.get_all_params(net), lr, momentum=mu)\n",
    "\n",
    "    # Theano function\n",
    "    print(\"Compiling theano functions...\")\n",
    "    inps = [tweet,t_mask,targets]\n",
    "    predict = theano.function([tweet,t_mask],predictions)\n",
    "    encode = theano.function([tweet,t_mask],emb)\n",
    "    cost_val = theano.function(inps,[cost_only,emb])\n",
    "    train = theano.function(inps,cost,updates=updates)\n",
    "    reg_val = theano.function([],reg_only)\n",
    "\n",
    "    # Training\n",
    "    print(\"Training...\")\n",
    "    uidx = 0\n",
    "    maxp = 0.\n",
    "    start = time.time()\n",
    "    valcosts = []\n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            n_samples = 0\n",
    "            train_cost = 0.\n",
    "            print(\"Epoch {}\".format(epoch))\n",
    "\n",
    "            # learning schedule\n",
    "            if len(valcosts) > 1 and SCHEDULE:\n",
    "                change = (valcosts[-1]-valcosts[-2])/abs(valcosts[-2])\n",
    "                if change < T1:\n",
    "                    lr, mu = schedule(lr, mu)\n",
    "                    updates = lasagne.updates.nesterov_momentum(cost, lasagne.layers.get_all_params(net), lr, momentum=mu)\n",
    "                    train = theano.function(inps,cost,updates=updates)\n",
    "                    T1 = T1/2\n",
    "\n",
    "            # stopping criterion\n",
    "            if len(valcosts) > 6:\n",
    "                deltas = []\n",
    "                for i in range(5):\n",
    "                    deltas.append((valcosts[-i-1]-valcosts[-i-2])/abs(valcosts[-i-2]))\n",
    "                if sum(deltas)/len(deltas) < T2:\n",
    "                    break\n",
    "\n",
    "            ud_start = time.time()\n",
    "        for xr,y in train_iter:\n",
    "            n_samples +=len(xr)\n",
    "            uidx += 1\n",
    "            x, x_m = batch.prepare_data(xr, tokendict, n_tokens=n_token)\n",
    "            if x.any()==None:\n",
    "                print(\"Minibatch with zero samples under maxlength.\")\n",
    "                uidx -= 1\n",
    "                continue\n",
    "\n",
    "            curr_cost = train(x,x_m,y)\n",
    "            train_cost += curr_cost*len(xr)\n",
    "            ud = time.time() - ud_start\n",
    "\n",
    "            if np.isnan(curr_cost) or np.isinf(curr_cost):\n",
    "                print(\"Nan detected.\")\n",
    "                return\n",
    "\n",
    "            if np.mod(uidx, DISPF) == 0:\n",
    "                print(\"Epoch {} Update {} Cost {} Time {}\".format(epoch,uidx,curr_cost,ud))\n",
    "\n",
    "            if np.mod(uidx,SAVEF) == 0:\n",
    "                print(\"Saving...\")\n",
    "                saveparams = OrderedDict()\n",
    "                for kk,vv in params.items():\n",
    "                    saveparams[kk] = vv.get_value()\n",
    "                    np.savez('%s/model-nlp-w2v-p2.npz' % save_path,**saveparams)\n",
    "                    print(\"Done.\")\n",
    "\n",
    "            print(\"Testing on Validation set...\")\n",
    "            preds = []\n",
    "            targs = []\n",
    "            for xr,y in val_iter:\n",
    "                x, x_m = batch.prepare_data(xr, tokendict, n_tokens=n_token)\n",
    "                if x.any()==None:\n",
    "                    print(\"Validation: Minibatch with zero samples under maxlength.\")\n",
    "                    continue\n",
    "\n",
    "                vp = predict(x,x_m)\n",
    "                ranks = np.argsort(vp)[:,::-1]\n",
    "                for idx,item in enumerate(xr):\n",
    "                    preds.append(ranks[idx,:])\n",
    "                    targs.append(y[idx])\n",
    "\n",
    "            validation_cost = precision(np.asarray(preds),targs,1)\n",
    "            regularization_cost = reg_val()\n",
    "\n",
    "            if validation_cost > maxp:\n",
    "                maxp = validation_cost\n",
    "                saveparams = OrderedDict()\n",
    "                for kk,vv in params.items():\n",
    "                    saveparams[kk] = vv.get_value()\n",
    "                np.savez('%s/best_model-nlp-w2v-p2.npz' % (save_path),**saveparams)\n",
    "\n",
    "            print(\"Epoch {} Training Cost {} Validation Precision {} Regularization Cost {} Max Precision {}\".format(epoch, train_cost/n_samples, validation_cost, regularization_cost, maxp))\n",
    "            print(\"Seen {} samples.\".format(n_samples))\n",
    "            valcosts.append(validation_cost)\n",
    "\n",
    "            print(\"Saving...\")\n",
    "            saveparams = OrderedDict()\n",
    "            for kk,vv in params.items():\n",
    "                saveparams[kk] = vv.get_value()\n",
    "            np.savez('%s/model-nlp-w2v-p2_%d.npz' % (save_path,epoch),**saveparams)\n",
    "            print(\"Done.\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    print(\"Total training time = {}\".format(time.time()-start))\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(train_path,val_path,save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-service",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
