{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Tweet2Vec:Character-Based Distributed Representations for Social Media\n",
    "\n",
    "#### Kumara Prasanna Jayaraju / Rinaldo Sonia Joseph Santhana Raj\n",
    "\n",
    "####  Emails: kjayaraju@ryerson.ca / rinaldo.joseph@ryerson.ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "#### Problem Description:\n",
    "\n",
    "Text from social media provides a set of challenges that can cause traditional NLP approaches to fail. Informal language, spelling errors, abbreviations, and special characters are all commonplace in these posts. \n",
    "\n",
    "#### Context of the Problem:\n",
    "\n",
    "This is leading to a prohibitively large vocabulary size for word-level approaches. In any natural language corpus a majority of the vocabulary word types will either be absent or occur in low frequency. Estimating the statistical properties of these rare word types is naturally a difficult task.This is analogous to the curse of dimensionality when we deal with sequences of tokens, most sequences will occur only once in the training data.\n",
    "\n",
    "#### Limitation About other Approaches:\n",
    "\n",
    "Traditional Neural Network Language Models (NNLMs) treat words as the basic units of language and assign independent vectors to each word type. This paper is motivated by bi-directional Long Short Term Memory (LSTM )for composing word vectors\n",
    "\n",
    "#### Solution:\n",
    "\n",
    "In this project, the authors explore a similar approach to learning distributed representations of social media posts by composing them from their constituent characters, with the goal of generalizing to out-of-vocabulary words as well as sequences at test time. The paper propose a character composition model, tweet2vec, which finds vector-space representations of whole tweets by learning complex, non-local dependencies in character sequences. The proposed model outperforms a word-level baseline at predicting user-annotated hashtags associated with the posts, doing significantly better when the input contains many out-of-vocabulary words or unusual character sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "| Reference |Explanation |  Dataset/Input |Weakness\n",
    "| --- | --- | --- | --- |\n",
    "| Bengio et al., 2003 [1] | Using neural networks to learn distributed representations of words dates back leased word2vec, a collection of word vectors trained using a recurrent neural network.| word2Vev, sentences, documents and paragraphs | Require storing an extremely large table of vectors for all word types and cannot be easily generalized to unseen words at test time.\n",
    "| Ling et al., 2015 [2] | the authors present a compositional character model based on bidirectional LSTMs as a potential solution to these problems.| Large data sets, sentences, paragraphs| generate word embeddings from character-level representations only, less accuracy. \n",
    "| Luong et al., 2013 [3] | Dealt with the problem of estimating rare word representations by building them from their constituent morphemes | Large data sets, sentences, paragraphs| approach requires a morpheme parser for preprocessing which may not perform well on noisy text like Twitter.\n",
    "| Dhingra et al., 2016 [4] | A character composition model, tweet2vec, which finds vector-space representations of whole tweets by learning complex, non-local dependencies in character sequences. | Feeds, Twitter| The paper is limited only to english language but can be extended for other languages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "Bi-GRU Encoder: Figure below shows our model for encoding tweets. It uses a similar structure to the C2W model in (Ling et al., 2015), with LSTM units replaced with GRU units.\n",
    "\n",
    "The input to the network is defined by an alphabet of characters C (this may include the entire unicode character set). The input tweet is broken into a stream of characters c1 , c2...cm each of which is represented by a 1-by-|C| encoding. These one-hot vectors are then projected to a character space by multiplying with the matrix PC ∈ R|C|×dc, where dc is the dimension of the character vector space.\n",
    "\n",
    "The encoder consists of a forward- GRU and a backward-GRU. Both have the same architecture, except the backward-GRU processes the sequence in reverse order. Each of the GRU units process these vectors sequentially, and start- ing with the initial state h0 compute the sequence h1, h2, ...hm. \n",
    "\n",
    "Finally, the tweet embedding is passed through a linear layer whose output is the same size as the number of hashtags L in the data set. We use a softmax layer to compute the posterior hashtag probabilities and the objective function is to optimize the categorical cross-entropy loss between predicted and true hashtags.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"> Bi-GRU Encoder </div>\n",
    "\n",
    "![Tweet2Vec_Encoder](Tweet2vec_encoder.png \"Bi-GRU Encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation:\n",
    "\n",
    "We didnt have any dataset associated with the paper due to confidentiality. So we planned to collect tweets on our own based on the following common life oriented keywords such as '#life', '#motivation', '#happy', '#emotions', '#friends', '#babies', '#dogs' to implement and test out the project. \n",
    "\n",
    "We have divided the paper into two parts: \n",
    "\n",
    "Part 1: Comparing the correctly predicted hashtags by Word Model baseline with Tweet2Vec\n",
    "\n",
    "Part 2: Training and testing the dataset to calculate Precision, Recall and Mean Rank.\n",
    "\n",
    "In this notebook, we are going show case the implemention of first part which is comparing the predicted hashtags by word model baseline with tweet2Vec Encoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the neccessary libraries:\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import json\n",
    "from textblob import TextBlob, Word, Blobber\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from textblob.taggers import NLTKTagger\n",
    "from decimal import *\n",
    "import re\n",
    "import sys\n",
    "import io\n",
    "import preprocessor as p\n",
    "import numpy as np\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import sys\n",
    "import batch_word as batch\n",
    "import pickle as pkl\n",
    "import os\n",
    "from w2v import tweet2vec, load_params\n",
    "from settings_word import N_BATCH, N_WORD, MAX_CLASSES\n",
    "\n",
    "#setting up conditions for Theano:\n",
    "theano.config.gcc.cxxflags = \"-Wno-c++11-narrowing\"\n",
    "os.environ['THEANO_FLAGS'] = \"device=cpu,floatX=float32\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting Tweets and Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting Tweets:\n",
    "'''\n",
    "please provide your twitter developer API crendentials before running this cell\n",
    "\n",
    "'''\n",
    "# please provide your twitter developer API crendentials:\n",
    "consumer_key = ''\n",
    "consumer_secret =''\n",
    "access_token = ''\n",
    "access_token_secret = ''\n",
    "\n",
    "\n",
    "class StdOutListener(StreamListener):\n",
    "    \n",
    "    def on_data(self, data):\n",
    "        dict_data = json.loads(data)\n",
    "\n",
    "        if \"text\" in dict_data.keys():\n",
    "            saveFile = open('Tweets2Vec_DA.rtf', 'a')\n",
    "            saveFile.write(dict_data[\"text\"])\n",
    "            saveFile.close()\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    l = StdOutListener()\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    stream = Stream(auth, l)\n",
    "\n",
    "    #This line filter Twitter Streams to capture data by below mentioned keywords:\n",
    "    stream.filter(languages=[\"en\"], track=['#life', '#motivation', '#happy', '#emotions', '#friends', '#babies', '#dogs'])\n",
    "\n",
    "# Preprocessing: \n",
    "\n",
    "# input and output files\n",
    "infile = \"/NLP_Final_Project/Part 1/data/Tweets2Vec_DA.rtf\"\n",
    "outfile = \"/NLP_Final_Project/Part 1/data/life_t2v_ds_en_op.txt\"\n",
    "\n",
    "regex_str = [\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)+' # anything else\n",
    "]\n",
    "\n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "def preprocess(s, lowercase=True):\n",
    "    tokens = tokenize(s)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    html_regex = re.compile('<[^>]+>')\n",
    "    tokens = [token for token in tokens if not html_regex.match(token)]\n",
    "\n",
    "    mention_regex = re.compile('(?:@[\\w_]+)')\n",
    "    tokens = ['@mention' if mention_regex.match(token) else token for token in tokens]\n",
    "\n",
    "    url_regex = re.compile('http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+')\n",
    "    tokens = ['@url' if url_regex.match(token) else token for token in tokens]\n",
    "\n",
    "    hashtag_regex = re.compile(\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\")\n",
    "    tokens = ['@hash' if hashtag_regex.match(token) else token for token in tokens]    \n",
    "\n",
    "    f = p.clean(' '.join([t for t in tokens if t]).replace('rt','')\n",
    "                   .replace(':','').replace('...','')\n",
    "                   .replace('@mention', '').replace('@url', '').replace('@hash', ''))\n",
    "    \n",
    "    return f\n",
    "\n",
    "with io.open(outfile, 'w') as tweet_processed_text, io.open(infile, 'r') as fin:\n",
    "    for line in fin:\n",
    "        tweet_processed_text.write(preprocess(line)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding_Tweets_W2V and saving predicted tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Data...\n",
      "Loading model params...\n",
      "Loading dictionaries...\n",
      "Building network...\n",
      "Compiling theano functions...\n",
      "Encoding...\n",
      "Saving...\n"
     ]
    }
   ],
   "source": [
    "# Encoding_Tweets_W2V\n",
    "\n",
    "import numpy as np\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import sys\n",
    "import batch_word as batch\n",
    "import pickle as pkl\n",
    "import io\n",
    "import os\n",
    "\n",
    "from w2v import tweet2vec, load_params\n",
    "from settings_word import N_BATCH, N_WORD, MAX_CLASSES\n",
    "\n",
    "#setting up conditions for Theano:\n",
    "theano.config.gcc.cxxflags = \"-Wno-c++11-narrowing\"\n",
    "os.environ['THEANO_FLAGS'] = \"device=cpu,floatX=float32\"\n",
    "\n",
    "\n",
    "def invert(d):\n",
    "    out = {}\n",
    "    for k,v in d.items():\n",
    "        out[v] = k\n",
    "    return out\n",
    "\n",
    "def classify(tweet, t_mask, params, n_classes, n_chars):\n",
    "    # tweet embedding\n",
    "    emb_layer = tweet2vec(tweet, t_mask, params, n_chars)\n",
    "    # Dense layer for classes\n",
    "    l_dense = lasagne.layers.DenseLayer(emb_layer, n_classes, W=params['W_cl'], b=params['b_cl'], nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return lasagne.layers.get_output(l_dense), lasagne.layers.get_output(emb_layer)\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    data_path = \"/NLP_Final_Project/Part 1/data/life_t2v_ds_en_op.txt\"\n",
    "    model_path = \"/NLP_Final_Project/Part 1/src\"\n",
    "    save_path = \"/NLP_Final_Project/Part 1/data\"\n",
    "    if len(args)>3:\n",
    "        m_num = int(args[3])\n",
    "\n",
    "    print(\"Preparing Data...\")\n",
    "    # Test data\n",
    "    Xt = []\n",
    "    with io.open(data_path,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            Xc = line.rstrip('\\n')\n",
    "            Xt.append(Xc)\n",
    "\n",
    "    # Model\n",
    "    print(\"Loading model params...\")\n",
    "    if len(args)>3:\n",
    "        params = load_params('%s/model-w2v_%d.npz' % (model_path,m_num))\n",
    "    else:\n",
    "        params = load_params('%s/best_model-nlp-w2v.npz' % model_path)\n",
    "\n",
    "    print(\"Loading dictionaries...\")\n",
    "    with open('%s/dict-nlp-w2v.pkl' % model_path, 'rb') as f:\n",
    "        chardict = pkl.load(f)\n",
    "    with open('%s/label_dict-nlp-w2v.pkl' % model_path, 'rb') as f:\n",
    "        labeldict = pkl.load(f)\n",
    "    n_char = min(len(chardict.keys()) + 1, N_WORD)\n",
    "    n_classes = min(len(labeldict.keys()) + 1, MAX_CLASSES)\n",
    "    inverse_labeldict = invert(labeldict)\n",
    "\n",
    "    print(\"Building network...\")\n",
    "    \n",
    "    # Tweet variables\n",
    "    tweet = T.itensor3()\n",
    "    t_mask = T.fmatrix()\n",
    "\n",
    "    # network for prediction\n",
    "    predictions, embeddings = classify(tweet, t_mask, params, n_classes, n_char)\n",
    "    \n",
    "    # Theano function\n",
    "    print(\"Compiling theano functions...\")\n",
    "    predict = theano.function([tweet,t_mask],predictions)\n",
    "    encode = theano.function([tweet,t_mask],embeddings)\n",
    "\n",
    "    # Test\n",
    "    print(\"Encoding...\")\n",
    "    out_pred = []\n",
    "    out_emb = []\n",
    "    numbatches = int(len(Xt)/N_BATCH + 1)\n",
    "    for i in range(numbatches):\n",
    "        xr = Xt[N_BATCH*i:N_BATCH*(i+1)]\n",
    "        x, x_m = batch.prepare_data(xr, chardict, n_tokens=n_char)\n",
    "        p = predict(x,x_m)\n",
    "        e = encode(x,x_m)\n",
    "        ranks = np.argsort(p)[:,::-1]\n",
    "\n",
    "        for idx, item in enumerate(xr):\n",
    "            out_pred.append(' '.join([inverse_labeldict[r] for r in ranks[idx,:5]]))\n",
    "            out_emb.append(e[idx,:])\n",
    "\n",
    "    # Save\n",
    "    print(\"Saving...\")\n",
    "    with io.open('%s/predicted_tags-nlp-w2v.txt'%save_path,'w') as f:\n",
    "        for item in out_pred:\n",
    "            f.write(item + '\\n')\n",
    "    with open('%s/embeddings-nlp-w2v.npy'%save_path,'wb') as f:\n",
    "        np.save(f,np.asarray(out_emb))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding_Tweets_T2V and saving predicted tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Data...\n",
      "Loading model params...\n",
      "Loading dictionaries...\n",
      "Building network...\n",
      "Compiling theano functions...\n",
      "Encoding...\n",
      "Saving...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import sys\n",
    "import batch_char as batch\n",
    "import pickle as pkl\n",
    "import io\n",
    "import os\n",
    "\n",
    "from t2v import tweet2vec, init_params, load_params\n",
    "from settings_char import N_BATCH, MAX_LENGTH, MAX_CLASSES\n",
    "\n",
    "\n",
    "#setting up conditions for Theano:\n",
    "theano.config.gcc.cxxflags = \"-Wno-c++11-narrowing\"\n",
    "os.environ['THEANO_FLAGS'] = \"device=cpu,floatX=float32\"\n",
    "\n",
    "\n",
    "def invert(d):\n",
    "    out = {}\n",
    "    for k,v in d.items():\n",
    "        out[v] = k\n",
    "    return out\n",
    "\n",
    "def classify(tweet, t_mask, params, n_classes, n_chars):\n",
    "    # tweet embedding\n",
    "    emb_layer = tweet2vec(tweet, t_mask, params, n_chars)\n",
    "    # Dense layer for classes\n",
    "    l_dense = lasagne.layers.DenseLayer(emb_layer, n_classes, W=params['W_cl'], b=params['b_cl'], nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return lasagne.layers.get_output(l_dense), lasagne.layers.get_output(emb_layer)\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    data_path = \"/NLP_Final_Project/Part 1/data/life_t2v_ds_en_op.txt\"\n",
    "    model_path = \"/NLP_Final_Project/Part 1/src\"\n",
    "    save_path = \"/NLP_Final_Project/Part 1/data\"\n",
    "    if len(args)>3:\n",
    "        m_num = int(args[3])\n",
    "\n",
    "    print(\"Preparing Data...\")\n",
    "    # Test data\n",
    "    Xt = []\n",
    "    with io.open(data_path,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            Xc = line.rstrip('\\n')\n",
    "            Xt.append(Xc[:MAX_LENGTH])\n",
    "\n",
    "    # Model\n",
    "    print(\"Loading model params...\")\n",
    "    if len(args)>3:\n",
    "        params = load_params('%s/model-nlp-t2v_%d.npz' % (model_path,m_num))\n",
    "    else:\n",
    "        params = load_params('%s/best_model-nlp-t2v.npz' % model_path)\n",
    "\n",
    "    print(\"Loading dictionaries...\")\n",
    "    with open('%s/dict-nlp-t2v.pkl' % model_path, 'rb') as f:\n",
    "        chardict = pkl.load(f)\n",
    "    with open('%s/label_dict-nlp-t2v.pkl' % model_path, 'rb') as f:\n",
    "        labeldict = pkl.load(f)\n",
    "    n_char = len(chardict.keys()) + 1\n",
    "    n_classes = min(len(labeldict.keys()) + 1, MAX_CLASSES)\n",
    "    inverse_labeldict = invert(labeldict)\n",
    "\n",
    "    print(\"Building network...\")\n",
    "    # Tweet variables\n",
    "    tweet = T.itensor3()\n",
    "    t_mask = T.fmatrix()\n",
    "\n",
    "    # network for prediction\n",
    "    predictions, embeddings = classify(tweet, t_mask, params, n_classes, n_char)\n",
    "\n",
    "    # Theano function\n",
    "    print(\"Compiling theano functions...\")\n",
    "    predict = theano.function([tweet,t_mask],predictions)\n",
    "    encode = theano.function([tweet,t_mask],embeddings)\n",
    "\n",
    "    # Test\n",
    "    print(\"Encoding...\")\n",
    "    out_pred = []\n",
    "    out_emb = []\n",
    "    numbatches = int(len(Xt)/N_BATCH + 1)\n",
    "    for i in range(numbatches):\n",
    "        xr = Xt[N_BATCH*i:N_BATCH*(i+1)]\n",
    "        x, x_m = batch.prepare_data(xr, chardict, n_chars=n_char)\n",
    "        p = predict(x,x_m)\n",
    "        e = encode(x,x_m)\n",
    "        ranks = np.argsort(p)[:,::-1]\n",
    "\n",
    "        for idx, item in enumerate(xr):\n",
    "            out_pred.append(' '.join([inverse_labeldict[r] if r in inverse_labeldict else 'UNK' for r in ranks[idx,:5]]))\n",
    "            out_emb.append(e[idx,:])\n",
    "\n",
    "    # Save\n",
    "    print(\"Saving...\")\n",
    "    with io.open('%s/predicted_tags-nlp-t2v.txt'%save_path,'w') as f:\n",
    "        for item in out_pred:\n",
    "            f.write(item + '\\n')\n",
    "    with open('%s/embeddings-nlp-t2v.npy'%save_path,'wb') as f:\n",
    "        np.save(f,np.asarray(out_emb))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output files are stored under the data files. we have combined top 10 tweets and its precticted hastags for the comparison and provided screenshot below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"> Examples of top predictions from the models </div>\n",
    "\n",
    "![Output_Comparison](output.png \"Examples of top predictions from the models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Future Direction\n",
    "\n",
    "Our learning from this project is that, tweet2vec encoder performs better than word baseline for social media posts trained using supervision from associated hashtags. However, based on our observation there were few tweets were words baseline had better prediction of hastags as well. With respect to performance, without doubt tweet2vec outperforms the word baseline. This paper was limited to English language however the model can be extended to other languages as well. Future direction of the project will focus on how the model can be used for domains specific classification such as news feeds, social media and any content based platforms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "[1]: [Dhingra1 et al.2016] Bhuwan Dhingra1, Zhong Zhou2, Dylan Fitzpatrick1,2\n",
    "Michael Muehl1 and William W. Cohen1, Tweet2Vec: Character-Based Distributed Representations for Social Media, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),2016\n",
    "\n",
    "\n",
    "[2]: [Bengio et al.2003] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neu- ral probabilistic language model. The Journal of Machine Learning Research, 3:1137–1155.\n",
    "\n",
    "[3]: [Godin et al.2013] Fréderic Godin, Viktor Slavkovikj, Wesley De Neve, Benjamin Schrauwen, and Rik Van de Walle. 2013. Using topic models for twit- ter hashtag recommendation. In Proceedings of the 22nd international conference on World Wide Web companion, pages 593–596. International World Wide Web Conferences Steering Committee.\n",
    "\n",
    "[4]: [Zhangetal.2015] XiangZhang,JunboZhao,andYann LeCun. 2015. Character-level convolutional net- works for text classification. In Advances in Neural Information Processing Systems, pages 649–657."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
