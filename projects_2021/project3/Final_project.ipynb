{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: Sentiment Classification using Document Embeddings trained with Cosine Similarity\r\n",
    "\r\n",
    "#### Members' Names: Maya Kodeih and Matthias Ekundayo\r\n",
    "\r\n",
    "####  Emails: mkodeih@ryerson.ca, mekundayo@ryerson.ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "#### Problem Description:\n",
    "\n",
    "Document embedding involves mapping documents to a dense, low dimensional vector, therefore each document in a sentiment classification is mapped to a fixed length vector. This paper proposes training document embedding using cosine similarity instead of dot-product. “This paper aims to improve existing document embedding models by training document embedding using cosine similarity.” [30] Evaluation of this document embedding is done by comparing the accuracy of the document embedding done with dot product in earlier papers. “Combining document embedding done with cosine similarity with Naïve Bayes weighted bag of n-grams achieves a new state of art accuracy of 97.42%.” [30]!\n",
    "\n",
    "The current paper [30] has trained and tested the model on the IMDB dataset. We extended the work to test the model on the Amazon reviews dataset and we achieved an accuracy score of 91%.\n",
    "\n",
    "#### Context of the Problem:\n",
    "\n",
    "When doing sentiment classification on documents, the representation of the documents plays a major part in the efficiency of the classification.  Document embedding models has been widely used for sentiment classification.  The aim of this paper is to improve the exisitng document embedding models by using cosine similary instead of the dot product to train document embeddings.  So instead of miximizing the dot product between document vectors and words/n-grams in the document the model will maximize the cosine similarity.\n",
    "\n",
    "Although cosine similarity is widely used to measure document similarity, the aprroach in this paper is to directly maximize the cosine similarity between similar documents, by reducing the angle between the documents to its lowest which may encode useful information for seperating different types of documents, and by doing so aims to improve exisitng embedding models.\n",
    "\n",
    "\n",
    "#### Limitation About other Approaches:\n",
    "\n",
    "Prior approaches have shown good accuracy results, however when compared to the approach used in this paper, the accuracy was lower in all cases.  This approach followed in this paper has provided higher accuracy result on the IMDB Dataset. \n",
    "\n",
    "This table summarize all accuracies obtained when test was conducted on IMDB dataset:\n",
    "\n",
    "| Model | IMDB Dataset Accuracy (%) \n",
    "| --- | --- \n",
    "| NB-SVM Bigrams (Wang and Manning, 2012)  | 91.22 \n",
    "| NB-SVM Trigrams (Mesnil et al., 2015) | 91.87\n",
    "| DV-ngram (Li et al., 2016a) | 92.14\n",
    "| Dot Product with L2 Regularization | 92.45\n",
    "| Paragraph Vector (Le and Mikolov, 2014) | 92.58\n",
    "| Document Vectors using Cosine Similarity | 93.13\n",
    "| W-Neural-BON Ensemble (Li et al., 2016b) | 93.51\n",
    "| TGNR Ensemble (Li et al., 2017) | 93.51\n",
    "| TopicRNN (Dieng et al., 2017)  | 93.76\n",
    "| One-hot bi-LSTM (Johnson and Zhang, 2016) | 94.06\n",
    "| Virtual Adversarial (Miyato et al., 2016) | 94.09\n",
    "| BERT large finetune UDA (Xie et al., 2019) | 95.8\n",
    "| NB-weighted-BON + DV-ngram  | 96.95\n",
    "| NB-weighted-BON + L2R Dot Product | 97.17\n",
    "| NB-weighted-BON + Cosine Similarity | 97.42\n",
    "\n",
    "\n",
    "#### Solution:\n",
    "This paper explores using the cosine similarity instead of the dot product in computing the similarity measure between the input and the output vector, it focuses specifically on modifications to **Paragraph Vector** (PV-DBOW) and the similar **Document Vector by predicting n-grams** (DV-ngram models).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "| Reference |Explanation |  Dataset/Input |Weakness\n",
    "| --- | --- | --- | --- |\n",
    "| Aurora et al. [2] | They produced Sentence Embeddings by computing the weighted average of word vectors, where each word is weighted using smooth inverse frequency, and removing the first principle component | All the datasets from SemEval semantic textual similarity (STS) tasks (2012-2015), and the SemEval 2015 Twitter task and the SemEval 2014 Semantic Relatedness task | Only 86% accuracy for similarity\n",
    "| Le and Mikolov [11] | They introduced Paragraph Vectors were as a modification to word emdeddings | Stanford sentiment treebank dataset and IMDB dataset | Lowest error, outperforming Weighted-bag-of-bigrams, bag-of-bigrams and bag-of-words.  Although the focus is to represent texts, the method can be applied to learn representations for sequential data\n",
    "| Li and al. [12] | They trained paragraph vectors to predict not only the words in the paragraph but n-grams in the paragraph as well using used Weighted Neural Bag of n-grams (W-neural-BON).  They introduced new methods for embedding n-grams |  IMDB, RT-2K, AthR, Xgraph, BbCrypt, RT, CR, MPQA | 93% on ensemble but require careful dataset-specific hyper-parameter tuning for better performance\n",
    "| Peters el al. [22] | They learned contextualized word embeddings by training a bidirectional LSTM on Lanuage modelling task (ELMO) | SNLI, CoNLL 2012 (coref/SRL), CoNLL 2003 (NER) , SQuAD, SST | 89.3 accuracy for ensemble.  No weaknesses mentioned in paper\n",
    "| Devlin et al. [4] | They use the masked language model objective which is predicting the masked word given the left and right context, in order to pre-train a multi-layer bidirectional Transformer (BERT) | GLUE, SQuAD v1.1, SQuAD v2.0, SWAG | No weaknesses mentioned |\n",
    "| Luo et al. [15] | They use cosine similarity insteasd of dot product in computing a layer's pre-activation as a regularization mechanism | MNIST, 20NEWS GROUP,  CIFAR-10/100, SVHN | No weaknesses or future works were mentioned in this paper |\n",
    "| Wieting et al. [28] | They train word vectors in such a way that the cosine similarity between the resultant document vectors a paraphrase pair is directly maximized | 24 textual similarity datasets, all datasets from every SemEval semantic textual similarity (STS) task (2012-2015), SemEval 2015 Twitter task and the SemEval 2014 Semantic Relatedness task, as well as two tasks that use PPDB data | Future work will focus on improving embeddings by effectively handling undertrained words as well as by exploring new models that generalize even better to the large suite of text similarity tasks used in the experiments |\n",
    "| Thongtan et al. [30] | They trained document embeddings using cosine similarity instead of dot products | IMDB | An important future development is to carry the experiments on other datasets. It is essential to benchmark on more than one dataset, to prevent superficially good results by overfitting hyperparameters or the cosine similarity model itself to the IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\r\n",
    "\r\n",
    "We were able to implement the cosine similarity using a different dataset (Amazon reviews) from what used in the paper(IMDB). This was achieved by transforming the Amazon review dataset into three different datasets of unigram, bi-gram and tri-gram. These ngrams were then fed into the model to generate the document embedding for the Amazon reviews dataset that was used for the classification tks\n",
    ".\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "In this section, you will provide the code and its explanation. You may have to create more cells after this. (To keep the Notebook clean, do not display debugging output or thousands of print statements from hundreds of epochs. Make sure it is readable for others by reviewing it yourself carefully.)\n",
    "\n",
    "The model reads from 3 difeerent sets of files.  One file contains the original reviews' text, another file contains the bigrams extracted from the reviews, and a third file contains the trigrams extracted from the reviews.  We will explain later how these files are built.\n",
    "\n",
    "The dataset we used was downloaded from Kaggle.  It contains 100,000 records of Amazon customer reviews.  The original file contained many features that were not necessary for this research and were therefore removed. \n",
    "\n",
    "<u>Data File Structure :</u>\n",
    "  \n",
    "id, dateAdded, dateUpdated, name, asins, brand, categories, primaryCategories, imageURLs, keys, manufacturer,\n",
    "manufacturerNumber, reviews.date, reviews.dateSeen, reviews.didPurchase, reviews.doRecommend, reviews.id, \n",
    "reviews.numHelpful, reviews.rating, reviews.sourceURLs, reviews.text, reviews.title, reviews.username, \n",
    "sourceURLs\n",
    "\n",
    "\n",
    "<u>File Length :</u>\n",
    "3 amazon files were concatenated to produce one file containing 100,000 reviews and match the size of the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from matplotlib.table import Table\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Record Count 100000\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "dataset1 = pd.read_csv(\"AmazonCustomerReviewsData/archive/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv\")\n",
    "dataset2 = pd.read_csv(\"AmazonCustomerReviewsData/archive/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv\")\n",
    "dataset3 = pd.read_csv(\"AmazonCustomerReviewsData/archive/1429_1_2.csv\")\n",
    "\n",
    "tot_cnt = 0\n",
    "a_file = open(\"alldata-id_p1gram.txt\", \"w\")\n",
    "\n",
    "#print(dataset1.head())\n",
    "reviews = np.array(dataset1['reviews.text'])\n",
    "\n",
    "i = 0\n",
    "for i,review in enumerate(reviews):\n",
    "    if len(review) < 100:\n",
    "        par = 100 / len(review) \n",
    "    else:\n",
    "        par = 1 \n",
    "    par = math.ceil(par)\n",
    "    review = par*(review)\n",
    "    a_file.write(\"_*\"+str(tot_cnt)+\" \"+str(review)+\" . \\n\")\n",
    "    tot_cnt += 1\n",
    "\n",
    "reviews = np.array(dataset2['reviews.text'])\n",
    "\n",
    "for i,review in enumerate(reviews):\n",
    "    if len(review) < 100:\n",
    "        par = 100 / len(review) \n",
    "    else:\n",
    "        par = 1\n",
    "    par = math.ceil(par)\n",
    "    review = par*(review)\n",
    "    a_file.write(\"_*\"+str(tot_cnt)+\" \"+str(review)+\" . \\n\")\n",
    "    tot_cnt += 1\n",
    "\n",
    "reviews = np.array(dataset3['reviews.text'].astype(str))\n",
    "\n",
    "for i,review in enumerate(reviews):\n",
    "    if len(review) < 100:\n",
    "        #print(review)\n",
    "        par = 100 / len(review) \n",
    "    else:\n",
    "        par = 1\n",
    "    par = math.ceil(par)\n",
    "    review = par*(review)\n",
    "    a_file.write(\"_*\"+str(tot_cnt)+\" \"+str(review)+\" . \\n\")\n",
    "    tot_cnt += 1\n",
    "\n",
    "print(\"Total Record Count\", tot_cnt)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create BiGram DataSet\n",
    "\n",
    "As we have mentioned in the previous section, the model reads 3 different files formats.  One file with unigrams, one with bigrams and one with trigrams.  The files have a very specific format and uses special characters to mark start of review and bigram or trigram concatenation.\n",
    "\n",
    "We followed the exact technique to build the bigrams and trigram files from the original unigram file on the Amazon dataset and produced files similar to the original files for IMDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [00:14, 6761.81it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "a_file = open(\"alldata-id_p1gram.txt\", \"r\")\n",
    "bigram_file = open(\"alldata-id_p2gram.txt\", \"w\")\n",
    "bigram_list=[]\n",
    "\n",
    "for i,line in tqdm(enumerate(a_file)):\n",
    "    words = line.split()\n",
    "    #print(words)\n",
    "    for j,word in enumerate(words):\n",
    "        if word[0]=='_' and word[1]=='*':\n",
    "            bigram_list.append(word)\n",
    "        else:\n",
    "            if j+1 < len(words):\n",
    "                bigram_list.append(word)\n",
    "                bigram_list.append(word+'@$'+words[j+1])\n",
    "            else:\n",
    "                bigram_list.append(word)\n",
    "            \n",
    "    for term in bigram_list:\n",
    "        bigram_file.writelines(str(term)+\" \")\n",
    "    bigram_file.writelines(\"\\n\")\n",
    "    bigram_list = []\n",
    "\n",
    "\n",
    "a_file.close()\n",
    "bigram_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TriGram DataSet\n",
    "\n",
    "The code below is where the trigram dataset is built, as we have mentioned, according to the original format used for the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [00:18, 5403.22it/s]\n"
     ]
    }
   ],
   "source": [
    "a_file = open(\"alldata-id_p1gram.txt\", \"r\")\n",
    "trigram_file = open(\"alldata-id_p3gram.txt\", \"w\")\n",
    "trigram_list=[]\n",
    "\n",
    "for i,line in tqdm(enumerate(a_file)):\n",
    "    words = line.split()\n",
    "    #print(words)\n",
    "    for j,word in enumerate(words):\n",
    "        if word[0]=='_' and word[1]=='*':\n",
    "            trigram_list.append(word)\n",
    "        else:\n",
    "            if j+2 < len(words):\n",
    "                trigram_list.append(word)\n",
    "                trigram_list.append(word+'@$'+words[j+1])\n",
    "                trigram_list.append(word+'@$'+words[j+1]+'@$'+words[j+2])\n",
    "            else:\n",
    "                trigram_list.append(word)\n",
    "            \n",
    "    for term in trigram_list:\n",
    "        trigram_file.writelines(str(term)+\" \")\n",
    "    trigram_file.writelines(\"\\n\")\n",
    "    trigram_list = []\n",
    "\n",
    "\n",
    "a_file.close()\n",
    "trigram_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model on the newly selected data\r\n",
    "\r\n",
    "The model run on Amazon data (the dataset we introduced), gives an accuracy of 90.996%.  The model gives an \r\n",
    "accuracy of 97.4% as has been shown below.  The reason for this difference in accuracy is explained in the next section.  An accuracy of 90.996% is considered high but brings this model down to the level of other models making it loose its advantage.\r\n",
    "\r\n",
    "To run the model we call the ensemble.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading embeddings\n",
      "Reading documents\n",
      "Extracting features\n",
      "Calculating probabilities\n",
      "Weighing features\n",
      "Training classifier\n",
      "Testing classifier\n",
      "Accuracy= 90.996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "%run 'ensemble.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Original Data and run the model\n",
    "\n",
    "To retrieve the original dataset we overwrite the data files from a backup directory containing the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alldata-id_p1gram.txt alldata-id_p2gram.txt alldata-id_p3gram.txt\n",
      "Old data file reinstalled\n",
      "Old data file reinstalled\n",
      "Old data file reinstalled\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "!ls Originaldata\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"alldata-id_p1gram.txt\"):\n",
    "    print(\"Old data file reinstalled\")\n",
    "    os.remove(\"alldata-id_p1gram.txt\")\n",
    "    shutil.copy2('Originaldata/alldata-id_p1gram.txt', 'alldata-id_p1gram.txt') \n",
    "else:\n",
    "    print(\"The file does not exist\")\n",
    "\n",
    "if os.path.exists(\"alldata-id_p2gram.txt\"):\n",
    "    print(\"Old data file reinstalled\")\n",
    "    os.remove(\"alldata-id_p2gram.txt\")\n",
    "    shutil.copy2('Originaldata/alldata-id_p2gram.txt', 'alldata-id_p2gram.txt') \n",
    "else:\n",
    "    print(\"The file does not exist\")\n",
    "    \n",
    "if os.path.exists(\"alldata-id_p3gram.txt\"):\n",
    "    print(\"Old data file reinstalled\")\n",
    "    os.remove(\"alldata-id_p3gram.txt\")\n",
    "    shutil.copy2('Originaldata/alldata-id_p3gram.txt', 'alldata-id_p3gram.txt') \n",
    "else:\n",
    "    print(\"The file does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running model after restoring original data\n",
    "\n",
    "We can see that the model has a higher accuracy on the original data.  This could be due to the fact that the original IMDB dataset sentences are much longer than the Amazon sentences.  In fact while working with the amazon data we found out that many ratings had a few words only, whereas the movie ratings are big paragraphs.\n",
    "\n",
    "The accuracy of the model run on original IMDB dataset is 97.4%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading embeddings\n",
      "Reading documents\n",
      "Extracting features\n",
      "Calculating probabilities\n",
      "Weighing features\n",
      "Training classifier\n",
      "Testing classifier\n",
      "Accuracy= 97.428\n"
     ]
    }
   ],
   "source": [
    "%run 'ensemble.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Future Direction\r\n",
    "\r\n",
    "Although we were able to achieve a good accuracy of 90.99% using a different dataset, this is lower than the accuracy of 97.42% that was achieved in the paper using IMDB dataset.\r\n",
    "This may mean that the parameters of the model have been over-tuned for the IMDB dataset and as such the performance drops when a different dataset is used.\r\n",
    "Future work might be to incorporate Lifelong Learning (LLL) in order for the model to be able to adapt to different datasets without re-tuning the model whenever a new dataset is encountered.\r\n",
    "Also, it will be worth finding out the performance of previous works on the Amazon review dataset so as to compare their performances against this project.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "[1]:  Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe. 2015. Semeval-2015 task 2: Semantic tex- tual similarity, english, spanish and pilot on interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation, pages 252–263.\n",
    "\n",
    "[2]: Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017. A simple but tough-to-beat baseline for sentence embeddings. In Proceedings of the 5th International Conference on Learning Representations.\n",
    "Yoshua Bengio, Re ́jean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3(Feb):1137–1155.\n",
    "\n",
    "[3]:  Andrew M. Dai, Christopher Olah, and Quoc V. Le. 2015. Document embedding with paragraph vectors. arXiv preprint arXiv:1507.07998.\n",
    "\n",
    "[4]:  Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n",
    "\n",
    "[5]:  Adji B. Dieng, Chong Wang, Jianfeng Gao, and John Paisley. 2017. Topicrnn: A recurrent neural network with long-range semantic dependency. In Proceedings of the 5th International Conference on Learning Representations.\n",
    "\n",
    "[6]:  Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. Journal of Machine Learning Research, 9(Aug):1871–1874.\n",
    "\n",
    "[7]:  Sepp Hochreiter and Jurgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735–1780.\n",
    "\n",
    "[8]:  Thorsten Joachims. 1998. Text categorization with support vector machines: Learning with many relevant features. In Proceedings of the 10th European Conference on Machine Learning, pages 137–142.\n",
    "\n",
    "[9]:  Rie Johnson and Tong Zhang. 2016. Supervised and semi-supervised text categorization using lstm for region embeddings. In Proceedings of the 4th International Conference on Learning Representations.\n",
    "\n",
    "[10]:  Jey Han Lau and Timothy Baldwin. 2016. An empirical evaluation of doc2vec with practical insights into document embedding generation. arXiv preprint arXiv:1607.05368.\n",
    "\n",
    "[11]:  Quoc V. Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In Proceedings of the 31st International Conference on Machine Learning, pages 1188–1196.\n",
    "\n",
    "[12]:  Bofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, and Zhe Zhao. 2016a. Learning document embeddings by predicting n-grams for sentiment classification of long movie reviews. In Proceedings of the 4th Inter- national Workshop on Learning Representations.\n",
    "\n",
    "[13]:  Bofang Li, Tao Liu, Zhe Zhao, Puwei Wang, and Xi-aoyong Du. 2017. Neural bag-of-ngrams. In Proceedings of the 31st AAAI Conference on Artificial Intelligence, pages 3067–3074.\n",
    "\n",
    "[14]:  Bofang Li, Zhe Zhao, Tao Liu, Puwei Wang, and Xi-aoyong Du. 2016b. Weighted neural bag-of-n-grams model: New baselines for text classification. In Proceedings of the 26th International Conference on Computational Linguistics, pages 1591–1600.\n",
    "\n",
    "[15]:  Chunjie Luo, Jianfeng Zhan, Lei Wang, and Qiang Yang. 2017. Cosine normalization: Using cosine similarity instead of dot product in neural networks. arXiv preprint arXiv:1702.05870.\n",
    "\n",
    "[16]:  Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the As- sociation for Computational Linguistics, pages 142– 150.\n",
    "\n",
    "[17]:  Gregoire Mesnil, Tomas Mikolov, Marc’Aurelio Ranzato, and Yoshua Bengio. 2015. Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews. In Proceedings of the 3rd International Workshop on Learning Representations.\n",
    "\n",
    "[18]:  Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor- rado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Proceedings of the 26th International Conference on Neural Information Processing Systems, pages 3111–3119.\n",
    "\n",
    "[19]:  Takeru Miyato, Andrew M. Dai, and Ian Goodfellow. 2016. Adversarial training methods for semi-supervised text classification. In Proceedings of the 4th International Conference on Learning Representations.\n",
    "\n",
    "[20]:  Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, page 271.\n",
    "\n",
    "[21]: Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in python. Journal of Machine Learning Research, 12(Oct):2825–2830.\n",
    "\n",
    "[22]: Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2227–2237.\n",
    "\n",
    "[23]: David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. 1986. Learning representations by back- propagating errors. Nature, 323(6088):533–536.\n",
    "\n",
    "[24]: Amit Singhal. 2001. Modern information retrieval: A brief overview. IEEE Data Engineering Bulletin, 24(4):35–43.\n",
    "\n",
    "[25]: Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642.\n",
    "\n",
    "[26]: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Sys- tems, pages 5998–6008.\n",
    "\n",
    "[27]: Sida Wang and Christopher D. Manning. 2012. Baselines and bigrams: Simple, good sentiment and topic classification. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 90–94.\n",
    "\n",
    "[28]: John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2015. Towards universal paraphrastic sentence embeddings. In Proceedings of the 4th International Conference on Learning Representations.\n",
    "\n",
    "[29]: Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Lu- ong, and Quoc V. Le. 2019. Unsupervised data augmentation. arXiv preprint arXiv:1904.12848.\n",
    "\n",
    "[30]: Thongtan, Tan and Phienthrakul, Tanasanee 2019. Thongtan, Tan  and Phienthrakul, Tanasanee. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 407-414."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
