{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: RMDL and Roberta BERT for text classification\n",
    "\n",
    "#### Individual's Name : Garima Malik\n",
    "\n",
    "####  Emails : garima.malik@ryerson.ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTRODUCTION:\n",
    "*********************************************************************************************************************\n",
    "#### AIM : \n",
    "To empirically analyse the feasibility of the proposed Model in the paper \"RMDL: Random Multimodel Deep Learning for Classification\" released in 2018.\n",
    "*********************************************************************************************************************\n",
    "#### Github Repo: \n",
    "https://github.com/kk7nc/RMDL\n",
    "*********************************************************************************************************************\n",
    "#### DESCRIPTION OF PAPER:\n",
    "RMDL model can be seen as ensemble approach for deep learning models.RMDL solves the problem of finding the best deep learning structure and architecture while simultaneously improving robustness and accuracy through ensembles of deep learning architectures. \n",
    "*********************************************************************************************************************\n",
    "#### PROBLEM STATEMENT :\n",
    "* Try to replicate the results given in paper on text classification datasets with RMDL models\n",
    "* choose 2 standard datsets : imdb and reuters\n",
    "* To assess the effectivness of RMDL, scrapped and preprocessed stack overflow classification dataset \n",
    "(link : \"https://www.kaggle.com/stackoverflow/stacksample?select=Questions.csv\")\n",
    "* To compare the performance of RMDL, trained ROBERTa BERT model on the above-mentioned datasets.\n",
    "*********************************************************************************************************************\n",
    "#### CONTEXT OF THE PROBLEM:\n",
    "* The continually increasing number of complex datasets each year necessitates ever improving machine learning methods for robust and accurate categorization of these data.\n",
    "* Generally, deep learning models involves a lot of randomization\n",
    "* Users need to manually do hyper parameter tuning by changing each and every parameter which results into longer execution times\n",
    "* So, They proposed an ensemble based approach for deep learning models.\n",
    "*********************************************************************************************************************\n",
    "#### SOLUTION:\n",
    "* The proposed approach uses a basic concept of randomization\n",
    "* It asks for max and min no of nodes user wants to train their neural network on \n",
    "* It builds the RMDL architecture using DNN + RNN + CNN vertically stacked and gives the prediction using a voting classifier which is created using the predictions from all these models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "*********************************************************************************************************************\n",
    "#### In this paper they divided their 'Related Work' into 3 parts :\n",
    "\n",
    "#### Feature Extraction \n",
    "\n",
    "|Reference|Explanation|Dataset/Input|Weakness|\n",
    "|------|------|------|------|\n",
    "|L. Krueger et. al.[1]|feature extraction methods based on word counting for text categorization in statistical learning  |pattern recognition dataset |weighing the words prove to be inconsistent with text classificcation datasets |\n",
    "| G. Salton et.al. [2]|Created the concept of TF-IDF by weighing the frequncy counts  |NPL (National Physical Laboratory) collection of 11429 documents |normalized TF-IDF did not perform better  |\n",
    "\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### Classification methods and techniques\n",
    "\n",
    "|Reference|Explanation|Dataset/Input|Weakness|\n",
    "|------|------|------|------|\n",
    "|K. Murphy [3],I. Rish [4]|introduced Naive Bayes Classifier (NBC) and its empirical analysis| introductory paper where derivations are provided for document classification using naive bayes|numerical underflow with probabilistic models and data characterstics requirements|\n",
    "|C. Yu et.al [5],S. Tong et. al.[6] |SVM with active learning and latent variables|Reuters, NewsGroup data|simple method is computationally fast than hybrid ones|\n",
    "\n",
    "*********************************************************************************************************************\n",
    "\n",
    "\n",
    "#### Deep learning for classification\n",
    "\n",
    "| Reference |Explanation |  Dataset/Input |Weakness|\n",
    "| --- | --- | --- | --- |\n",
    "|D. Cires [7] |multi column deep neural networks for classification tasks |GTSRB (German test sign classification)|seccond best model in the competition and planning to embed in more general system|\n",
    "|K. Kowsari et. al.[8]|Hierarchical Deep learning for text classification|WOS datasets|present results with only one dataset and more hierarchy can be added to models|\n",
    "|(Implemented Paper)RMDL[9]|a new ensemble, deep learning approach for classification |Reuters, IMDB, and Reuters|Computationally expensive, excessive randomization, Uses same model for image,text and face recognition|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "*********************************************************************************************************************\n",
    "#### Basic Details of RMDL :\n",
    "* The novelty of this work is in using multi random deep learning models including DNN, RNN, and CNN for text.\n",
    "* DNN, RNN, and CNN are trained in parallel.\n",
    "* Can be used in any kind of dataset for classification i.e. not just text , it can be extended to image as well.\n",
    "* DNN uses TF-IDF and (RNN,CNN) uses Glove embedding\n",
    "* r + c + d = n (n layer RMDL where r is RNN layers, d is DNN layers and c is CNN layers)\n",
    "* after all RDL models are trained , the final prediction is calculated using majority votes of these models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"images/rmdl_voting.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='images/rmdl_voting.png', width=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"images/rmdl_archi.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='images/rmdl_archi.png', width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "********************************************************************************************************************\n",
    "#### PART - 1: RMDL  Paper  Replication\n",
    "#### PART - 2: EDA of Stack Overflow Dataset\n",
    "#### PART - 3: Training of Roberta BERT model for comparison with RMDL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NECESSARY LIBRARIES\n",
    "NOTE : If running RMDL as a library so please installit using pip command\n",
    "! pip install RMDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)\n",
      "sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import nltk\n",
    "nltk.download(\"reuters\")\n",
    "from RMDL import text_feature_extraction as txt\n",
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "from nltk.corpus import reuters\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from RMDL import RMDL_Text as rmdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sets Loading ....\n",
    "* IMDB : 50,000 documents with 2 classes\n",
    "* Reuters : 21,578 documents with 90 categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation \n",
    "Using Keras library to load the imdb dataset with maximum unique words as 1000 and cleaning the text using RMDL library 'text_cleaner'function and preparing the training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB dataset....\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading IMDB dataset....\")\n",
    "MAX_NB_WORDS = 1000\n",
    "(X_train_i, y_train_i), (X_test_i, y_test_i) = imdb.load_data(num_words=MAX_NB_WORDS)\n",
    "#print(len(X_train))\n",
    "#print(y_test)\n",
    "word_index = imdb.get_word_index()\n",
    "index_word = {v: k for k, v in word_index.items()}\n",
    "X_train_i = [txt.text_cleaner(' '.join(index_word.get(w) for w in x)) for x in X_train_i]\n",
    "X_test_i = [txt.text_cleaner(' '.join(index_word.get(w) for w in x)) for x in X_test_i]\n",
    "X_train_i = np.array(X_train_i)\n",
    "X_train_i = np.array(X_train_i).ravel()\n",
    "X_test_i = np.array(X_test_i[:50])\n",
    "X_test_i = np.array(X_test_i).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "Reuters dataset is downloaded with the help of NLTK library, it contains 21578 documents with 90 categories. Used multi label binarizer to transform the labels to one hot encoded so that it can be fed into RMDL models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Reuters Dataset ......\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Reuters Dataset ......\")\n",
    "documents = reuters.fileids()\n",
    "train_docs_id = list(filter(lambda doc: doc.startswith(\"train\"),documents))\n",
    "test_docs_id = list(filter(lambda doc: doc.startswith(\"test\"),documents))\n",
    "X_train_r = [(reuters.raw(doc_id)) for doc_id in train_docs_id]\n",
    "X_test_r = [(reuters.raw(doc_id)) for doc_id in test_docs_id]\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train_r = mlb.fit_transform([reuters.categories(doc_id)\n",
    "                           for doc_id in train_docs_id])\n",
    "y_test_r = mlb.transform([reuters.categories(doc_id)\n",
    "                      for doc_id in test_docs_id])\n",
    "y_train_r = np.argmax(y_train_r, axis=1)\n",
    "y_test_r = np.argmax(y_test_r, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the RMDL model on IMDB data \n",
    "*********************************************************************************************************************\n",
    "### Parameters Explanation :\n",
    "1. Input : x_train, y_train, x_test, y_test\n",
    "2. MAX_SEQUENCE_LENGTH : Maximum length of sequence or document in datasets, it will default to 500.\n",
    "3. MAX_NB_WORDS : Maximum number of unique words in datasets, it will default to 75000.\n",
    "4. GloVe_dir : Address of GloVe or any pre-trained directory, it will default to null which glove.6B.zip will be download\n",
    "5. GloVe_file: Which version of GloVe or pre-trained word emending will be used, it will default to glove.6B.50d.txt.\n",
    "6. random_deep : Number of ensembled model used in RMDL random_deep[0] is number of DNN, random_deep[1] is number of RNN, random_deep[2] is number of CNN\n",
    "7. epochs : Number of epochs in each ensembled model used in RMDL\n",
    "8. [min_hidden_layer_dnn, max_hidden_layer_dnn] : Ranges of layers user wants to experiment with\n",
    "9. [min_nodes_dnn,max_nodes_dnn] : Ranges of node value corresponding to DNN model\n",
    "10. random_optimizor : Boolean, if you wanna use random optimizers as well in RMDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done1\n",
      "tf-idf with 967 features\n",
      "/Users/macbookpro/Documents/GitHub/Named-Entity-Recognition-with-Bidirectional-LSTM-CNNs/embeddings/glove.6B.100d.txt\n",
      "Found 998 unique tokens.\n",
      "(25050, 50)\n",
      "Total 400000 word vectors.\n",
      "2\n",
      "DNN 0\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.90000, saving model to weights\\weights_DNN_0.hdf5\n",
      "RNN 0\n",
      "4\n",
      "90\n",
      "Train on 25000 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      " - 79s - loss: 0.7027 - accuracy: 0.5040 - val_loss: 0.6918 - val_accuracy: 0.5400\n",
      "(50, 2)\n",
      "Accuracy of 2 models: [0.9, 0.54]\n",
      "Accuracy: 0.9\n",
      "F1_Micro: (0.9, 0.9, 0.9, None)\n",
      "F1_Macro: (0.900974025974026, 0.8977455716586151, 0.898989898989899, None)\n",
      "F1_weighted: (0.9003246753246753, 0.9, 0.8997979797979798, None)\n"
     ]
    }
   ],
   "source": [
    "model_i = rmdl.Text_Classification(X_train_i, y_train_i, X_test_i,  y_test_i, batch_size=128,\n",
    "                        EMBEDDING_DIM=100,MAX_SEQUENCE_LENGTH = 50, MAX_NB_WORDS = 1000,\n",
    "                        GloVe_dir=\"/Users/macbookpro/Documents/GitHub/Named-Entity-Recognition-with-Bidirectional-LSTM-CNNs/embeddings/\",\n",
    "                 GloVe_file = \"glove.6B.100d.txt\",\n",
    "                        sparse_categorical=True, random_deep=[1, 1, 0], epochs=[1, 1, 1],  plot=False,\n",
    "                        min_hidden_layer_dnn=1, max_hidden_layer_dnn=8, min_nodes_dnn=128, max_nodes_dnn=256,\n",
    "                        min_hidden_layer_rnn=1, max_hidden_layer_rnn=5, min_nodes_rnn=32,  max_nodes_rnn=128,\n",
    "                        min_hidden_layer_cnn=3, max_hidden_layer_cnn=10, min_nodes_cnn=128, max_nodes_cnn=512,\n",
    "                        random_state=42, random_optimizor=False, dropout=0.5,no_of_classes=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar calling for Reuters Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "sparse_categorical = 0\n",
    "n_epochs = [120, 120, 120]  ## DNN--RNN-CNN\n",
    "Random_Deep = [3, 0, 0]  ## DNN--RNN-CNN\n",
    "model_r = rmdl.Text_Classification(X_train_r, y_train_r, X_test_r, y_test_r,\n",
    "                             batch_size=batch_size,\n",
    "                             sparse_categorical=True,\n",
    "                             random_deep=Random_Deep,\n",
    "                             epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********************************************************************************************************************\n",
    "### My Additions in the Project :\n",
    "* Scraped and Preprocessed the stack overflow data set from kaggle\n",
    "* Applied RMDL on this new data set to assess the effectiveness\n",
    "* Additionally, trained Roberta BERT model to compare the performance of RMDL model on all of these data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack Overflow Data Set EDA\n",
    "* The dataset is taken from kaggle (Question - answer data and labels : programming language)\n",
    "* Initial Distribution looks like this :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "   <img=src=\"images/initial_dist.png\" width=\"700\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Preprocessing \n",
    "\n",
    "* preprocessed and converted into text classification dataset\n",
    "* For simplicity selected only 10,000 documents (1000 from each class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stack = pd.read_csv(\"data/stack_overflow_10000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stack = df_stack.drop(['Unnamed: 0','Unnamed: 0.1','Id'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation :\n",
    "\n",
    "Data set is showing the 'Title' as questions asked on stack overflow 'body' as answers and 'Tags' as labels and 'Single_label' \n",
    "as category coded labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "      <th>single_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Face recognize using Opencv4Android SDK tutorial?</td>\n",
       "      <td>I am a student. Recently I've been building a ...</td>\n",
       "      <td>['android']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chat messages ordering strategy in PubNub</td>\n",
       "      <td>We are building a chat application in Android ...</td>\n",
       "      <td>['android']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Disappearance of R.java file</td>\n",
       "      <td>I was working on an android project and i pres...</td>\n",
       "      <td>['android']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Android GoogleMap V2 on zoom map doesn't get s...</td>\n",
       "      <td>I'm currently working on Android and have star...</td>\n",
       "      <td>['android']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Where is the App permission for \"identity\" in ...</td>\n",
       "      <td>I am trying to use an emulator with which come...</td>\n",
       "      <td>['android']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Face recognize using Opencv4Android SDK tutorial?   \n",
       "1          Chat messages ordering strategy in PubNub   \n",
       "2                       Disappearance of R.java file   \n",
       "3  Android GoogleMap V2 on zoom map doesn't get s...   \n",
       "4  Where is the App permission for \"identity\" in ...   \n",
       "\n",
       "                                                Body         Tags  \\\n",
       "0  I am a student. Recently I've been building a ...  ['android']   \n",
       "1  We are building a chat application in Android ...  ['android']   \n",
       "2  I was working on an android project and i pres...  ['android']   \n",
       "3  I'm currently working on Android and have star...  ['android']   \n",
       "4  I am trying to use an emulator with which come...  ['android']   \n",
       "\n",
       "   single_label  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stack.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model with stack overflow data\n",
    "*******************************************************************************************************************************\n",
    "To assess the effectiveness of RMDL methodology, I have applied the same model configuration on the above-mentioned data set. Complete implementation is present in src folder.\n",
    "\n",
    "* RMDL model testing : refer [https://github.com/garima2751/NLP_Project/blob/main/Src/stackoverflowRMDL.ipynb](link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Roberta BERT model over all the data sets\n",
    "*******************************************************************************************************************************\n",
    "Roberta Model is trained using Simple transformers library and I have used fine tuned version of roberta BERT model and all the implementation is present in src folder in github repo. For more explanation i have mentioned urls for each data set training with roberta BERT.\n",
    "\n",
    "* For Stackoverflow Data : refer [https://github.com/garima2751/NLP_Project/blob/main/Src/Roberta_stack.ipynb](link)\n",
    "* For IMDB : refer [https://github.com/garima2751/NLP_Project/blob/main/Src/Roberta_imdb.ipynb](link)\n",
    "* For Reuters : refer [https://github.com/garima2751/NLP_Project/blob/main/Src/Roberta_Reuters.ipynb](link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results :\n",
    "*******************************************************************************************************************************\n",
    "For Results each dataset is trained with RMDL and RobertaBERT model and comparison is present in the form of accuracy, f1-macro and f1-weighted. RMDL paper only shows accuarcy in their results however I feel f1-score should be a better metric to assess the performance of text classification tasks.\n",
    "\n",
    "#### Observations :\n",
    "*******************************************************************************************************************************\n",
    "* For IMDB Data: both the models perform equally well\n",
    "* For Reuters Data : there were 90 categories present in the document and RMDL shows better f1 score compare to Roberta BERT.\n",
    "* For Stack overflow data : Roberta BERT performs well however the execution time was 13+ hours.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"images/nlp_result_table.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='images/nlp_result_table.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphical Representation of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"images/nlp_plot_result.png\" width=\"900\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='images/nlp_plot_result.png', width=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and Future Direction :\n",
    "*******************************************************************************************************************************\n",
    "#### Learnings : \n",
    "During this project, I learnt how to replicate a paper using github repo and make changes in the source code of a package. RMDL was already present as a python package however I cloned the repo made some changes in the source code for the smooth working of code. I also learnt how we can vertically stack together various deep learning model layers and randomize the input layer and number of nodes.\n",
    "\n",
    "* RMDL source code was changed to reduce the randomness in code as the hardware requirements was limited to implement the RMDL models.\n",
    "* Learnt the mechanism of building a voting classifier with random multi model deep learning\n",
    "* Managed to train the Roberta BERT Model with simple transformers library, which was really easy to code with the help of google colab and simple transformers library functions.\n",
    "* Major learning would be that RMDL often tries every combination of layers and input nodes so that training and execution times become uncontrollable.\n",
    "* RMDL also gives different model with same set of input which is a big disadvantage for experimentation purpose.\n",
    "* Same with Roberta BERT model, as it takes 13h+ times to train on a data set with more than 10,000 rows\n",
    "*******************************************************************************************************************************\n",
    "#### Results Discussion :\n",
    "Results depicted in the above graphs summarizes the performance of both the models with each data set. My goal was to see whether newly released fine tuned BERT model is able to perform better or not compare to RMDL models.\n",
    "\n",
    "* In case of stack overflow dataset : BERT model outperformes RMDL models with an f1-score of 80 %\n",
    "* In case of IMDB dataset : Both models perform equally well\n",
    "* In case of Reuters dataset : RMDL perform slightly better than BERT \n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Limitations :\n",
    "In terms of Limitations, RMDL methodology has following problems:\n",
    "\n",
    "* Longer execution times because of every parameter is randomized in the methodology. for example if we are providing the range of DNN nodes as [3,8] then it will create DNN models with all the possible combinations of the range provided. Imagine doing for RNN and CNN as well so you end up losing all your RAM in the training of RMDL model.\n",
    "\n",
    "* Although RMDL claims to improve the accuracy and robustness of models however they have only worked with pretty standard datasets and did not provide the f1-scores for the text classification datasets and accuracies shown in the paper is achieveable using standalone deep learning architectures or BERT models\n",
    "\n",
    "* Randomization of deep learning models results into different models everytime when you try to run RMDL for the same input.\n",
    "\n",
    "In terms of BERT training the limitations are as follows :\n",
    "\n",
    "* Cannot pad the sentences or input after a certain limit and for longer sentence it is purely loss in information\n",
    "* Hardware limitations also leads to inability in training efficient BERT models (RAM availability)\n",
    "* Training or execution time is endless for more than 10,000 rows.\n",
    "\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Future Extension :\n",
    "For future considerations, RMDL can be implemented with new set of embeddings and feature sets such as ELMo, BERT and fastText.\n",
    "RMDL can also be extended to do the extensive hyperparameter tuning by trying every possible parameters and provides the optimal parameters with the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "[1]:  Krueger, L. E., & Shapiro, R. G. (1979). Letter detection with rapid serial visual presentation: Evidence against word superiority at feature extraction. Journal of Experimental Psychology: Human Perception and Performance, 5(4), 657.\n",
    "\n",
    "[2]:  Salton, G., & Buckley, C. (1987). Term weighting approaches in automatic text retrieval. Cornell University.\n",
    "\n",
    "[3]: Murphy, K. P. (2006). Naive bayes classifiers. University of British Columbia, 18(60)\n",
    "\n",
    "[4]: Rish, I. (2001, August). An empirical study of the naive Bayes classifier. In IJCAI 2001 workshop on empirical methods in artificial intelligence (Vol. 3, No. 22, pp. 41-46)\n",
    "\n",
    "[5]: Yu, C. N. J., & Joachims, T. (2009, June). Learning structural svms with latent variables. In Proceedings of the 26th annual international conference on machine learning (pp. 1169-1176).\n",
    "\n",
    "[6]: ong, S., & Koller, D. (2001). Support vector machine active learning with applications to text classification. Journal of machine learning research, 2(Nov), 45-66.\n",
    "\n",
    "[7]: CireAan, D., Meier, U., Masci, J., & Schmidhuber, J. (2012). Multi-column deep neural network for traffic sign classification. Neural networks, 32, 333-338.\n",
    "\n",
    "[8]: Kowsari, K., Brown, D. E., Heidarysafa, M., Meimandi, K. J., Gerber, M. S., & Barnes, L. E. (2017, December). Hdltex: Hierarchical deep learning for text classification. In 2017 16th IEEE international conference on machine learning and applications (ICMLA) (pp. 364-371). IEEE\n",
    "\n",
    "[9]: Kowsari, K., Heidarysafa, M., Brown, D. E., Meimandi, K. J., & Barnes, L. E. (2018, April). Rmdl: Random multimodel deep learning for classification. In Proceedings of the 2nd International Conference on Information System and Data Mining (pp. 19-28)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
