{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pun Generation\n",
    "\n",
    "#### Members Names:\n",
    " \n",
    "Kevin ioi <br>\n",
    "Dougall Percival\n",
    "\n",
    "https://github.com/KevinIoi/Puns_Generation.git\n",
    "\n",
    "#### Members Emails:\n",
    "kevin.ioi@ryerson.ca <br>\n",
    "dougall.percival@ryerson.ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "#### Problem Description:\n",
    "\n",
    "The broad topic that will be discussed in this notebook is the puzzle of designing a machine learning algorithm that is able be (or mimic being) *creative*. That is to say, teach a computer to produce creative content. Specifically, the researchers have taken on the problem of automated pun generation.\n",
    "\n",
    "#### Context of the Problem:\n",
    "\n",
    "In recent years, the power of current machine learning algorithms has become immense. Researchers have figured out how to design algorithms that can learn and solve incredilbly complex tasks. However, one domain that remains out of reach, artificial creativity. Part of the difficulty is that creative tasks are by definition, not quantifiable. Thus there is no obvious formula to fall back on when trying to frame the problem.\n",
    "\n",
    "Puns in particular are seemingly good candidates for early research into the area. They are essentially regular sentence that have been tweaked to be humorous. A regular sentence can be converted to a pun by cleverly replacing a word or two with another.\n",
    "\n",
    "Regular sentence (maybe a little dramatic):\n",
    "> Yesterday I accidentally swallowed some food colouring. The doctor says I'm OK, but I feel like I've **died** a little inside.\n",
    "\n",
    "Pun sentence:\n",
    "> Yesterday I accidentally swallowed some food colouring. The doctor says I'm OK, but I feel like I've **dyed** a little inside.\n",
    "\n",
    "This of course requires an understanding of human lanuage, which is something that machines are getting quite good at. Again, the primary problem is inserting creativity into the process, in this case with the end goal of being funny. One difficulty with this is that generally text generation requires a large corpus of training data, and there is no such corpus of puns.\n",
    "\n",
    "#### Limitation About other Approaches:\n",
    "\n",
    "The first, and fairly major limitation, is that there is not a large dataset of existing puns. SemEval offers a small dataset, but it cannot be considered a full corpus of puns.  \n",
    "The recent paper by Yu et al., proposed a language model, jointly decoding conditioned on pun/alternate words, adding ambiguity to sentences. However, the paper by Kao et al. [4] already demonstrated that ambiguity is not enough to add humour to a sentence. The meanings must also support one another across the entire phrase. Both of these papers are used as baselines to compare against the SurGen method.\n",
    "\n",
    "#### Solution:\n",
    "\n",
    "This paper (Pun Generation with Surprise [6]) approaches the problem by defining the idea of 'global-local surprisal' as the driving factor behind humour. They attempt to alter 'unhumorous' sentences, by swapping out a keyword with a homophone and utilizing multiple language models to maximize this surprisal metric while maintaining a logically fluid sentence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Explain the related work using the following table\n",
    "\n",
    "| Reference |Explanation |  Dataset/Input |Weakness\n",
    "| --- | --- | --- | --- |\n",
    "| Yu et al. [1] | Train a sequence-to-sequence neural model on unhumorous text. Then use a decoder conditioned on two wordsense (pun and regular word) to create the pun  | English Wikipedia | The encoded sentence doesn't always have the capcity to merge with both primary and secondary word senses\n",
    "| Petrovic & Matthews [2] | Apply rules to identify word combinations that will create humor when inserted into a template sentence | Google N-gram data | Very restrictive model with structured output. Lacks the flexibility to be consistenly humorous and gramatically correct. 16% sucess rate\n",
    "| Fuli et al. [5]| Use and adversarial learning approach to teach a text seq-to-seq generator to make sentences using two word senses and create believable ambiguity| English  Wikipedia, SemEval 2017 | very well done, although it is still effectively just altering provided sentences. Complete sentence generation would be interesting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "As previously discussed the motivation for the proposed system is that a good pun sentence is the result of a 'pun' word being more likely given the overall context of the sentence, while the normal word is more likely in the local context. The local context is defined as with in a 3-5 word span of the target word.\n",
    "<br><br>\n",
    "They call this metric: 'Global-Local surprise' which is a ratio of the global and local surprisal scores. The surprisal scores are calculated as:<br>\n",
    "\\begin{equation*}\n",
    "S(c) = -\\log \\dfrac{P(w^{pun}|c)}{P(w^{alter}|c)} = -\\log \\dfrac{P(w^{pun},c)}{P(w^{alter},c)}\n",
    "\\end{equation*}\n",
    "Where the context (c) will change to define local or global context.<br>\n",
    "Using that they define their Global-Local surprise as :\\begin{equation*}\n",
    "S(c)_{Global-Local} = -\\log \\dfrac{S(c_{local}}{S(c_{global}}\n",
    "\\end{equation*}<br>\n",
    "Where a higher score theoretically indicates a better pun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 primary components to the system:\n",
    "1. A list of target words and homophones (pun words), as well as a large corpus of unhumorous text\n",
    "2. The retriever, which retrieves potential sentences from the unhumorous corpus based on our target words\n",
    "3. The a Skip-gram model, which is used to identify topics related to our new pun\n",
    "4. A Neural smoother which will 'smooth' the sentence after the new topic is inserted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Corpuses\n",
    "\n",
    "- Bookcorpus is used as the text corpus of generally unhumorous sentences, which will be the 'templates' for our puns\n",
    "- SemEval 2017, was used as a list of target words and homophones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Retriever\n",
    "\n",
    "To begin, a **Retriever** vectorizes the corpus into a TF-IDF matrix, providing a way to identify similar words based on a given keyword, and seed sentences based on pun keywords.\n",
    "Given an alternative word (word that could be replaced by a pun word), the retriever returns 500 candidates. The top 100 are taken as seed sentences. Seed sentences are sentences with the potential to be transformed into puns. They come from a large, generic corpus (in this case, all of the books). These sentences satisfy three requirements: a strong association between alternative word and the local context; strong association between pun word and distant context; both words are interpretable given local and global context to maintain ambiguity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each template retrieved, the target word is swapped out for the pun word "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip-gram\n",
    "\n",
    "Then to change the global topic of the sentence a Skip-gram model is used to find words that are similar to the newly added pun word\n",
    "\n",
    "A Skip-gram model is used to identify relatedness between two words. In the case of pun generation it is used to identify relations between an alternate word, near the end of a seed sentence (word to be replaced in the pun), and k candidate topic words, at the beginning of the sentence. \n",
    "Candidate topic words are selected using a “distant” skip-gram model. This model maximizes *pθ(wj|wi)*, for all *wi, wj* in a sentence between *d1* and *d2* words apart. See equation below.  \n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{j=1-d_1}^{i-d_2} \\log p_0(w_j|w_i) + \\sum_{j=1+d_1}^{i+d_2} \\log p_0(w_j|w_i) \n",
    "\\end{equation*}\n",
    "\n",
    "From this model, the top k predictions from *pθ(w|wp)*, where *wp* is the pun word, and *w* is the candidate topic to be further filtered. In this paper, Skip-gram with Negative Sampling (SGNS) is implemented. This reduces the training time for the Skipgram Model by feeding it many negative examples, rather than the few positive examples that would exist for any word-context pair.   \n",
    "*This model was trained on d1 = 5, d2 = 10. Embedding size was 300, and 15 epochs were run.*  \n",
    "\n",
    "\n",
    "A further check is performed on the candidate topic word, to ensure it is type consistent. Since verbs have more chance of causing a nonsense sentence, the swap words are constrained to nouns and pronouns. Wordnet Synsets are then used (with word/POS tag pair) to ensure the candidate word, and its possible replacement are above a certain similarity threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Smoother\n",
    "\n",
    "Given the pun sentence and a new topic, the neural smoother will insert the topic into the sentence 'smoothly'\n",
    "\n",
    "The Neural Smoother is an encoder-decoder model that is used to 'smooth' the insertion of the new topic word. Both the encoder and decoders are single layer LSTMs with an attention module connecting the two. To train the model, they select target words in each sentence of a large corpus and delete the surrounding words, inserting a placeholder token. The model is then tasked to predict the deleted words based on the context of the target word and the rest of the sentence.\n",
    "For example:\n",
    "\n",
    "    Original sentence: \"Dave was enjoying the lovely summer weather\"\n",
    "    Target Word: 'enjoying'\n",
    "    Removed tokens: ['was', 'the']\n",
    "    \n",
    "    Input  -> \"Dave <placeholder> enjoying <placeholder> lovely summer weather\"\n",
    "    Target output -> ['was', 'the']\n",
    "\n",
    "In the system, this model is given the location of a token to remove and a topic word to insert. The system swaps out the original token with the new topic and deletes the words surrounding it. It then passes the sentence to the smoother which predicts words that are most likely to surround it, thus 'smoothing' the sentence around the new topic.\n",
    "\n",
    "Here is an example of the architecture, however the one in the image has multiple RNN layers in the encoder and decoder:\n",
    "<img src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Encoder-Decoder-Architecture-for-Neural-Machine-Translation.png\" alt=\"drawing\" width=\"500\"/>\n",
    "Image obtained from:<br>\n",
    "Denny Britz, Anna Goldie, Minh-Thang Luong, & Quoc Le. (2017). Massive Exploration of Neural Machine Translation Architectures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********************************************************************************************************************  \n",
    "*********************************************************************************************************************\n",
    "\n",
    "# Installation Note\n",
    "Running this notebook requires a very specific setup, including Python version 3.6, and PyTorch version 0.4.0. Here is the information for setting up the environment properly.  \n",
    "First, navigate to the downloaded pungen folder.  \n",
    "If you are using Anaconda, run the following command (where envname is your environment name):  \n",
    "  \n",
    "*conda env create --name envname --file=environments.yml*\n",
    "\n",
    "If you are using venv, you can install packages via pip. After creating a new venv, use:  \n",
    "\n",
    "*pip install -r requirements.txt*\n",
    "\n",
    "This will install the needed packages.  \n",
    "\n",
    "## fairseq Installation\n",
    "When installing the package fairseq, note that the distribution being installed is not the most up to date version. The paper's original authors actually edited modules in fairseq, and have packaged their version on their github. While the requirements.txt file points to this distribution, to install the followings commands may be used.\n",
    "\n",
    "*git clone -b pungen https://github.com/hhexiy/fairseq.git  \n",
    "cd fairseq  \n",
    "pip install -r requirements.txt  \n",
    "python setup.py build develop*\n",
    "\n",
    "## Directory structure/pretrained models\n",
    "To ensure the directory structure fits for the program implementation and so avoid having to train very large models it is recommended you download our premade folder. The Wikitext model below will have to be aquired separately. https://drive.google.com/a/ryerson.ca/file/d/1Wmh7gbgxZV6GEPEl5F4Net8d4cHE3H7b/view?usp=sharing\n",
    "\n",
    "\n",
    "## Wikitext Model\n",
    "One of the required models, Wikitext-103, comes from the actual fairseq site. To setup properly, use these commands:\n",
    "\n",
    "*curl --create-dirs --output models/wikitext/model https://dl.fbaipublicfiles.com/fairseq/models/wiki103_fconv_lm.tar.bz2  \n",
    "tar xjf models/wikitext/model -C models/wikitext  \n",
    "rm models/wikitext/model*  \n",
    "\n",
    "## Other requirements\n",
    "We have tried to packages as much of the required material as possible here. Any missing files will be generated at run-time.\n",
    "\n",
    "## Possible Issues\n",
    "When trying to set up the required environment on Windows, my computer ran into compiler issues, and was unable to complete the setup. The solution was to set up Ubuntu on a virtual box to set up properly. I gave my Virtual Box 4Gb of RAM and 50Gb of storage, and it was sufficient for this task.  \n",
    "\n",
    "The original source of data for this project (smashwords.com) no longer allows users to fully download the corpus. As such, only a sample of the original source was available for this implementation.  \n",
    "\n",
    "*********************************************************************************************************************\n",
    "*********************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever\n",
    "The first step is to create a retriever.  \n",
    "\n",
    "See methodology section for full explanation. The retriever creates a TF-IDF matrix of words in the corpus, and generates a list of \"seed sentences\", identifying sentences where a pun can likely be created. The retriever also permits querying, based on a keyword, to find similar words. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from functools import total_ordering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from enum import IntEnum\n",
    "from utility import sentence_iterator\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('pungen')\n",
    "\n",
    "# establish retriever and Template classes\n",
    "# retriever is essentially a TFIDF vectorized matrix for our book corpus\n",
    "\n",
    "@total_ordering\n",
    "class Template(object):\n",
    "    def __init__(self, tokens, keyword, id_):\n",
    "        self.id = int(id_)\n",
    "        self.tokens = tokens\n",
    "        self.keyword_positions = [i for i, w in enumerate(tokens) if w == keyword]\n",
    "        self.num_key = len(self.keyword_positions)\n",
    "        self.keyword_id = None if self.num_key == 0 else max(self.keyword_positions)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def replace_keyword(self, word):\n",
    "        tokens = list(self.tokens)\n",
    "        tokens[self.keyword_id] = word\n",
    "        return tokens\n",
    "\n",
    "    def __str__(self):\n",
    "        return ' '.join(['[{}]'.format(w) if i == self.keyword_id else w for i, w in enumerate(self.tokens)])\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        # Containing keyword is better\n",
    "        if self.num_key == 0:\n",
    "            return True\n",
    "        # Fewer keywords is better\n",
    "        if self.num_key > other.num_key:\n",
    "            return True\n",
    "        # Later keywords is better\n",
    "        if self.keyword_id < other.keyword_id:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if self.num_key == 0 and other.num_key == 0:\n",
    "            return True\n",
    "        if self.num_key == other.num_key and self.keyword_id == other.keyword_id:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "class Retriever(object):\n",
    "    def __init__(self, doc_files, path=None, overwrite=False):\n",
    "        logger.info('reading retriever docs from {}'.format(' '.join(doc_files)))\n",
    "        self.docs = [line.strip() for line in open(doc_files[0], 'r')]\n",
    "\n",
    "        if overwrite or (path is None or not os.path.exists(path)):\n",
    "            logger.info('building retriever index')\n",
    "            self.vectorizer = TfidfVectorizer(analyzer=str.split)\n",
    "            self.tfidf_matrix = self.vectorizer.fit_transform(self.docs)\n",
    "            if path is not None:\n",
    "                self.save(path)\n",
    "        else:\n",
    "            logger.info('loading retriever index from {}'.format(path))\n",
    "            with open(path, 'rb') as fin:\n",
    "                obj = pickle.load(fin)\n",
    "                self.vectorizer = obj['vectorizer']\n",
    "                self.tfidf_matrix = obj['tfidf_mat']\n",
    "\n",
    "    def save(self, path):\n",
    "        with open(path, 'wb') as fout:\n",
    "            obj = {\n",
    "                    'vectorizer': self.vectorizer,\n",
    "                    'tfidf_mat': self.tfidf_matrix,\n",
    "                    }\n",
    "            pickle.dump(obj, fout)\n",
    "\n",
    "    def query(self, keywords, k=1):\n",
    "        features = self.vectorizer.transform([keywords])\n",
    "        scores = self.tfidf_matrix * features.T\n",
    "        scores = scores.todense()\n",
    "        scores = np.squeeze(np.array(scores), axis=1)\n",
    "        ids = np.argsort(scores)[-k:][::-1]\n",
    "        return ids\n",
    "\n",
    "    def valid_template(self, template):\n",
    "        return template.num_key == 1\n",
    "\n",
    "    def retrieve_pun_template(self, alter_word, len_threshold=10, pos_threshold=0.5, num_cands=500, num_templates=None):\n",
    "        ids = self.query(alter_word, num_cands)\n",
    "        templates = [Template(self.docs[id_].split(), alter_word, id_) for id_ in ids]\n",
    "        templates = [t for t in templates if t.num_key > 0 and len(t.tokens) > len_threshold]\n",
    "        if len(templates) == 0:\n",
    "            logger.info('FAIL: no retrieved sentence contains the keyword {}.'.format(alter_word))\n",
    "            return []\n",
    "\n",
    "        valid_templates = [t for t in templates if self.valid_template(t)]\n",
    "        if len(valid_templates) == 0:\n",
    "            valid_templates = templates\n",
    "        templates = sorted(valid_templates, reverse=True)[:num_templates]\n",
    "        return templates\n",
    "    \n",
    "# helper function\n",
    "\n",
    "Word = IntEnum('Word', [(x, i) for i, x in enumerate('TOKEN LEMMA TAG'.split())])   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a file of tokenized sentences\n",
    "input_file = \"data/train.txt\"\n",
    "output_file = \"data/bookcorpus/raw/sent.tokenized.txt\"\n",
    "ner = False\n",
    "\n",
    "with open(output_file, 'w') as fout:\n",
    "    for s in sentence_iterator(input_file):\n",
    "        if ner:\n",
    "            ss = []\n",
    "            [ss.extend(w[Word.TOKEN].split('_')) for w in s]\n",
    "            ss = ' '.join([x.lower() for x in ss])\n",
    "        else:\n",
    "            ss = ' '.join([w[Word.TOKEN] for w in s])\n",
    "        fout.write(ss + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply retriever\n",
    "doc_file = \"data/bookcorpus/raw/sent.tokenized.txt\"\n",
    "ret_file = \"retriever/our_retriever.pkl\"\n",
    "overwrite = True\n",
    "\n",
    "# generate the retreiver\n",
    "retriever = Retriever([doc_file], ret_file, overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Smoother (Combiner)\n",
    "\n",
    "The Neural Smoother is an encoder-decoder model that is used to 'smooth' the insertion of the new topic word. Both the encoder and decoders are single layer LSTMs with an attention module connecting the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess training data\n",
    "\n",
    "The Training data are sentences that have had 3 sequential words removed and replaced with the token <placeholder\\> <br>\n",
    "\n",
    "Like this:<br>\n",
    "- Original sentence: \"Dave **was enjoying the** lovely summer weather\"<br>\n",
    "- Altered sentence: \"Dave **<placeholder\\>** lovely summer weather\"<br>\n",
    "- Removed tokens: ['was', 'enjoying', 'the']<br>\n",
    "<br>\n",
    "\n",
    "The middle word (enjoying) is saved as the 'target' word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# helper functions\n",
    "#\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from enum import IntEnum\n",
    "\n",
    "# quick way to break the word chunks we created\n",
    "Word = IntEnum('Word', [(x, i) for i, x in enumerate('TOKEN LEMMA TAG'.split())])\n",
    "\n",
    "def split_sent(words, delete_frac=0.5, window_size=1):\n",
    "    \"\"\"\n",
    "        Finds a word good candidate word to smooth around\n",
    "        Removes the candidate word and words within 'window_size' range around it\n",
    "        Replaces them with <placeholder> token\n",
    "        \n",
    "        returns:\n",
    "        - The altered sentence (template)\n",
    "        - The key word chosen to smooth around\n",
    "        - The deleted words\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(words)\n",
    "    n = max(1, int(delete_frac * N))\n",
    "    deleted_keyword = None\n",
    "\n",
    "    # find an appropriate word to delete, in the sentence provided\n",
    "    for i, w in enumerate(words):\n",
    "        if i < n and w[Word.TAG] in ('NOUN', 'PROPN', 'PRON') and not w[Word.TOKEN] in STOP_WORDS:\n",
    "            left, right = i, i+1\n",
    "            deleted_keyword = w[Word.TOKEN]\n",
    "\n",
    "            w = window_size\n",
    "            left = max(0, i - w)\n",
    "            right = min(N, i + w + 1)\n",
    "    \n",
    "    # didn't find any good candidate words\n",
    "    if not deleted_keyword:\n",
    "        return None, None, None\n",
    "\n",
    "    # makes a 'Template' sentence with the target word and one word on either side removed\n",
    "    template = [w[Word.TOKEN] for w in words[:left]] + ['<placeholder>'] + [w[Word.TOKEN] for w in words[right:]]\n",
    "\n",
    "    # holds the words that were deleted\n",
    "    deleted = [w[Word.TOKEN] for w in words[left:right]]\n",
    "\n",
    "    return template, deleted_keyword, deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Example of preprocessing\n",
    "#raw_sentence_iterator = nlp.pipe([' '.join(t.tokens) for t in templates])\n",
    "import utility\n",
    "import spacy\n",
    "from spacy.symbols import ORTH, LEMMA, POS, TAG\n",
    "from fairseq import tokenizer\n",
    "nlp = spacy.load('en_core_web_sm', disaable=['ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:\n",
      " Dave was enjoying the lovely summer weather\n",
      "Pre'Preprocessing:\n",
      " [[Dave, 'Dave', 'PROPN'], [was, 'be', 'AUX'], [enjoying, 'enjoy', 'VERB'], [the, 'the', 'DET'], [lovely, 'lovely', 'ADJ'], [summer, 'summer', 'NOUN'], [weather, 'weather', 'NOUN']]\n",
      "\n",
      "\n",
      "Altered sentence:\n",
      " ['<placeholder>', enjoying, the, lovely, summer, weather]\n",
      "Target word:\n",
      " Dave\n",
      "Deleted words:\n",
      " [Dave, was]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Dave was enjoying the lovely summer weather\"\n",
    "\n",
    "# we need to pre'preprocess the sentence so each token is in format <token|lemma|pos>\n",
    "# we will do that using the spaCy package\n",
    "prepreprocess_sent = []\n",
    "for word in nlp(sentence):\n",
    "    prepreprocess_sent.append([word,word.lemma_,word.pos_])\n",
    "\n",
    "# prepreprocess_sent = \" \".join(prepreprocess_sent)\n",
    "print(\"Original sentence:\\n\",sentence)\n",
    "print(\"Pre'Preprocessing:\\n\",prepreprocess_sent)\n",
    "\n",
    "altered_sentence, target, deleted_words = split_sent(prepreprocess_sent)\n",
    "\n",
    "print(\"\\n\\nAltered sentence:\\n\",altered_sentence)\n",
    "print(\"Target word:\\n\",target)\n",
    "print(\"Deleted words:\\n\",deleted_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This cell will preprocess a corpus of data from a file\n",
    "#\n",
    "\n",
    "fileName = \"data/sampleCorpus.txt\"\n",
    "src_output = \"data/1split.src\"\n",
    "tgt_ouput = \"data/1split.tgt\"\n",
    "\n",
    "import os\n",
    "from utility import sentence_iterator\n",
    "\n",
    "# ensure output folder exists \n",
    "if not os.path.isdir('data'):os.mkdir('data')\n",
    "    \n",
    "# open file objects to print output\n",
    "fp_src = open(src_output, 'w')\n",
    "fp_tgt = open(tgt_ouput, 'w')\n",
    "\n",
    "# iterate through each line in input file\n",
    "for words in sentence_iterator(fileName):\n",
    "    \n",
    "    # selects target_word\n",
    "    # removes target_word and words on either side of it replacing them with <placeholder>\n",
    "    template, target_word, deleted = split_sent(words)\n",
    "    if not template:#failed to generate (probably empty line)\n",
    "        continue\n",
    "\n",
    "    # print target (words that were deleted)\n",
    "    fp_tgt.write('{}\\n'.format(' '.join(deleted)))\n",
    "\n",
    "    # print target_word, then sentence with <placeholder> on next line\n",
    "    fp_src.write('{target}\\n{temp}\\n'.format(\n",
    "        target=target_word,\n",
    "        temp=' '.join(template)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Smoother Training\n",
    "\n",
    "The model encodes the altered sentence and then conditioned on the 'Target' word the decoder tries to fill in the placeholder to recreate the sentence.\n",
    "\n",
    "Altered sentence: \"Dave <placeholder> lovely summer weather\"<br>\n",
    "Target token: 'enjoying'\n",
    "\n",
    "Hopfully the model will be able to learn how to 'smooth' sentences to make words fit into them. This takes a lot of training and a large corpus of text  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq import options, tasks, utils\n",
    "from fairseq.data import iterators\n",
    "from fairseq.trainer import Trainer\n",
    "from argparse import Namespace\n",
    "import utility\n",
    "import itertools\n",
    "\n",
    "# Setup fairseq task, controls/defines the training process\n",
    "task_args = Namespace(task='edit', data='data/', target_lang='tgt', source_lang='src',\n",
    "                      left_pad_source=True, left_pad_target=True, combine='token', insert='target',raw_text=True,\n",
    "                     max_source_positions=400,max_target_positions=400)\n",
    "task = tasks.setup_task(task_args)\n",
    "\n",
    "# have the task object load datasets\n",
    "utility.load_dataset_splits(task, ['split'])\n",
    "\n",
    "# Construct model\n",
    "# this is essentially just making an encoder-decoder model with single layer LSTMs and using attention\n",
    "model_args = Namespace(encoder_embed_dim= 512,encoder_hidden_size= 512,encoder_num_layers= 1,\n",
    "                    encoder_dropout_in= 0.1,encoder_dropout_out= 0.1,encoder_bidirectional= False,\n",
    "                    encoder_pretrained_embed= None,decoder_embed_dim= 512,decoder_hidden_size= 512,\n",
    "                    decoder_out_embed_dim= 512,decoder_num_layers= 1,decoder_dropout_in= 0.1,\n",
    "                    decoder_dropout_out= 0.1,decoder_attention= True,decoder_encoder_embed_dim= 512,\n",
    "                    decoder_encoder_output_units= 512,decoder_pretrained_embed= False)\n",
    "model = task.build_model(model_args)\n",
    "\n",
    "# essentially creates an object to evaluate loss \n",
    "criterion_args = Namespace(criterion='cross_entropy', sentence_avg=False)\n",
    "criterion = task.build_criterion(criterion_args)\n",
    "\n",
    "trainer = Trainer(args, task, model, criterion, dummy_batch)\n",
    "\n",
    "# Initialize dataloader\n",
    "epoch_itr = task.get_batch_iterator(\n",
    "    dataset=task.dataset('train'),\n",
    "    ignore_invalid_inputs=True,\n",
    "    required_batch_size_multiple=8,\n",
    "    seed=101)\n",
    "\n",
    "# train for all epochs\n",
    "while epoch_itr.epoch < max_epoch:\n",
    "    train(args, trainer, task, epoch_itr)\n",
    "    \n",
    "utils.save_state(\n",
    "    \"checkpoint_best.pt\", trainer.args, trainer.get_model(), trainer.criterion, trainer.optimizer,\n",
    "    trainer.lr_scheduler, trainer._num_updates, trainer._optim_history,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a trained smoother from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from fairseq.sequence_generator import SequenceGenerator\n",
    "from fairseq import tasks, utils\n",
    "\n",
    "def get_Smoother():\n",
    "    \n",
    "    # define task model will perform\n",
    "    task_args = Namespace(task='edit', data='combiner-data/',left_pad_source=True,\n",
    "                          left_pad_target=False,max_source_positions=1024,max_target_positions=1024,\n",
    "                          combine='embedding', insert='none', source_lang = 'src',target_lang = 'tgt', shard_id=0 )\n",
    "    task = tasks.setup_task(task_args)\n",
    "\n",
    "    # load model \n",
    "    models, model_args = utils.load_ensemble_for_inference([\"combiner/models/checkpoint_best.pt\"], task)\n",
    "    model = models[0]\n",
    "\n",
    "    # create sequence generator with model\n",
    "    generator = SequenceGenerator(\n",
    "                [model], task.target_dictionary, beam_size=20, stop_early=True,\n",
    "                normalize_scores=True, len_penalty=1,\n",
    "                unk_penalty=100.0, sampling=False, sampling_topk=-1,\n",
    "                sampling_temperature=1,minlen=1)\n",
    "    \n",
    "    return generator, task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pun Scoring Metric: Global-Local Surprise\n",
    "\n",
    "The scoring metric used in the paper focuses on maximizing the local surprise of the new pun word, while minimizing the global surprise\n",
    "<br>\n",
    "To measure surprise they use the probabilities of spans of tokens. To calculate this they utilize a neural language model from another paper \"Pointer sentinel mixture models.\"(Merity, 2016)\n",
    "\n",
    "<br>\n",
    "\n",
    "A basic unigram model is also used to calculate the unconditional probabilities of the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from fairseq.sequence_scorer import SequenceScorer\n",
    "from fairseq import options, tasks, utils\n",
    "from utility import LMScorer\n",
    "import os\n",
    "\n",
    "def getLanguageModel():\n",
    "    ''' Loads a pretrained language model, in a \"scorer\" wraper\n",
    "        Will be used to score the probablilty of a sentence\n",
    "        \n",
    "        This model is the product of another paper:\n",
    "            S. Merity, C. Xiong, J. Bradbury, and R. Socher. 2016.Pointer sentinel mixture models.\n",
    "            arXiv preprint arXiv:1609.07843.\n",
    "    '''\n",
    "    \n",
    "    # define the task for the model\n",
    "    scorer_task_args = Namespace(data=os.path.dirname(\"models/wikitext/\"), path=\"models/wikitext/\", cpu=True,\n",
    "    task='language_modeling',output_dictionary_size=-1, self_target=False, future_target=False, past_target=False)\n",
    "    task = tasks.setup_task(scorer_task_args)\n",
    "    \n",
    "    # load dictionary and weights\n",
    "    models, _ = utils.load_ensemble_for_inference([\"models/wikitext/wiki103.pt\"], task)\n",
    "\n",
    "    # regenerate model\n",
    "    scorer = SequenceScorer(models, task.target_dictionary)\n",
    "    languageModel = LMScorer(task, scorer)\n",
    "    \n",
    "    return languageModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramModel(object):\n",
    "    '''\n",
    "        Simple model to hold the unconditional probabilities of of words in our corpus\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, counts_path, oov_prob=0.03):\n",
    "        self.word_counts = self.load_model(counts_path)\n",
    "        self.total_count = sum(self.word_counts.values())\n",
    "        self.oov_prob = oov_prob # out-of-vocab probability\n",
    "        self._oov_smoothing_prob = self.oov_prob * (1. / self.total_count)\n",
    "\n",
    "    def load_model(self, dict_path):\n",
    "        counts = {}\n",
    "        with open(dict_path, 'r') as fin:\n",
    "            for line in fin:\n",
    "                ss = line.strip().split()\n",
    "                counts[ss[0]] = int(ss[1])\n",
    "        return counts\n",
    "\n",
    "    def _score(self, token):\n",
    "        p = self.word_counts.get(token, 0) / float(self.total_count)\n",
    "        smoothed_p = (1 - self.oov_prob) * p + self._oov_smoothing_prob\n",
    "        return np.log(smoothed_p)\n",
    "\n",
    "    def score(self, tokens):\n",
    "        return [self._score(token) for token in tokens]\n",
    "    \n",
    "def getUnigramModel(fileName=\"word-counts/dict.txt\"):\n",
    "    unigram_model = UnigramModel(fileName)\n",
    "    return unigram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surprisal_score(pun_sent, pun_word_index, alter_word, lm,um,local_window_size=2):\n",
    "    '''\n",
    "        The Metrix this paper uses to score puns\n",
    "    \n",
    "        Uses a condiational language model to estimate probability of spans of text\n",
    "    \n",
    "        Focuses on the full pun sentence for 'global surprise'\n",
    "        Focuses on the small 'local_window' around the pun word for 'local surprise'\n",
    "    '''\n",
    "    \n",
    "    # recreate original sentence\n",
    "    alter_sent = list(pun_sent)\n",
    "    alter_sent[pun_word_index] = alter_word\n",
    "\n",
    "    # define local context range\n",
    "    local_start = pun_word_index - local_window_size\n",
    "    local_end  = pun_word_index + local_window_size\n",
    "    local_pun_sent = pun_sent[local_start:local_end]\n",
    "    local_alter_sent = alter_sent[local_start:local_end]\n",
    "\n",
    "    # gather the different sentences\n",
    "    sents = [alter_sent, pun_sent, local_alter_sent, local_pun_sent]\n",
    "    \n",
    "    # get the probabilities of the sentences (log prob)\n",
    "    lm_scores = lm.score_sents(sents, tokenize=lambda x: x)\n",
    "    \n",
    "    # get unigram probability of pun sente\n",
    "    unigram_score = um.score(pun_sent)\n",
    "    \n",
    "    # calculate global and local surprise values \n",
    "    global_surprisal = np.sum(lm_scores[0]) - np.sum(lm_scores[1])\n",
    "    local_surprisal = np.sum(lm_scores[2]) - np.sum(lm_scores[3])\n",
    "\n",
    "    # grammar score, how much more likely the sentence is given context\n",
    "    grammar = (np.sum(lm_scores[1]) - np.sum(unigram_score)) / len(pun_sent)\n",
    "\n",
    "    if global_surprisal < 0 or local_surprisal < 0:\n",
    "        # discount sentences with negative surprisal scores\n",
    "        r = -1.\n",
    "    else:\n",
    "        # calc surprise ration\n",
    "        r = local_surprisal / global_surprisal  # larger is better\n",
    "    \n",
    "    return r+local_surprisal+global_surprisal+grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pun Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility import SkipGram\n",
    "import utility\n",
    "import spacy\n",
    "from spacy.symbols import ORTH, LEMMA, POS, TAG\n",
    "from fairseq import tokenizer\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  To generate a pun we first need a pair of homophones to use as the pun and alter words Then we need some sentences to try to turn into puns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He had come alone , and he was going to [die] .\n",
      "If we do n't , he or she may [die] .\n",
      "\" I 'm not going to let you [die] . \"\n",
      "And I 'm not going to let him [die] . \"\n",
      "\" You mean he 's not going to [die] ? \"\n",
      "And that way you would n't have to [die] at all . \"\n",
      ", he asked , \" How did he [die] ? \"\n",
      "He whispered , \" It 's to [die] for . \"\n",
      "\" I 'm going to [die] , are n't I ? \"\n",
      "\" And you will probably [die] just like her . \"\n"
     ]
    }
   ],
   "source": [
    "# define words\n",
    "alter_word = \"die\"\n",
    "pun_word = \"dye\"\n",
    "\n",
    "# initialize retriever, will get sentences containing our alter_word\n",
    "retriever = Retriever([\"tmp/train.tokenized.txt\"], \"retriever/retriever.pkl\", False)\n",
    "\n",
    "# pull sentences that contain the alter word\n",
    "retrieved_templates = retriever.retrieve_pun_template(alter_word, num_templates=20)\n",
    "pun_word_indicies = [t.keyword_id for t in retrieved_templates]\n",
    "\n",
    "sentences = []\n",
    "templates = []\n",
    "for t in retrieved_templates:\n",
    "    if t.tokens not in sentences:\n",
    "        templates.append(t)\n",
    "        sentences.append(t.tokens)\n",
    "\n",
    "# keep track of the index of the pun word in each sentence\n",
    "pun_word_indicies = [t.keyword_id for t in templates]\n",
    "        \n",
    "for i, template in enumerate(templates):\n",
    "    print(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now to find words to switch out to switch the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get positions of possible topic words to replace\n",
    "raw_sentence_iterator = nlp.pipe([' '.join(t.tokens) for t in templates])\n",
    "\n",
    "min_distance_to_topic = 0.1 # % of sentence length that should separate pun and topic \n",
    "topic_word_indicies = [] # locations of topics\n",
    "\n",
    "for i, sent in enumerate(raw_sentence_iterator):\n",
    "    topic_word_indicies.append([])\n",
    "    for j, word in enumerate(sent[:pun_word_indicies[i]-len(sent)*min_distance_to_topic]):\n",
    "        if word.pos_ in ('NOUN', 'PROPN', 'PRON'):# topics have to be these type of words\n",
    "            topic_word_indicies[-1].append(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 6], [1, 5], [1, 3], [1, 3], [1, 3], [2, 3], [1], [0, 4], [1], [2]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the indices of the word that will help switch the topic of the phrase\n",
    "topic_word_indicies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Putting the Puns together\n",
    "\n",
    "Lets only do once sentence and one replacment topic for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He had come alone , and he was going to die .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(templates[0].tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'had', 'come', 'alone', ',', 'and', 'he', 'was', 'going', 'to', 'dye', '.']\n"
     ]
    }
   ],
   "source": [
    "# get first topic word suggestion\n",
    "topic_word_index =topic_word_indicies[0][0]\n",
    "alter_sent = templates[0].tokens\n",
    "\n",
    "# replace the alter word with in the pun word\n",
    "pun_sent = templates[0].replace_keyword(pun_word)\n",
    "pun_word_index = template.keyword_id\n",
    "print(pun_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use wordnet word sense to determine the 'type' of the pun word\n",
    "\n",
    "This will be used to filter potential topic words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to identify the wordnet 'sense' of a word\n",
    "type_recognizer = utility.TypeRecognizer(threshold=0.2)\n",
    "\n",
    "# get the lemma and the word sense of the original topic word\n",
    "old_topic_word_lemma = nlp(alter_sent[topic_word_index])[0].lemma_\n",
    "if old_topic_word_lemma == \"-PRON-\":\n",
    "    old_topic_word_lemma = alter_sent[topic_word_index]\n",
    "types = type_recognizer.get_type(old_topic_word_lemma, 'noun')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use the skipgram model to find related topic words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fishnet',\n",
       " 'artificially',\n",
       " 'factionless',\n",
       " 'creamer',\n",
       " 'strappy',\n",
       " 'bangle',\n",
       " 'daub',\n",
       " 'frizz',\n",
       " 'encode',\n",
       " 'hair',\n",
       " 'josey',\n",
       " 'cm',\n",
       " 'artful',\n",
       " 'frizzy',\n",
       " 'magenta',\n",
       " 'coloring',\n",
       " 'floppy',\n",
       " 'tanning',\n",
       " 'beehive',\n",
       " 'stylist']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the skipgram model to find words that are related to the pun word\n",
    "skipgram = SkipGram.load_model(\"skipgram/dict.txt\", \"skipgram/model.pt\", embedding_size=300, cpu=False)\n",
    "potential_topic_words = skipgram.predict_neighbors(pun_word, k=100, masked_words=[old_topic_word_lemma])\n",
    "potential_topic_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['colour']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some of those are better than others to say the least\n",
    "# lets filter them so they have similar usage to our original word\n",
    "\n",
    "replacement_topic_words = []\n",
    "\n",
    "# filter potentil topic words to find those with similar sense as original topic word\n",
    "for word in potential_topic_words:\n",
    "    if type_recognizer.is_types(word, types, 'noun'):\n",
    "        replacement_topic_words.append(word)\n",
    "replacement_topic_words = [replacement_topic_words[1]]\n",
    "replacement_topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| [src] dictionary: 37360 types\n",
      "| [tgt] dictionary: 18912 types\n",
      "| dictionary: 267744 types\n",
      "['<placeholder>', 'colour', '<placeholder>', 'come', 'alone', ',', 'and', 'he', 'was', 'going', 'to', 'dye', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1339: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new sentence:\n",
      "the woman thing he saw come alone , and he was going to dye .\n",
      "The local-global surprisal score: -0.5802893262516355\n"
     ]
    }
   ],
   "source": [
    "# Now lets try inserting the topic words and 'smoothing' the sentences\n",
    "\n",
    "# for this we will be using the neural smoother model\n",
    "smoother, smoother_task = get_Smoother()\n",
    "\n",
    "# at the same time we will be scoring our puns\n",
    "scorer = surprisal_score\n",
    "lm = getLanguageModel()\n",
    "um = getUnigramModel()\n",
    "\n",
    "# try to smooth the sentence around the newly inserted topic word\n",
    "for word in replacement_topic_words:\n",
    "    \n",
    "    # remove the original topic word as well as the words around it\n",
    "    smoothing_input_sent = pun_sent[:max(0,topic_word_index-1)] + ['<placeholder>'] + [word]+['<placeholder>'] + pun_sent[topic_word_index+2:]\n",
    "\n",
    "    src_dict = smoother_task.source_dictionary\n",
    "    max_positions = (100000, 100000)\n",
    "    \n",
    "    # setting up the 'dataset' for the fairseq model\n",
    "    for inputs in utility.makeFairseqDataset([smoothing_input_sent], [word], src_dict, max_positions,smoother_task):\n",
    "        tokens = inputs['net_input']['src_tokens']\n",
    "        token_lens = inputs['net_input']['src_lengths']\n",
    "        encoder_input = {'src_tokens': tokens, 'src_lengths': token_lens}\n",
    "        \n",
    "        # generate the smoothed sentence\n",
    "        outputs = smoother.generate(encoder_input, maxlen=200)\n",
    "        tgt_dict = smoother_task.target_dictionary\n",
    "        smoothed_tokens,smoothed_sent,_ = utils.post_process_prediction(hypo_tokens=outputs[0][0]['tokens'].int().cpu(),src_str=None,alignment=None,align_dict=None,tgt_dict=tgt_dict,remove_bpe=\"None\")\n",
    "        smoothed_sent =pun_sent[:max(0,topic_word_index-1)] + smoothed_sent.split() + pun_sent[topic_word_index+2:]\n",
    "        \n",
    "    surp_score = scorer(smoothed_sent,topic_word_index+1 ,alter_word,lm,um )\n",
    "    print(\"The new sentence:\")\n",
    "    print(\" \".join(smoothed_sent))\n",
    "    print(f\"The local-global surprisal score: {surp_score}\")\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Batches of puns\n",
    "\n",
    "This function will pull relevent sentences from the corpus to generate multiple puns using a given pun/alter word pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePuns(alter_word, pun_word, retriever, skip_gram, smoother, smoother_task, scorer, lm, um):\n",
    "\n",
    "    # helper to identify the wordnet 'sense' of a word\n",
    "    type_recognizer = utility.TypeRecognizer(threshold=0.20)\n",
    "\n",
    "    # pull sentences that contain the alter word\n",
    "    retrieved_templates = retriever.retrieve_pun_template(alter_word, num_templates=20)\n",
    "    \n",
    "    # remove duplicate sentences\n",
    "    sentences = []\n",
    "    templates = []\n",
    "    for t in retrieved_templates:\n",
    "        if t.tokens not in sentences:\n",
    "            templates.append(t)\n",
    "            sentences.append(t.tokens)\n",
    "    \n",
    "    pun_word_indicies = [t.keyword_id for t in templates]\n",
    "    \n",
    "    # Get positions of possible topic words to replace\n",
    "    raw_sentence_iterator = nlp.pipe([' '.join(t.tokens) for t in templates])\n",
    "    min_distance_to_topic = 0.3 # % of sentence length that should separate pun and topic \n",
    "    topic_word_indicies = [] # locations of topics\n",
    "    for i, sent in enumerate(raw_sentence_iterator):\n",
    "        topic_word_indicies.append([])\n",
    "        for j, word in enumerate(sent[:pun_word_indicies[i]-len(sent)*min_distance_to_topic]):\n",
    "            if word.pos_ in ('NOUN', 'PROPN', 'PRON'):# topics have to be these type of words\n",
    "                topic_word_indicies[-1].append(j)\n",
    "\n",
    "    puns = []\n",
    "    alter_sentences = []\n",
    "    for i, (template, topic_word_idxs) in enumerate(zip(templates, topic_word_indicies)):\n",
    "        for topic_word_id in  topic_word_idxs:\n",
    "            # store the original sentence\n",
    "            alter_sent = template.tokens\n",
    "\n",
    "            # swap pun word\n",
    "            pun_sent = template.replace_keyword(pun_word)\n",
    "            pun_word_index = template.keyword_id\n",
    "\n",
    "            # get the lemma and the word sense of the original topic word\n",
    "            old_topic_word_lemma = nlp(alter_sent[topic_word_id])[0].lemma_\n",
    "            if old_topic_word_lemma == \"-PRON-\":\n",
    "                old_topic_word_lemma = alter_sent[topic_word_id]\n",
    "            types = type_recognizer.get_type(old_topic_word_lemma, 'noun')\n",
    "\n",
    "            # find words that are related to the pun word\n",
    "            potential_topic_words = skip_gram.predict_neighbors(pun_word, k=100, masked_words=[old_topic_word_lemma])\n",
    "\n",
    "            replacement_topic_words = []\n",
    "            for word in potential_topic_words:\n",
    "                if type_recognizer.is_types(word, types, 'noun'):\n",
    "                    replacement_topic_words.append(word)\n",
    "\n",
    "            # try to smooth the sentence around the newly inserted topic word\n",
    "            for word in replacement_topic_words:\n",
    "                smoothing_input_sent = pun_sent[:max(0,topic_word_id-1)] + ['<placeholder>'] + [word]+['<placeholder>'] + pun_sent[topic_word_id+2:]\n",
    "   \n",
    "                src_dict = smoother_task.source_dictionary\n",
    "                max_positions = (100000, 100000)\n",
    "\n",
    "                for inputs in utility.makeFairseqDataset([smoothing_input_sent], [word], src_dict, max_positions,smoother_task):\n",
    "                    tokens = inputs['net_input']['src_tokens']\n",
    "                    token_lens = inputs['net_input']['src_lengths']\n",
    "                    encoder_input = {'src_tokens': tokens, 'src_lengths': token_lens}\n",
    "                    outputs = smoother.generate(encoder_input, maxlen=200)\n",
    "                    tgt_dict = smoother_task.target_dictionary\n",
    "                    smoothed_tokens,smoothed_sent,_ = utils.post_process_prediction(hypo_tokens=outputs[0][0]['tokens'].int().cpu(),src_str=None,alignment=None,align_dict=None,tgt_dict=tgt_dict,remove_bpe=\"None\")\n",
    "                    smoothed_sent =pun_sent[:topic_word_id-1] + smoothed_sent.split() + pun_sent[topic_word_id+3:]\n",
    "\n",
    "                surp_score = scorer(smoothed_sent,pun_word_index+1 ,alter_word,lm,um )\n",
    "                puns.append([smoothed_sent,surp_score])\n",
    "        \n",
    "    return puns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| [src] dictionary: 37360 types\n",
      "| [tgt] dictionary: 18912 types\n",
      "| dictionary: 267744 types\n"
     ]
    }
   ],
   "source": [
    "retriever = Retriever([\"tmp/train.tokenized.txt\"], \"retriever/retriever.pkl\", False)\n",
    "smoother, task = get_Smoother()\n",
    "skipgram = SkipGram.load_model(\"skipgram/dict.txt\", \"skipgram/model.pt\", embedding_size=300, cpu=False)\n",
    "surprisal_scorer = surprisal_score\n",
    "lm = getLanguageModel()\n",
    "um = getUnigramModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1339: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "puns = makePuns(\"die\", \"dye\", retriever, skipgram, smoother, task, surprisal_scorer, lm, um)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He had come alone , and he was going to dye the woman thing he saw alone , and he was going to dye .\n",
      "the only thing those , , he or she may dye .\n",
      "the only thing those in , he or she may dye .\n",
      "the woman time i saw , he or she may dye .\n",
      ", and that 's , he or she may dye .\n",
      "the smile nine that , he or she may dye .\n",
      "the woman thing i saw , he or she may dye .\n",
      "the woman thing he saw , he or she may dye .\n",
      "the only thing he 's , he or she may dye .\n",
      "the woman thing i have going to let you dye . \"\n",
      "the woman thing i know going to let you dye . \"\n",
      "the woman thing i have going to let him dye . \"\n",
      ", \" goodness looking , going to let him dye . \"\n",
      "\" You the woman thing i know going to dye ? \"\n",
      "\" You the smile thing you have going to dye ? \"\n",
      "\" You the smile thing you kept going to dye ? \"\n",
      "\" You the smile nine you have going to dye ? \"\n",
      "\" You the smile thing that 's going to dye ? \"\n",
      "\" You \" \" field , going to dye ? \"\n",
      "And the way you earth n't have to dye at all . \"\n",
      "And the smile way , n't have to dye at all . \"\n",
      "And and the way you earth n't have to dye at all . \"\n",
      "And that the way you earth have to dye at all . \"\n",
      "And that the smile way i running have to dye at all . \"\n",
      "And that the smile way , have to dye at all . \"\n",
      "and the rest of the \" How did he dye ? \"\n",
      "and the best of the \" How did he dye ? \"\n",
      "the smile nine and \" How did he dye ? \"\n",
      "as a lifted , and \" How did he dye ? \"\n",
      "the woman thing he saw \" How did he dye ? \"\n",
      "He whispered , \" It 's to dye for . the woman thing i saw \" It 's to dye for . \"\n",
      "He whispered , \" It 's to dye for . , \" goodness said , \" It 's to dye for . \"\n"
     ]
    }
   ],
   "source": [
    "seen_puns = []\n",
    "for pun in puns:\n",
    "    if pun[0] not in seen_puns:\n",
    "        print(\" \".join(pun[0]))\n",
    "        seen_puns.append(pun[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Future Direction\n",
    "\n",
    "The local-global surprisal principle demonstrates some improvement over previous attempts, as seen in the 2013 paper from Petrovic & Matthews [2]. This method makes detecting candidate pun sentences more effective, although there is still a long way to go to meet human funniness standards.\n",
    "Future progress will involve finding ways to have language models separate creative, well-formed material from throw away nonsensical sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "[1]: Yu, Zhiwei & Tan, Jiwei & Wan, Xiaojun. (2018). A Neural Approach to Pun Generation. 1650-1660. 10.18653/v1/P18-1153\n",
    "\n",
    "[2]:  Petrovic & Matthews. (2013). Unsupervised  joke generation from big  data. In proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013, Volume 2: Short Papers, pages 228–232.\n",
    "\n",
    "[3]: S. Merity, C. Xiong, J. Bradbury, and R. Socher. 2016.Pointer sentinel mixture models.\n",
    "            arXiv preprint arXiv:1609.07843.\n",
    "            \n",
    "[4]: J.T. Kao, R.Levy and N. D. Goodman. 2015. A computational model of linguistic humour in puns. Cognitive Science.\n",
    "\n",
    "[5] Fuli Luo, Shunyao Li, Pengcheng Yang, Lei li, Baobao Chang, Zhifang Sui, & Xu Sun. (2019). Pun-GAN: Generative Adversarial Network for Pun Generation. \n",
    "\n",
    "[6] He He, Nanyun Peng, & Percy Liang (2019). Pun Generation with Surprise. In North American Chapter of the Association for Computational Linguistics (NAACL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
