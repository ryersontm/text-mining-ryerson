{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks for Sentence Classification\n",
    "\n",
    "#### Members Names: Gagandip Chane, Devika Kabe\n",
    "\n",
    "#### Members Emails: gchane@ryerson.ca, dkabe@ryerson.ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "#### Problem Description:\n",
    "\n",
    "Perform sentence-level classification tasks using CNNs with only one layer of convolution.\n",
    "\n",
    "#### Context of the Problem:\n",
    "\n",
    "This problem is important because it demonstrates the importance of pre-trained vectors. It shows that they are universal feature extractors that can be utilized for various classification tasks e.g. sentence classification, question classification and polarity detection. This paper also demonstrates that CNNs are computationally efficient and there is no need to use information stored in the sequential nature of the data when performing tasks such as sentiment analysis, since the main takeaway will be whether or not something was \"good\" or \"bad\" etc. Thus, we can conclude that CNNs can be sufficient in comparison to traditional approaches such as Naive Bayes, Linear Regression, and Support Vector Machines. \n",
    "\n",
    "#### Limitation About other Approaches:\n",
    "\n",
    "One paper, Kalchbrenner et al., (2014) uses a max Time Delay Neural Network (TDNN) as a comparison. They report that the size of the filters is limited to the span of the weights. Increasing the span makes the range of the filters larger which requires increasing the minimum sentence size. Roller et al., (2016) implement a very simple multi-group norm constraint convolutional neural network (MGNC-CNN) which aims to have short run time, but as the word embeddings increase, run time increases. Schutze and Yin (2014) propose a Multichannel Variable Size CNN (MVCNN), a CNN architecture for sentence classification. The model is complex both in terms of implementation and run time and requires that input word embeddings have the same dimensionality. \n",
    "\n",
    "#### Solution:\n",
    "\n",
    "Train a simple CNN with one layer of convolution on top of word vectors. Concatenate the word vectors and then generate features by applying a filter to a window of h words. This produces a feature map. Apply max-pooling over the feature map to take the maximum value as the feature corresponding to this filter. Do this for multiple features. These features are passed through a fully connected softmax layer whose output is the probability distribution over labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "\n",
    "\n",
    "| Reference |Explanation |  Dataset/Input |Weakness\n",
    "| --- | --- | --- | --- |\n",
    "| Kalchbrenner et al. [1] | Applies max-TDNN to multipe datasets. The sentence is viewed as having a time dimension and the convolution is applied over the time dimension. | SST1,  TREC, Twitter sentiment | Max-TDNN only 37.4% accuracy, size of feature detectors (filters) is limited. If they increase it, then they have to increase the minimum size of the sentence required.|\n",
    "| Zhang et al. [2] | They implement a simple multi-group norm constraint CNN. What this means is it treats each word embedding as a distinct group and applies CNNs independently to each one. | SST1, SST2, Subj, TREC, Irony | High accuracy but must tune norm constraint hyperparameter for all word embeddings, run time will increase as word embeddings increase. |\n",
    "| Yin and Schutze [3] | Multichannel Variable-Size CNN for sentence classification. This means that the input is a 3 dimensional array of size c x d x s where c is the number of word embeddings, d is the dimension of word embeddings and s is the sentence length. | SST1, Sentiment140, Subj | Requires that input word embeddings have the same dimensionality and model is complex in terms of implementation and run time. |\n",
    "| Kim, Y. [4] | Train CNN on top of word vectors with one convolution layer and apply max over-time pooling. | MR, SST1, SST2, Subj, TREC, CR, MPQA | Further work on regularizing the fine-tuning process is warranted. Multichannel architecture did not prevent overfitting as hoped. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The paper uses several variants of the model, namely:\n",
    "#### CNN-rand: \n",
    "This is used as a baseline. All words are initialized at random and then modified during training. <br>\n",
    "#### CNN-static: \n",
    "A model with pre-trained vectors from word2vec. All words—including the unknown ones that are randomly initialized—are kept static and only the other parameters of the model are learned <br>\n",
    "#### CNN-non-static: \n",
    "Same as above but the pretrained vectors are fine-tuned for each task. <br>\n",
    "#### CNN-multichannel: \n",
    "A model with two sets of word vectors. Each set of vectors is treated as a ‘channel’ and each filter is applied to both channels, but gradients are backpropagated only through one of the channels.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: This step processes the input data which creates word embeddings for all words in the sentences and concatenates them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $x_{i}$ ∈ $R_{k}$ be the k-dimensional word vector corresponding to the i-th word in the sentence. <br>\n",
    "A sentence of length n (padded where necessary) is represented as: <br>\n",
    "$x_{1:n}$ = $x_{1}$ ⊕ $x_{2}$ ⊕ . . . ⊕ $x_{n}$ <br>\n",
    "⊕ is the concatenation operator, so it will look like:\n",
    "![Alternate text ](image3.png \"Max pooling applied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Apply a filter to a sliding window of words (similar to that of image classification)  to create a new matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A filter, $w$ ∈ $R_{hk}$ is applied to a window of $h$ words to produce a new feature, e.g.: <br>\n",
    "$c_{i}$ = $f$($w$·$x_{i:i+h-1}$ + $b$) <br>\n",
    "$b$ is a bias term and $f$ is a non-linear function. This filter is applied to each possible window of words in the sentence {$x_{1:h}$, $x_{2:h+1}$,..., $x_{n-h+1:n}$) to produce a feature map: <br>\n",
    "$c$ = [$c_{1}$, $c_{2}$,...,$c_{n-h+1}$] with c ∈ $R^{n-h+1}$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Using max over-time pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a max over-time pooling operation over the feature map and take the maximum value: <br>\n",
    "$\\hat{c}$ = max{$c$} <br>\n",
    "The idea here is to capture the most important feature. \n",
    "\n",
    "Max over-time pooling is slightly different from max pooling in that it simply returns one number which is the maximum over the entire vector, as opposed to a matrix of new dimensions.\n",
    "\n",
    "This step shows it only for one filter, the model uses multiple filters to obtain multiple features which would look like:\n",
    "![Alternate text ](image2.png \"Max pooling applied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Flatten the dimensions and feed through final layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features form the final layer and are passed to a fully connected softmax layer whose output is the probability over labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization: Using dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Employ dropout on final layer, e.g.:\n",
    "if $z$ = [$\\hat{c_{1}}$,...,$\\hat{c_{m}}$]\n",
    "then for the output unit $y$ in forward propagation, dropout uses $y$ = $w$·($z$◦$r$) + $b$ where ◦ is the element-wise multiplication operator and $r$ ∈ $R_{m}$ is a ‘masking’ vector of Bernoulli random variables with probability p of being 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alternate text ](image1.jpg \"Model architecture with two channels for an example sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "The implementation is from:\n",
    "https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please install the gensim package in the notebook or through anaconda prompt as the dependent w2v.py file which loads the word vectors makes use of this package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from w2v import train_word2vec # separate w2v.py file to load the pre-trained vectors from word2vec\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of the model are defined below. \n",
    "There are some things that are different in this implementation compared to the original paper:\n",
    "- Embedding dimension for the word vectors from word2vec is 50 instead of 300.\n",
    "- Filter sizes are (3, 8) instead of (3, 4, 5)\n",
    "- For each filter size, there are 10 filters, instead of original 100. Experiments showed that 3-10 filters are enough.\n",
    "- There are two elements in the dropout_prob tuple. The first one is used after the embedding layer (p=0.5) and the second one is used after the convolutional layer (p=0.8). The original paper just uses dropout only once with p = 0.5 after the convolutional layer. I tried both dropout methods and the one shown in this implementation (0.5, 0.8) yields the same validation accuracy with training accuracy being closer to the validation accuracy, hence it pre-vents over-fitting.\n",
    "- The implementation shows training on two datasets however the implementation provided here is done on one:\n",
    "    - polarity dataset v1.0 dataset found at the following link: https://www.cs.cornell.edu/people/pabo/movie-review-data/. \n",
    "    - IMDB 25k reviews dataset (shown in this notebook)\n",
    "\n",
    "As stated above, there are multiple variations of the model. They can be changed by adjusting the model_type variable. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Parameters section -------------------\n",
    "#\n",
    "# Model type. See Kim Yoon's Convolutional Neural Networks for Sentence Classification, Section 3\n",
    "model_type = \"CNN-non-static\"  # CNN-rand|CNN-non-static|CNN-static\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 50 # dimension of word vectors in the embedding layer\n",
    "filter_sizes = (3, 8)  # kernel size in convolutional layer\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50 # hidden neurons in the fully connected layer\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64 # training batch size\n",
    "num_epochs = 10\n",
    "\n",
    "# Prepossessing parameters\n",
    "max_words = 5000 # maximum number of words to include in each review -- used for pulling data\n",
    "sequence_length = 400 # maximum length of each input -- used for padding\n",
    "\n",
    "# Word2Vec parameters (see train_word2vec)\n",
    "min_word_count = 1 # minimum count of words to consider -- if a word occurs less than this, ignore it \n",
    "\n",
    "# window size to consider for each word in the input when training word vectors\n",
    "# 10 means 10 words before and 10 words after\n",
    "context = 10 \n",
    "\n",
    "#\n",
    "# ---------------------- Parameters end -----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the function to load the imdb data and preprocess it. Specific comments are provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "print(\"Load data...\")\n",
    "\n",
    "# import imdb reviews dataset and limit the maximum words to 5000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words, start_char=None,\n",
    "                                                            oov_char=None, index_from=None)\n",
    "        \n",
    "# perform padding based on defined sequence in parameters section\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=sequence_length, padding=\"post\", truncating=\"post\")\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=sequence_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "vocabulary = imdb.get_word_index() # get index of all vocabulary\n",
    "vocabulary_inv = dict((v, k) for k, v in vocabulary.items()) # flip the key and values to put word index as key\n",
    "vocabulary_inv[0] = \"<PAD/>\" # add <PAD/> to vocabulary for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Vocabulary Size: 88585\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocabulary_inv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_word2vec is a function in the w2v.py file that retrives the pre-trained vectors from word2vec as the embedding layer weights. x_train and x_test are stacked and passed. Vocabulary including words and their index are passed. The embedding dimension defined in the parameters section is passed. Minimum word count and context length is passed as well. \n",
    "Some things to note:\n",
    "- If the model_type is CNN-static, the retrieved word embedding weights are the ones that are used throughout the training process and are not trained (hence static). Due to this, in the sub-condition below they are stacked based on sentences and words in the training and test set.\n",
    "- If the model_type is CNN-non-static, the pre-trained embedding weights are passed into the embedding layer in the model code blocks and then further trained.\n",
    "- If the model_type is CNN-rand (baseline model), there are no pre-trained embedding weights and are randomly initialized and then trained in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type is CNN-non-static\n",
      "Load existing Word2Vec model '50features_1minwords_10context'\n"
     ]
    }
   ],
   "source": [
    "# Prepare embedding layer weights and convert inputs for static model\n",
    "print(\"Model type is\", model_type)\n",
    "# model_type from parameters section\n",
    "if model_type in [\"CNN-non-static\", \"CNN-static\"]:\n",
    "    embedding_weights = train_word2vec(np.vstack((x_train, x_test)), vocabulary_inv, num_features=embedding_dim,\n",
    "                                       min_word_count=min_word_count, context=context)\n",
    "    \n",
    "    # stacking embedding weights for all words in all sentences which will stay static throughout training\n",
    "    if model_type == \"CNN-static\":\n",
    "        x_train = np.stack([np.stack([embedding_weights[word] for word in sentence]) for sentence in x_train])\n",
    "        x_test = np.stack([np.stack([embedding_weights[word] for word in sentence]) for sentence in x_test])\n",
    "        print(\"x_train static shape:\", x_train.shape)\n",
    "        print(\"x_test static shape:\", x_test.shape)\n",
    "\n",
    "# no pre-trained embeddings if CNN-rand\n",
    "elif model_type == \"CNN-rand\":\n",
    "    embedding_weights = None\n",
    "else:\n",
    "    raise ValueError(\"Unknown model type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Initializing embedding layer with word2vec weights, shape (88585, 50)\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "\n",
    "# setting input shape based on model type\n",
    "if model_type == \"CNN-static\":\n",
    "    input_shape = (sequence_length, embedding_dim)\n",
    "else:\n",
    "    input_shape = (sequence_length,)\n",
    "\n",
    "model_input = Input(shape=input_shape)\n",
    "\n",
    "# static model does not have embedding layer\n",
    "if model_type == \"CNN-static\":\n",
    "    z = model_input\n",
    "else:\n",
    "    # embedding layer added if model type is CNN-rand or CNN-non-static\n",
    "    z = Embedding(len(vocabulary_inv), embedding_dim, input_length=sequence_length, name=\"embedding\")(model_input)\n",
    "\n",
    "# dropout with probability 0.5 (pre-defined) after embedding layer\n",
    "z = Dropout(dropout_prob[0])(z)\n",
    "\n",
    "# convolutional layer with 10 filters (pre-defined), (3, 8) as filter sizes (pre-defined)\n",
    "conv_blocks = []\n",
    "# iterates through all filter sizes and adds to convolutional layer\n",
    "for sz in filter_sizes:\n",
    "    conv = Convolution1D(filters=num_filters,\n",
    "                         kernel_size=sz,\n",
    "                         padding=\"valid\",\n",
    "                         activation=\"relu\",\n",
    "                         strides=1)(z)\n",
    "    conv = MaxPooling1D(pool_size=2)(conv) # maxpool layer\n",
    "    conv = Flatten()(conv) # flatten to pass into fully conencted layer\n",
    "    conv_blocks.append(conv)\n",
    "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0] # all filters are concateneted if more than 1\n",
    "\n",
    "# dropout probability of 0.8 (pre-defined) after convolutional layer\n",
    "z = Dropout(dropout_prob[1])(z)\n",
    "\n",
    "# hidden neurons (pre-defined) in fully connected layer\n",
    "z = Dense(hidden_dims, activation=\"relu\")(z) \n",
    "\n",
    "# output layer with sigmoid activation for positive/negative\n",
    "model_output = Dense(1, activation=\"sigmoid\")(z) \n",
    "\n",
    "# create model instance\n",
    "model = Model(model_input, model_output)\n",
    "\n",
    "# compile model with loss functiona as binary cross-entropy, adam optimizer is used to iterate and optimize\n",
    "# the objective function\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# if model type is CNN-non-static, the embedding layer is initialized with the embedding weights from word2vec\n",
    "if model_type == \"CNN-non-static\":\n",
    "    weights = np.array([v for v in embedding_weights.values()])\n",
    "    print(\"Initializing embedding layer with word2vec weights, shape\", weights.shape)\n",
    "    embedding_layer = model.get_layer(\"embedding\")\n",
    "    embedding_layer.set_weights([weights])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model below. Note that if the model type is CNN-static, x_train and x_test are are the sentences whose words are based on pre-trained word vectors. This is done in the above code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 147s 6ms/step - loss: 0.6966 - accuracy: 0.5028 - val_loss: 0.6927 - val_accuracy: 0.5281\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 144s 6ms/step - loss: 0.6578 - accuracy: 0.5784 - val_loss: 0.4656 - val_accuracy: 0.7945\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 139s 6ms/step - loss: 0.3913 - accuracy: 0.8256 - val_loss: 0.3192 - val_accuracy: 0.8700\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 140s 6ms/step - loss: 0.3039 - accuracy: 0.8717 - val_loss: 0.2923 - val_accuracy: 0.8808\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 143s 6ms/step - loss: 0.2724 - accuracy: 0.8880 - val_loss: 0.2798 - val_accuracy: 0.8848\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 140s 6ms/step - loss: 0.2571 - accuracy: 0.8958 - val_loss: 0.2729 - val_accuracy: 0.8875\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 136s 5ms/step - loss: 0.2427 - accuracy: 0.9030 - val_loss: 0.2796 - val_accuracy: 0.8854\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 147s 6ms/step - loss: 0.2282 - accuracy: 0.9102 - val_loss: 0.2757 - val_accuracy: 0.8852\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 142s 6ms/step - loss: 0.2217 - accuracy: 0.9113 - val_loss: 0.2772 - val_accuracy: 0.8859\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 143s 6ms/step - loss: 0.2128 - accuracy: 0.9155 - val_loss: 0.2746 - val_accuracy: 0.8846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a43a191d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_data=(x_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Future Direction\n",
    "\n",
    "In this paper, they explained a series of experiments with convolutional neural networks built on top of word2vec. Despite little tuning of hyperparameters, a simple CNN with one layer of convolution performs remarkably well. This adds to the research that unsupervised pre-training of word vectors is an important aspect of deep learning for NLP. The CNN\n",
    "models discussed in the paper improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification. They will continue to study multichannel architecture to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "[1]:  Kalchbrenner, E. Grefenstette, P. Blunsom. 2014. A\n",
    "Convolutional Neural Network for Modelling Sentences. In Proceedings of ACL 2014.\n",
    "\n",
    "[2]:  Zhang, Y., Roller, S., and Wallace, B. (2016). MGNC-CNN: A simple approach to exploiting\n",
    "multiple word embeddings for sentence classification. Proc. of NAACL.\n",
    "\n",
    "[3]: Wenpeng Yin and Hinrich \n",
    "Schutze. 2015. Multichannel variable-size convolution\n",
    "for sentence classification. In Proceedings of the Conference on Computational Natural Language Learning,\n",
    "pages 204–214.\n",
    "\n",
    "[4]: Yoon Kim. 2014. Convolutional neural\n",
    "networks for sentence classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
