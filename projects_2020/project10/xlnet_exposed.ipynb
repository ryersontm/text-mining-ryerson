{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: XLNet Exposed\n",
    "\n",
    "#### Members Names: Oscar Bobadilla, Igor Ilic\n",
    "\n",
    "#### Members Emails: {oscar.bobadilla, iilic} @ ryerson.ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "#### Problem Description:\n",
    "\n",
    "Language modelling is a tough task, as we have seen throughout the course. Computers have\n",
    "a difficult time picking up on a lot of nuances that human's have learned. As well, each language has\n",
    "a different structure, without logical rules. We need a computer to be able to bring logic to an illogical world.\n",
    "\n",
    "Language Modelling Tasks include:\n",
    "- Text Classification\n",
    "- Question Answering\n",
    "- Document Ranking\n",
    "- Reading Comprehension\n",
    "  - Text classification (example benchmark: SST-2)\n",
    "  - Question Answering (example benchmark: SQuad v2.0)\n",
    "- etc.\n",
    "\n",
    "***\n",
    "#### Context of the Problem:\n",
    "\n",
    "##### BERT [4]\n",
    "The biggest improvment in recent history is BERT, (Bidirectional Encoder Representations from Transformers). BERT is able to capture bidirectional importance using transformer networks [10], and masking. This brings a huge improvement from unidirectional methods prior (like ELMo [1], which uses LSTMs [11]).\n",
    "\n",
    "##### Transformer-XL [5]\n",
    "Another important network is TransformerXL. Transformers are limited by the fact that they have a fixed window when encoding sequences. This means the long sequences need to be broken down into multiple smaller sequences, losing the positional relation. TransformerXL fixes this by adding some state (not like RNNs though), to allow future sequeneces to understand previous sequences. This makes it autoregressive in nature.\n",
    "\n",
    "***\n",
    "#### Limitation About other Approaches:\n",
    "\n",
    "##### BERT\n",
    "Using a [MASK] token is problematic because it doesn't exist in the fine-tuning portion. As well, it is trivial to predict an unmasked word. The authors of BERT tried to address this problem by taking a portion of [MASK]ed words, and un[MASK]ing them. As well, for a portion of the un[MASK]ed words, they were randomly replaced. This tried to address the [MASK]ing problem\n",
    "\n",
    "As well, BERT performed all predictions in parallel. In some cases, the ordering of predictions matters. Take:  \n",
    "> I'm [MASK], I should [MASK]. \n",
    "\n",
    "This could be filled with \"energetic\" + \"dance\", and \"tired\" + \"sleep\", but BERT would allow for \"energetic\" + \"sleep\" to be predicted, due to the paralleized nature of BERT. The difference in prediction strategy can be seen below [7].\n",
    "\n",
    "<img src=\"images/conceptual_difference.png\" alt=\"Alt text that describes the graphic\" title=\"Title text\" />\n",
    "  \n",
    "***\n",
    "#### Solution:\n",
    "\n",
    "XLNet [3] fixes the [MASK] problem by using permutation language modelling. This allows for bidirectional training, without corrupting the data. This works by predicting words in a random order. As well, during the pretraining phase, XLNet utilizes a random permutation of words, combined with a positional encoding to capture bidirectionality.\n",
    "\n",
    "Then, XLNet incorporates features from TransformerXL to further enhance itself. This allows for longer term dependencies. They found incorporating TransformerXL into BERT alone yielded benefits, but they skipped the intermediate step and directly incorporated TransformerXL into their own paper.\n",
    "\n",
    "By using TransformerXL, the model retains state. This previous state is frozen and cached, then fed into the current state, in whole. It can be fed in whole because all the words exist, so the permutation order doesn't matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "Results have been pulled from XLNet paper directly. [3]\n",
    "\n",
    "**Full Comparison With BERT**\n",
    "Comparison with BERT on all major Language Modelling Benchmarks\n",
    "<img src=\"images/bert_comparison.png\" alt=\"BERT Comparison\" title=\"BERT Comparison\" />\n",
    "***\n",
    "\n",
    "**Reading Comprehension Comparison**\n",
    "Comparison on reading comprehension tasks.\n",
    "<img src=\"images/roberta_comparison.png\" alt=\"Test 2 Results\" title=\"Test 2 Results\" />\n",
    "\n",
    "**Question Answering [2]**\n",
    "Comparison on question and answering tasks. IE: Can a model answer a question about a document.\n",
    "<img src=\"images/qa_comparison.png\" alt=\"QA Comparison\" title=\"QA Comparison\" />\n",
    "\n",
    "**Text Classification**\n",
    "Ability to group/classify documents. This includes the IMDb dataset we have worked with.\n",
    "<img src=\"images/text_classification.png\" alt=\"Text Classification\" title=\"Text Classification\" />\n",
    "\n",
    "**GLUE [6]**\n",
    "Popular benchmark to measure Language Modelling results. \n",
    "<img src=\"images/glue.png\" alt=\"GLUE Results\" title=\"GLUE Results\" />\n",
    "\n",
    "\n",
    "XLNet is cleary very strong, and currently the state of the art method. Deep Language modelling has come a long way with BERT coming out, and is changing rapidly, so we suspect improvements to come soon.\n",
    "\n",
    "From personal use, we have found this to be a huge deep learning task, requiring powerful computers. It would be great to see a minimized version of XLNet come out (similarly to how DistilBERT [9] came out after BERT). This would allow XLNet to be more widely used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "Though there are many ways to use bidirectional transformers, the way\n",
    "we explored was classification with the SST2 database. This dataset is commonly used as a benchmark,\n",
    "so we determined it to be a good place to explore.\n",
    "\n",
    "SST-2 consists of many different strings, which are classified as positive (1) or negative (0).\n",
    "Samples have been included below.\n",
    "\n",
    "Training an XLnet from scratch is a very complicated task. Because of this, we instead use a pretrained, light version of the model (xlnet-base-cased: 110M params), and grabbed embeddings to put into a classification layer. This is different from the results from XLNet.\n",
    "\n",
    "Typically, the way to use XLNet would be to take the pretrained version (xlnet-large-cased), and then fine-tune to the data set. This would yield the high results in the paper. We briefly discuss this at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "*Similar to Alammar (2019) [8] demonstration of using BERT.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "fvFvBLJV0Dkv",
    "outputId": "140119e5-4cee-4604-c0d2-be279c18b125"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# XLNet specifics\n",
    "import torch\n",
    "from transformers import XLNetModel, XLNetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset info\n",
    "Using dataset SST-2 (*Note: XLNet reported 94.4% accuracy using XLNet large*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv',\n",
    "                 delimiter='\\t',\n",
    "                 names=['sentence','label'])\n",
    "\n",
    "df_test = pd.read_csv('https://raw.githubusercontent.com/clairett/pytorch-sentiment-classification/master/data/SST2/test.tsv',\n",
    "                 delimiter='\\t',\n",
    "                 names=['sentence','label'])\n",
    "\n",
    "split_point = len(df_train)\n",
    "df = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 1: a stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films\n",
      "Label 0: apparently reassembled from the cutting room floor of any given daytime soap\n",
      "Label 0: they presume their audience wo n't sit still for a sociology lesson , however entertainingly presented , so they trot out the conventional science fiction elements of bug eyed monsters and futuristic women in skimpy clothes\n",
      "Label 1: this is a visually stunning rumination on love , memory , history and the war between art and commerce\n",
      "Label 1: jonathan parker 's bartleby should have been the be all end all of the modern office anomie films\n"
     ]
    }
   ],
   "source": [
    "for s, l in df.head().values:\n",
    "    print(f'Label {l}: {s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "jGvcfcCP5xpZ",
    "outputId": "4c4a8afc-1035-4b21-ba9a-c4bb6cfc6347"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3610\n",
       "0    3310\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    912\n",
       "1    909\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7_MO08_KiAOb"
   },
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "q1InADgf5xm2",
    "outputId": "dbc52856-4d52-42f8-8a74-a89944280a02"
   },
   "outputs": [],
   "source": [
    " # See here for all pretrained https://huggingface.co/transformers/pretrained_models.html?highlight=pretrained\n",
    "pretrained_label = 'xlnet-base-cased'\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained(pretrained_label)\n",
    "model = XLNetModel.from_pretrained(pretrained_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dg82ndBA5xlN"
   },
   "outputs": [],
   "source": [
    "tokenized = df['sentence'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These added special tokens include a classifier token, \"\\< cls\\>\" which we are mainly interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer string form: <cls>, id: 3\n"
     ]
    }
   ],
   "source": [
    "print(f'Tokenizer string form: {tokenizer.cls_token}, id: {tokenizer.cls_token_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URn-DWJt5xhP"
   },
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "cls_positions = np.zeros(len(tokenized), dtype=np.int64)\n",
    "for posn, i in enumerate(tokenized.values):\n",
    "    cls_positions[posn] = len(i) - 1\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mdjg306wjjmL"
   },
   "source": [
    "A sample padded sentence looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   24, 16003,    17,    19,  5787,    21,  1381, 21469,    17,\n",
       "          88,  7693, 15930,    56,    20,  4111,    21,    18, 11740,\n",
       "          21,  4974,    23,  6941,  2701,     4,     3,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the cls token is at the end of the sentence, before the padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sDZBsYSDjzDV"
   },
   "source": [
    "### Masking\n",
    "A slight nuance, we need to pass in a attention masking map, which allows XLNet to identify where the sentence is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4K_iGRNa_Ozc",
    "outputId": "d03b0a9b-1f6e-4e32-831e-b04f5389e57c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8741, 86)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "We need to pass the tokenized sentences into XLNet now, and get the embeddings to pass into\n",
    "another classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "39UVjAV56PJz"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(inputs, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to select the correct term, we select the classifier token from the mapping.\n",
    "We know that the final hidden state maps the classifier tokens to their respective position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "final_hidden_state = last_hidden_states[0]\n",
    "X = final_hidden_state[np.arange(len(final_hidden_state)), cls_positions]\n",
    "\n",
    "# Labels\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iaoEvM2evRx1"
   },
   "source": [
    "## Classification\n",
    "\n",
    "With all of the encoded values, now we can pass into any classifier we'd like to. For simplicity,\n",
    "we chose to take the default values of the sklearn MLPClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ddAqbkoU6PP9"
   },
   "outputs": [],
   "source": [
    "num_test_pts = len(df) - split_point\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=num_test_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "gG-EVWx4CzBc",
    "outputId": "9252ceff-a7d0-4359-fef9-2f72be89c7d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iCoyxRJ7ECTA",
    "outputId": "cfd86dea-5d16-476c-ab9b-47cbee3a014f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7874794069192751"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a lot better than random guessing! It doesn't reach XLNets full accuracy in the paper (94.4%), but this model has:  \n",
    "- Simple, unoptimized classification layer\n",
    "- Fewer, untuned weights in XLNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning XLNet\n",
    "\n",
    "Although the heursitic test above is fast and easy to implement, it doesn't obtain the results\n",
    "found in the XLNet paper (94.4% accuracy).\n",
    "\n",
    "By running fine tuning on the SST-2 dataset, we were able to up the accuracy to 94.0%. This was accomplished\n",
    "by setting up the transformers repo, and running the [examples](https://github.com/huggingface/transformers/blob/master/examples/README.md):\n",
    "\n",
    "```bash\n",
    "export GLUE_DIR=/path/to/data\n",
    "export TASK_NAME=SST-2\n",
    "\n",
    "python run_glue.py \\\n",
    "  --model_type xlnet \\\n",
    "  --model_name_or_path xlnet-base-cased \\\n",
    "  --task_name $TASK_NAME \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --data_dir $GLUE_DIR/$TASK_NAME \\\n",
    "  --max_seq_length 128 \\\n",
    "  --per_gpu_train_batch_size 16 \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --num_train_epochs 3.0 \\\n",
    "  --output_dir $GLUE_DIR/tmp/$TASK_NAME/\n",
    "```\n",
    "\n",
    "yields:\n",
    "```bash\n",
    "$ cat eval_results.txt\n",
    "acc = 0.9403669724770642\n",
    "```\n",
    "\n",
    "This task took significantly longer to do, but was able to fine tune to a particular data set incredibly well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Future Direction\n",
    "\n",
    "These results have shown that even using the raw pretrained XLNet weights can be hugely useful in making predictions on new tasks with limited data. However, as we have seen, there can be great improvement by fine tuning XLNet from the data set.\n",
    "\n",
    "We were able to see an increase from 78.7% up to 94.0% by fine-tuning XLNet instead of using the classification layer. This task required powerful computers (128GB RAM, 2x GeForce RTX2070 GPUs, and a 16core i7 processor), and still took an hour to compute one number.\n",
    "\n",
    "Since we were able to reproduce the results, in the future it would be great to extend use of XLNet beyond classic metrics (SST-2, SQuadv2.0, etc.). By using different datasets, it is possible to push other benchmarks, and bring XLNet into industry production as well.\n",
    "\n",
    "Finally, it is important to see what comes after XLNet. XLNet is still really new, and there are definitely new models that will come out during 2020 that will outperform XLNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "[1]: Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). Deep contextualized word representations. arXiv preprint arXiv:1802.05365.\n",
    "\n",
    "[2]: Rajpurkar, P., Zhang, J., Lopyrev, K., & Liang, P. (2016). Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250.\n",
    "\n",
    "[3]: Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., & Le, Q. V. (2019). Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in neural information processing systems (pp. 5754-5764).\n",
    "\n",
    "[4]: Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n",
    "\n",
    "[5]: Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., & Salakhutdinov, R. (2019). Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860.\n",
    "\n",
    "[6]: Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., & Bowman, S. R. (2018). Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461.\n",
    "\n",
    "[7]: Kurita, Keita (2019). Paper Dissected: “XLNet: Generalized Autoregressive Pretraining for Language Understanding” Explained [Blog post]. Retrieved from https://mlexplained.com/2019/06/30/paper-dissected-xlnet-generalized-autoregressive-pretraining-for-language-understanding-explained/\n",
    "\n",
    "[8]: Alammar, Jay (2019). A Visual Guide to Using BERT for the First Time [Blog post]. Retrieved from http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n",
    "\n",
    "[9]: Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.\n",
    "\n",
    "[10]: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008).\n",
    "\n",
    "[11]: Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
