{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: Identifying regulations containing outdated technologies\n",
    "\n",
    "#### Members Names: Shaofang Xu, Dongrui Zhang\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "#### Problem Description: \n",
    "\n",
    "The government faces a heavy burden of regulations make reference to outdated technologies by finding such words as: paper, scan, print, mail, fax, signature, written, pen ,pencil, ink, physical, carbon copy, hard copy, original copy, in person, proof, signed, notary, on site, file, filing, submit, submission(non-electronic), document (excluding electronic).\n",
    "\n",
    "#### Context of the Problem: \n",
    "\n",
    "For updating the regulations fitting to modern world that electronic device are used more widely and conveniently, our project attempts to provide a mean of finding these regulations. \n",
    "\n",
    "#### Limitation About other Approaches:\n",
    "\n",
    "We tried to use name entity tagger on some text with such kinds of words and phrases form Justice Canada, then generate a score based on some heuristic. But we found the score only sovled sequence labeling task for words, not very helpful for classifying sentences and documents which refered to outdated technologies.\n",
    "\n",
    "#### Solution: \n",
    "\n",
    "Maximum Entropy Classifier (MaxEnt) and Logistic regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Explain the related work using the following table\n",
    "\n",
    "| Reference |Explanation |  Dataset/Input |Weakness\n",
    "| --- | --- | --- | --- |\n",
    "| Miles Osborne et al. [1] | They showed how maximum entropy could be used for sentence extraction, that adding prior could deal with the categorical nature of the features | NA | Costly to produce\n",
    "| John Mount et al. [2] | They show that the simpler derivation already given is a very good way to demonstrate the equivalence of logistic regression and maximum entropy modeling| NA | The sigmoid form is less trouble than appealing to maximum entropy\n",
    "\n",
    "\n",
    "The last row in this table should be about the method discussed in this paper (If you can't find the weakenss of this method then write about the future improvement, see the future work section of the paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "1. Build datasets:\n",
    "\n",
    "1) Parser extracts all text XML format from the Justice Canada FTP server (ftp://205.193.86.89/) and remove stop words.\n",
    "\n",
    "2) Create function for generating sentence table that involves technologies in the keywords.\n",
    "\n",
    "3) Review and label the tables manually.\n",
    "\n",
    "2. Train and test datasets:\n",
    "\n",
    "1) Split the features and the label, and use k-fold Cross-Validation method\n",
    "\n",
    "2) Train the datasets by Maximum Entropy probability distribution\n",
    "\n",
    "3) Train the datasets by Logistic distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "In this section, you will provide the code and its explanation. You may have to create more cells after this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This parser extracts all text from XML in one field alongside some other fields in the form of Python dictionary \n",
    "object.\n",
    "\n",
    "@author: Shariyar\n",
    "\n",
    "\"\"\"\n",
    "import xml.etree.ElementTree\n",
    "\n",
    "def updateContent(child: xml.etree.ElementTree, content:str):\n",
    "    if (child.text is not None):\n",
    "        content=content+child.text+\" \"\n",
    "    if (child.tail is not None):\n",
    "        content=content+child.tail+ \" \"\n",
    "    return content\n",
    "            \n",
    "    \n",
    "# Funnction to parse regulations only\n",
    "def parseRegulation(root: xml.etree.ElementTree):\n",
    "    '''\n",
    "      Parses regulation based xml file\n",
    "      :param root: xml node of type xml.etree.ElementTree\n",
    "      :returns dictionary of keys and values\n",
    "    '''\n",
    "    xRefXternal=list()\n",
    "    content=\"\"\n",
    "    modifiedYear=\"NA\"\n",
    "    regYear=\"NA\"\n",
    "    consolidationYear=\"NA\"\n",
    "    #dt=\"\"\n",
    "    #modFlag=False\n",
    "    xmlDict=dict()\n",
    "    \n",
    "    \n",
    "    for child in root.iter():\n",
    "        #print (child.tag,child.text,child.tail)\n",
    "        if child.tag==\"XRefExternal\":\n",
    "            txt=\"\"\n",
    "            if child.text is None:\n",
    "                if child[0].tail is not None:\n",
    "                    txt=child[0].tail\n",
    "                else:\n",
    "                     txt=child[0].text\n",
    "            else:\n",
    "                txt=child.text\n",
    "            xRefXternal.append(txt)\n",
    "            content=updateContent(child,content)\n",
    "            #content+=txt+\" \"\n",
    "       # get modifed date \n",
    "        elif (\"ModifiedDate\" in child.tag):\n",
    "         \n",
    "            dateTag=child[0]\n",
    "            #print(date[1].text)\n",
    "            modifiedYear=dateTag[0].text\n",
    "        elif (child.tag==\"RegistrationDate\"):\n",
    "            dateTag=child[0]\n",
    "            #print(date[1].text)\n",
    "            regYear=dateTag[0].text\n",
    "        elif (child.tag==\"ConsolidationDate\"):\n",
    "            dateTag=child[0]\n",
    "            #print(date[1].text)\n",
    "            consolidationYear=dateTag[0].text \n",
    "        elif (child.tag==\"InstrumentNumber\"):\n",
    "            xmlDict[\"instrumentNumber\"]=child.text\n",
    "        elif (child.tag==\"ShortTitle\"):\n",
    "            xmlDict[\"shorttitle\"]=child.text\n",
    "        elif (child.tag==\"RegulationMaker\"):\n",
    "            xmlDict[\"regulationmaker\"]=child.text\n",
    "        elif (child.tag==\"LongTitle\"):\n",
    "            xmlDict[\"longtitle\"]=child.text \n",
    "            content+=child.text+\". \"\n",
    "        elif (child.tag==\"TitleText\"):\n",
    "            if child.text is None:\n",
    "                if len(child)>0  and child[0].text is not None:\n",
    "                    content+=child[0].text+\". \"\n",
    "            else:\n",
    "                content+=child.text+\". \"\n",
    "       \n",
    "            \n",
    "        elif child.tag==\"MM\" or child.tag==\"DD\" or child.tag==\"YYYY\"  or child.tag==\"Label\":\n",
    "            continue;\n",
    "        elif child.tag==\"Repealed\":\n",
    "            # print (child.text)\n",
    "            return None # igonre reglations with repeal keywords\n",
    "            \n",
    "        else:\n",
    "            content=updateContent(child,content)\n",
    "            #print(child.tag,child.text)\n",
    "            '''if (child.text is not None):\n",
    "            \n",
    "                content=content+child.text+\" \"\n",
    "            if (child.tail is not None):\n",
    "                content=content+child.tail+ \" \"'''\n",
    "                \n",
    "           \n",
    "    xmlDict[\"modifiedyear\"]=modifiedYear\n",
    "    xmlDict[\"registrationyear\"]=regYear\n",
    "    xmlDict[\"consolidationyear\"]=consolidationYear\n",
    "    xmlDict[\"xrefxternal\"]=xRefXternal\n",
    "    xmlDict[\"content\"]=content\n",
    "    \n",
    "    \n",
    "    return (xmlDict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseReg(directory, filename):\n",
    "    path=os.path.join(directory, filename)\n",
    "    tree=ET.parse(path)\n",
    "    d=parseRegulation(tree.getroot())\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory=\"regulations\"\n",
    "\n",
    "# List of stemmed keywords\n",
    "keyWords = ['paper', 'scan', 'print', 'mail', 'fax', 'signatur', 'written', 'pen', 'pencil', 'ink', 'physic',\n",
    "            'carbon copi', 'hard copi', 'hardcopi', 'origin copi', 'in person', 'proof', 'sign', 'notari', 'on site', \n",
    "            'file', 'submit', 'submiss', 'document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function for generating sentence table that involves technologies in the keywords.\n",
    "def buildSentTable(directory, kw):\n",
    "    \n",
    "    doc = pd.DataFrame()\n",
    "    # Initialize stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    kw_str = '|'.join(kw)\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "\n",
    "        try:        \n",
    "            d = parseReg(directory, filename)\n",
    "            num_sent = 0\n",
    "            sents = nltk.sent_tokenize(d['content'])\n",
    "            del d['content']\n",
    "            \n",
    "            for sent in sents:\n",
    "                \n",
    "                # Extract sentences if any key word(s) present(s)\n",
    "                if re.search(kw_str, stemmer.stem(sent)):\n",
    "                    \n",
    "                    num_sent += 1\n",
    "                    d['Related Sentence'] = sent\n",
    "                    d['Sentence #'] = num_sent\n",
    "                    d['Regulation Filename'] = filename\n",
    "                    doc = doc.append(d, ignore_index=True)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sentance table for manual review\n",
    "sentTable = buildSentTable(directory, keyWords)\n",
    "sentTable_pkl = sentTable.to_pickle('sentTable_v2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>Related Sentence</th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>consolidationyear</th>\n",
       "      <th>instrumentNumber</th>\n",
       "      <th>longtitle</th>\n",
       "      <th>modifiedyear</th>\n",
       "      <th>registrationyear</th>\n",
       "      <th>shorttitle</th>\n",
       "      <th>xrefxternal</th>\n",
       "      <th>regulationmaker</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2095</td>\n",
       "      <td>If a request for review is submitted to a comp...</td>\n",
       "      <td>10</td>\n",
       "      <td>2018</td>\n",
       "      <td>SI/2017-41</td>\n",
       "      <td>Proclamation giving notice of the entry into f...</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['OLD AGE SECURITY ACT', 'Old Age Security Act...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6320</td>\n",
       "      <td>Content of application Unless these Rules prov...</td>\n",
       "      <td>77</td>\n",
       "      <td>2018</td>\n",
       "      <td>SOR/2012-256</td>\n",
       "      <td>Refugee Protection Division Rules</td>\n",
       "      <td>2012</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['IMMIGRATION AND REFUGEE PROTECTION ACT', 'Im...</td>\n",
       "      <td>P.C.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4687</td>\n",
       "      <td>The master of a vessel carrying solid bulk car...</td>\n",
       "      <td>11</td>\n",
       "      <td>2018</td>\n",
       "      <td>SOR/2007-128</td>\n",
       "      <td>Cargo, Fumigation and Tackle Regulations</td>\n",
       "      <td>2007</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['CANADA SHIPPING ACT, 2001', 'Canada Shipping...</td>\n",
       "      <td>P.C.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9934</td>\n",
       "      <td>The notice referred to in subsection (1) shall...</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>SOR/92-677</td>\n",
       "      <td>Regulations Respecting the Confirmation of Spe...</td>\n",
       "      <td>2008</td>\n",
       "      <td>1992.0</td>\n",
       "      <td>Specific Agreement Confirmation Regulations</td>\n",
       "      <td>['INDIAN LANDS AGREEMENT (1986) ACT', 'Indian ...</td>\n",
       "      <td>P.C.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7868</td>\n",
       "      <td>Termination for failure to meet deadlines The ...</td>\n",
       "      <td>7</td>\n",
       "      <td>2018</td>\n",
       "      <td>SOR/2015-167</td>\n",
       "      <td>Mutual Property and Casualty Insurance Company...</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['INSURANCE COMPANIES ACT', 'Insurance Compani...</td>\n",
       "      <td>P.C.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    id                                   Related Sentence  Sentence #  \\\n",
       "0      0  2095  If a request for review is submitted to a comp...          10   \n",
       "1      1  6320  Content of application Unless these Rules prov...          77   \n",
       "2      2  4687  The master of a vessel carrying solid bulk car...          11   \n",
       "3      3  9934  The notice referred to in subsection (1) shall...           1   \n",
       "4      4  7868  Termination for failure to meet deadlines The ...           7   \n",
       "\n",
       "   consolidationyear instrumentNumber  \\\n",
       "0               2018       SI/2017-41   \n",
       "1               2018     SOR/2012-256   \n",
       "2               2018     SOR/2007-128   \n",
       "3               2018       SOR/92-677   \n",
       "4               2018     SOR/2015-167   \n",
       "\n",
       "                                           longtitle  modifiedyear  \\\n",
       "0  Proclamation giving notice of the entry into f...          2017   \n",
       "1                  Refugee Protection Division Rules          2012   \n",
       "2           Cargo, Fumigation and Tackle Regulations          2007   \n",
       "3  Regulations Respecting the Confirmation of Spe...          2008   \n",
       "4  Mutual Property and Casualty Insurance Company...          2015   \n",
       "\n",
       "   registrationyear                                   shorttitle  \\\n",
       "0            2017.0                                          NaN   \n",
       "1            2012.0                                          NaN   \n",
       "2            2007.0                                          NaN   \n",
       "3            1992.0  Specific Agreement Confirmation Regulations   \n",
       "4            2015.0                                          NaN   \n",
       "\n",
       "                                         xrefxternal regulationmaker  label  \n",
       "0  ['OLD AGE SECURITY ACT', 'Old Age Security Act...             NaN      0  \n",
       "1  ['IMMIGRATION AND REFUGEE PROTECTION ACT', 'Im...            P.C.      0  \n",
       "2  ['CANADA SHIPPING ACT, 2001', 'Canada Shipping...            P.C.      0  \n",
       "3  ['INDIAN LANDS AGREEMENT (1986) ACT', 'Indian ...            P.C.      0  \n",
       "4  ['INSURANCE COMPANIES ACT', 'Insurance Compani...            P.C.      0  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import reviewed sentence table\n",
    "reviewedTable = pd.read_pickle('reviewed.pkl')\n",
    "reviewedTable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function for generating words within sentences generated in the previous reviewed sentence table with associated features.\n",
    "def buildWordTable(sent_df, kw):\n",
    "    \n",
    "    doc = pd.DataFrame()\n",
    "    features = dict()\n",
    "    \n",
    "    # Initialize stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # Pretrained statistical models in spaCy. It assigns context-specific token vectors, POS tags, \n",
    "    # dependency parse and named entities.\n",
    "    nlp = en_core_web_sm.load()\n",
    "    kw_str = '|'.join(kw)\n",
    "    cols = list(sent_df.columns)\n",
    "    \n",
    "    for s in range(len(sent_df)):\n",
    "        \n",
    "        for col in ['instrumentNumber', 'Sentence #', 'label', 'modifiedyear', 'registrationyear']:\n",
    "            features[col] = sent_df.iloc[s, cols.index(col)]\n",
    "        \n",
    "        sent = sent_df.iloc[s, cols.index('Related Sentence')]\n",
    "        doc_nlp = nlp(sent)\n",
    "        l = len(list(doc_nlp))\n",
    "\n",
    "        for i in range(l):\n",
    "\n",
    "            txt1W = doc_nlp[i]\n",
    "            stem_txt1W = stemmer.stem(txt1W.text)\n",
    "            # Create 2-word phrase\n",
    "            txt2W = [doc_nlp[i], doc_nlp[min(i+1, l-1)]]\n",
    "            \n",
    "            if stem_txt1W in kw or stemmer.stem(txt2W[0].text+\" \"+txt2W[1].text) in kw:\n",
    "                \n",
    "                for j in range(1,4):\n",
    "                \n",
    "                    # Add features for previous word(s)\n",
    "                    if i == j-1:\n",
    "                        features[\"previous \" + str(j) + \" word\"]=''\n",
    "                        features[\"previous \" + str(j) + \" POS\"]=''\n",
    "                        features[\"previous \" + str(j) + \" POS tag\"]=''\n",
    "                        features[\"previous \" + str(j) + \" Entity tag\"]=''\n",
    "                    else:\n",
    "                        features[\"previous \" + str(j) + \" word\"]=doc_nlp[i-j].text\n",
    "                        features[\"previous \" + str(j) + \" POS\"]=doc_nlp[i-j].pos_\n",
    "                        features[\"previous \" + str(j) + \" POS tag\"]=doc_nlp[i-j].tag_\n",
    "                        if doc_nlp[i-j].ent_iob_ != 'O':\n",
    "                            features[\"previous \" + str(j) + \" Entity tag\"] = doc_nlp[i-1].ent_type_\n",
    "                        else:\n",
    "                            features[\"previous \" + str(j) + \" Entity tag\"] = 'O'\n",
    "                            \n",
    "                # Add other features\n",
    "                if stem_txt1W in kw:\n",
    "                    features['Original word'] = txt1W.text\n",
    "                    features['Stemmed word'] = stem_txt1W\n",
    "                    features['lemma'] = txt1W.lemma_\n",
    "                    features[\"isCapital\"] = txt1W.text[0].upper()==txt1W.text[0]\n",
    "                    features['Dependency'] = txt1W.dep_ # Syntactic dependency, i.e. the relation between tokens.\n",
    "                    # features['Shape'] = txt1W.shape_ # The word shape – capitalization, punctuation, digits.\n",
    "                    # features['Is Alpha char'] = txt1W.is_alpha # Is the token an alpha character? (a letter of the alphabet)\n",
    "                    # features['Is Stop word'] = txt1W.is_stop\n",
    "                    features['POS'] = txt1W.pos_\n",
    "                    features['POS Tag'] = txt1W.tag_\n",
    "                else:\n",
    "                    features['Original word'] = txt2W[0].text + \" \" + txt2W[1].text\n",
    "                    features['Stemmed word'] = stemmer.stem(features['Original word'])\n",
    "                    features['lemma'] = ' '.join(list(map(lambda x: x.lemma_, txt2W)))\n",
    "                    features[\"isCapital\"] = txt2W[0].text[0].upper()==txt2W[0].text[0]\n",
    "                    features['Dependency'] = txt2W[1].dep_\n",
    "                    features['POS'] = ' '.join(list(map(lambda x: x.pos_, txt2W)))\n",
    "                    features['POS Tag'] = ' '.join(list(map(lambda x: x.tag_, txt2W)))\n",
    "\n",
    "                doc = doc.append(features, ignore_index=True)\n",
    "                \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordTable = buildWordTable(reviewedTable, keyWords)\n",
    "wordTable.to_pickle('data for modeling_v3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dependency</th>\n",
       "      <th>Original word</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS Tag</th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Stemmed word</th>\n",
       "      <th>instrumentNumber</th>\n",
       "      <th>isCapital</th>\n",
       "      <th>label</th>\n",
       "      <th>lemma</th>\n",
       "      <th>modifiedyear</th>\n",
       "      <th>previous 1 Entity tag</th>\n",
       "      <th>previous 1 POS</th>\n",
       "      <th>previous 1 POS tag</th>\n",
       "      <th>previous 1 word</th>\n",
       "      <th>previous 2 Entity tag</th>\n",
       "      <th>previous 2 POS</th>\n",
       "      <th>previous 2 POS tag</th>\n",
       "      <th>previous 2 word</th>\n",
       "      <th>previous 3 Entity tag</th>\n",
       "      <th>previous 3 POS</th>\n",
       "      <th>previous 3 POS tag</th>\n",
       "      <th>previous 3 word</th>\n",
       "      <th>registrationyear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>amod</td>\n",
       "      <td>written</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBN</td>\n",
       "      <td>27.0</td>\n",
       "      <td>written</td>\n",
       "      <td>SOR/90-264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>write</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>O</td>\n",
       "      <td>ADV</td>\n",
       "      <td>WRB</td>\n",
       "      <td>where</td>\n",
       "      <td>O</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "      <td>mechanisms</td>\n",
       "      <td>1990.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>relcl</td>\n",
       "      <td>files</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>file</td>\n",
       "      <td>SOR/86-547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>file</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>O</td>\n",
       "      <td>PRON</td>\n",
       "      <td>WP</td>\n",
       "      <td>who</td>\n",
       "      <td>O</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>person</td>\n",
       "      <td>O</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>a</td>\n",
       "      <td>1986.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>amod</td>\n",
       "      <td>physical</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>16.0</td>\n",
       "      <td>physic</td>\n",
       "      <td>SOR/90-264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>physical</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>O</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>CC</td>\n",
       "      <td>and</td>\n",
       "      <td>O</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>1990.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>pobj</td>\n",
       "      <td>document</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>39.0</td>\n",
       "      <td>document</td>\n",
       "      <td>SOR/2010-277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>document</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>O</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>version</td>\n",
       "      <td>2010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>pobj</td>\n",
       "      <td>document</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>39.0</td>\n",
       "      <td>document</td>\n",
       "      <td>SOR/2010-277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>document</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>O</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>link</td>\n",
       "      <td>2010.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Dependency Original word   POS POS Tag  Sentence # Stemmed word  \\\n",
       "320       amod       written  VERB     VBN        27.0      written   \n",
       "321      relcl         files  VERB     VBZ         1.0         file   \n",
       "322       amod      physical   ADJ      JJ        16.0       physic   \n",
       "323       pobj      document  NOUN      NN        39.0     document   \n",
       "324       pobj      document  NOUN      NN        39.0     document   \n",
       "\n",
       "    instrumentNumber  isCapital  label     lemma  modifiedyear  \\\n",
       "320       SOR/90-264        0.0    1.0     write        2006.0   \n",
       "321       SOR/86-547        0.0    0.0      file        2010.0   \n",
       "322       SOR/90-264        0.0    0.0  physical        2006.0   \n",
       "323     SOR/2010-277        0.0    0.0  document        2010.0   \n",
       "324     SOR/2010-277        0.0    0.0  document        2010.0   \n",
       "\n",
       "    previous 1 Entity tag previous 1 POS previous 1 POS tag previous 1 word  \\\n",
       "320                     O            ADV                WRB           where   \n",
       "321                     O           PRON                 WP             who   \n",
       "322                     O            ADP                 IN              of   \n",
       "323                     O            DET                 DT             the   \n",
       "324                     O            DET                 DT             the   \n",
       "\n",
       "    previous 2 Entity tag previous 2 POS previous 2 POS tag previous 2 word  \\\n",
       "320                     O          PUNCT                  ,               ,   \n",
       "321                     O           NOUN                 NN          person   \n",
       "322                     O          CCONJ                 CC             and   \n",
       "323                     O            ADP                 IN              of   \n",
       "324                     O            ADP                 IN              to   \n",
       "\n",
       "    previous 3 Entity tag previous 3 POS previous 3 POS tag previous 3 word  \\\n",
       "320                     O           NOUN                NNS      mechanisms   \n",
       "321                     O            DET                 DT               a   \n",
       "322                     O          PUNCT                  ,               ,   \n",
       "323                     O           NOUN                 NN         version   \n",
       "324                     O           NOUN                 NN            link   \n",
       "\n",
       "     registrationyear  \n",
       "320            1990.0  \n",
       "321            1986.0  \n",
       "322            1990.0  \n",
       "323            2010.0  \n",
       "324            2010.0  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 50)\n",
    "wordTable.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train by Maximum Entropy Classifier method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import maxent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "wordTable = pd.read_pickle('data for modeling_v3.pkl')\n",
    "\n",
    "# Train and test using all features\n",
    "data = wordTable.set_index(['Sentence #', 'instrumentNumber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Dependency': 'advcl',\n",
       "  'Original word': 'submitted',\n",
       "  'POS': 'VERB',\n",
       "  'POS Tag': 'VBN',\n",
       "  'Stemmed word': 'submit',\n",
       "  'isCapital': 0.0,\n",
       "  'label': 0.0,\n",
       "  'lemma': 'submit',\n",
       "  'modifiedyear': 2017.0,\n",
       "  'previous 1 Entity tag': 'O',\n",
       "  'previous 1 POS': 'AUX',\n",
       "  'previous 1 POS tag': 'VBZ',\n",
       "  'previous 1 word': 'is',\n",
       "  'previous 2 Entity tag': 'O',\n",
       "  'previous 2 POS': 'NOUN',\n",
       "  'previous 2 POS tag': 'NN',\n",
       "  'previous 2 word': 'review',\n",
       "  'previous 3 Entity tag': 'O',\n",
       "  'previous 3 POS': 'ADP',\n",
       "  'previous 3 POS tag': 'IN',\n",
       "  'previous 3 word': 'for',\n",
       "  'registrationyear': 2017.0},\n",
       " {'Dependency': 'relcl',\n",
       "  'Original word': 'submitted',\n",
       "  'POS': 'VERB',\n",
       "  'POS Tag': 'VBN',\n",
       "  'Stemmed word': 'submit',\n",
       "  'isCapital': 0.0,\n",
       "  'label': 0.0,\n",
       "  'lemma': 'submit',\n",
       "  'modifiedyear': 2017.0,\n",
       "  'previous 1 Entity tag': 'O',\n",
       "  'previous 1 POS': 'AUX',\n",
       "  'previous 1 POS tag': 'VBD',\n",
       "  'previous 1 word': 'was',\n",
       "  'previous 2 Entity tag': 'O',\n",
       "  'previous 2 POS': 'NOUN',\n",
       "  'previous 2 POS tag': 'NN',\n",
       "  'previous 2 word': 'request',\n",
       "  'previous 3 Entity tag': 'O',\n",
       "  'previous 3 POS': 'DET',\n",
       "  'previous 3 POS tag': 'DT',\n",
       "  'previous 3 word': 'the',\n",
       "  'registrationyear': 2017.0},\n",
       " {'Dependency': 'amod',\n",
       "  'Original word': 'written',\n",
       "  'POS': 'VERB',\n",
       "  'POS Tag': 'VBN',\n",
       "  'Stemmed word': 'written',\n",
       "  'isCapital': 0.0,\n",
       "  'label': 0.0,\n",
       "  'lemma': 'write',\n",
       "  'modifiedyear': 2012.0,\n",
       "  'previous 1 Entity tag': 'O',\n",
       "  'previous 1 POS': 'DET',\n",
       "  'previous 1 POS tag': 'DT',\n",
       "  'previous 1 word': 'a',\n",
       "  'previous 2 Entity tag': 'O',\n",
       "  'previous 2 POS': 'ADP',\n",
       "  'previous 2 POS tag': 'IN',\n",
       "  'previous 2 word': 'in',\n",
       "  'previous 3 Entity tag': 'O',\n",
       "  'previous 3 POS': 'PUNCT',\n",
       "  'previous 3 POS tag': ',',\n",
       "  'previous 3 word': ',',\n",
       "  'registrationyear': 2012.0},\n",
       " {'Dependency': 'dobj',\n",
       "  'Original word': 'document',\n",
       "  'POS': 'NOUN',\n",
       "  'POS Tag': 'NN',\n",
       "  'Stemmed word': 'document',\n",
       "  'isCapital': 0.0,\n",
       "  'label': 0.0,\n",
       "  'lemma': 'document',\n",
       "  'modifiedyear': 2007.0,\n",
       "  'previous 1 Entity tag': 'O',\n",
       "  'previous 1 POS': 'DET',\n",
       "  'previous 1 POS tag': 'DT',\n",
       "  'previous 1 word': 'a',\n",
       "  'previous 2 Entity tag': 'O',\n",
       "  'previous 2 POS': 'NOUN',\n",
       "  'previous 2 POS tag': 'NN',\n",
       "  'previous 2 word': 'board',\n",
       "  'previous 3 Entity tag': 'O',\n",
       "  'previous 3 POS': 'ADP',\n",
       "  'previous 3 POS tag': 'IN',\n",
       "  'previous 3 word': 'on',\n",
       "  'registrationyear': 2007.0},\n",
       " {'Dependency': 'relcl',\n",
       "  'Original word': 'submitted',\n",
       "  'POS': 'VERB',\n",
       "  'POS Tag': 'VBN',\n",
       "  'Stemmed word': 'submit',\n",
       "  'isCapital': 0.0,\n",
       "  'label': 0.0,\n",
       "  'lemma': 'submit',\n",
       "  'modifiedyear': 2008.0,\n",
       "  'previous 1 Entity tag': 'O',\n",
       "  'previous 1 POS': 'AUX',\n",
       "  'previous 1 POS tag': 'VB',\n",
       "  'previous 1 word': 'be',\n",
       "  'previous 2 Entity tag': 'O',\n",
       "  'previous 2 POS': 'PART',\n",
       "  'previous 2 POS tag': 'TO',\n",
       "  'previous 2 word': 'to',\n",
       "  'previous 3 Entity tag': 'O',\n",
       "  'previous 3 POS': 'NOUN',\n",
       "  'previous 3 POS tag': 'NN',\n",
       "  'previous 3 word': 'question',\n",
       "  'registrationyear': 1992.0},\n",
       " {'Dependency': 'nsubjpass',\n",
       "  'Original word': 'documents',\n",
       "  'POS': 'NOUN',\n",
       "  'POS Tag': 'NNS',\n",
       "  'Stemmed word': 'document',\n",
       "  'isCapital': 0.0,\n",
       "  'label': 0.0,\n",
       "  'lemma': 'document',\n",
       "  'modifiedyear': 2015.0,\n",
       "  'previous 1 Entity tag': 'O',\n",
       "  'previous 1 POS': 'VERB',\n",
       "  'previous 1 POS tag': 'VBN',\n",
       "  'previous 1 word': 'required',\n",
       "  'previous 2 Entity tag': 'O',\n",
       "  'previous 2 POS': 'DET',\n",
       "  'previous 2 POS tag': 'DT',\n",
       "  'previous 2 word': 'the',\n",
       "  'previous 3 Entity tag': 'O',\n",
       "  'previous 3 POS': 'SCONJ',\n",
       "  'previous 3 POS tag': 'IN',\n",
       "  'previous 3 word': 'if',\n",
       "  'registrationyear': 2015.0},\n",
       " {'Dependency': 'advcl',\n",
       "  'Original word': 'submitted',\n",
       "  'POS': 'VERB',\n",
       "  'POS Tag': 'VBN',\n",
       "  'Stemmed word': 'submit',\n",
       "  'isCapital': 0.0,\n",
       "  'label': 0.0,\n",
       "  'lemma': 'submit',\n",
       "  'modifiedyear': 2015.0,\n",
       "  'previous 1 Entity tag': 'O',\n",
       "  'previous 1 POS': 'PART',\n",
       "  'previous 1 POS tag': 'RB',\n",
       "  'previous 1 word': 'not',\n",
       "  'previous 2 Entity tag': 'O',\n",
       "  'previous 2 POS': 'AUX',\n",
       "  'previous 2 POS tag': 'VBP',\n",
       "  'previous 2 word': 'are',\n",
       "  'previous 3 Entity tag': 'O',\n",
       "  'previous 3 POS': 'NOUN',\n",
       "  'previous 3 POS tag': 'NNS',\n",
       "  'previous 3 word': 'documents',\n",
       "  'registrationyear': 2015.0},\n",
       " {'Dependency': 'acl',\n",
       "  'Original word': 'submitting',\n",
       "  'POS': 'NOUN',\n",
       "  'POS Tag': 'NN',\n",
       "  'Stemmed word': 'submit',\n",
       "  'isCapital': 0.0,\n",
       "  'label': 1.0,\n",
       "  'lemma': 'submitting',\n",
       "  'modifiedyear': 2007.0,\n",
       "  'previous 1 Entity tag': 'O',\n",
       "  'previous 1 POS': 'NOUN',\n",
       "  'previous 1 POS tag': 'NN',\n",
       "  'previous 1 word': 'party',\n",
       "  'previous 2 Entity tag': 'O',\n",
       "  'previous 2 POS': 'DET',\n",
       "  'previous 2 POS tag': 'DT',\n",
       "  'previous 2 word': 'A',\n",
       "  'previous 3 Entity tag': 'O',\n",
       "  'previous 3 POS': 'NOUN',\n",
       "  'previous 3 POS tag': 'NN',\n",
       "  'previous 3 word': 'transcript',\n",
       "  'registrationyear': 2007.0},\n",
       " {'Dependency': 'pobj',\n",
       "  'Original word': 'document',\n",
       "  'POS': 'NOUN',\n",
       "  'POS Tag': 'NN',\n",
       "  'Stemmed word': 'document',\n",
       "  'isCapital': 0.0,\n",
       "  'label': 0.0,\n",
       "  'lemma': 'document',\n",
       "  'modifiedyear': 2016.0,\n",
       "  'previous 1 Entity tag': 'O',\n",
       "  'previous 1 POS': 'DET',\n",
       "  'previous 1 POS tag': 'DT',\n",
       "  'previous 1 word': 'a',\n",
       "  'previous 2 Entity tag': 'O',\n",
       "  'previous 2 POS': 'ADP',\n",
       "  'previous 2 POS tag': 'IN',\n",
       "  'previous 2 word': 'in',\n",
       "  'previous 3 Entity tag': 'O',\n",
       "  'previous 3 POS': 'ADP',\n",
       "  'previous 3 POS tag': 'RP',\n",
       "  'previous 3 word': 'out',\n",
       "  'registrationyear': 2016.0},\n",
       " {'Dependency': 'amod',\n",
       "  'Original word': 'written',\n",
       "  'POS': 'VERB',\n",
       "  'POS Tag': 'VBN',\n",
       "  'Stemmed word': 'written',\n",
       "  'isCapital': 0.0,\n",
       "  'label': 1.0,\n",
       "  'lemma': 'write',\n",
       "  'modifiedyear': 2017.0,\n",
       "  'previous 1 Entity tag': 'O',\n",
       "  'previous 1 POS': 'VERB',\n",
       "  'previous 1 POS tag': 'VB',\n",
       "  'previous 1 word': 'serve',\n",
       "  'previous 2 Entity tag': 'O',\n",
       "  'previous 2 POS': 'VERB',\n",
       "  'previous 2 POS tag': 'MD',\n",
       "  'previous 2 word': 'must',\n",
       "  'previous 3 Entity tag': 'O',\n",
       "  'previous 3 POS': 'NOUN',\n",
       "  'previous 3 POS tag': 'NN',\n",
       "  'previous 3 word': 'record',\n",
       "  'registrationyear': 2002.0}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = data.to_dict(orient='records')\n",
    "dic[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Dependency</th>\n",
       "      <th>Original word</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS Tag</th>\n",
       "      <th>Stemmed word</th>\n",
       "      <th>isCapital</th>\n",
       "      <th>label</th>\n",
       "      <th>lemma</th>\n",
       "      <th>modifiedyear</th>\n",
       "      <th>previous 1 Entity tag</th>\n",
       "      <th>...</th>\n",
       "      <th>previous 1 word</th>\n",
       "      <th>previous 2 Entity tag</th>\n",
       "      <th>previous 2 POS</th>\n",
       "      <th>previous 2 POS tag</th>\n",
       "      <th>previous 2 word</th>\n",
       "      <th>previous 3 Entity tag</th>\n",
       "      <th>previous 3 POS</th>\n",
       "      <th>previous 3 POS tag</th>\n",
       "      <th>previous 3 word</th>\n",
       "      <th>registrationyear</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence #</th>\n",
       "      <th>instrumentNumber</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">10.0</th>\n",
       "      <th>SI/2017-41</th>\n",
       "      <td>advcl</td>\n",
       "      <td>submitted</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBN</td>\n",
       "      <td>submit</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>submit</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>O</td>\n",
       "      <td>...</td>\n",
       "      <td>is</td>\n",
       "      <td>O</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>review</td>\n",
       "      <td>O</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>for</td>\n",
       "      <td>2017.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SI/2017-41</th>\n",
       "      <td>relcl</td>\n",
       "      <td>submitted</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBN</td>\n",
       "      <td>submit</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>submit</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>O</td>\n",
       "      <td>...</td>\n",
       "      <td>was</td>\n",
       "      <td>O</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>request</td>\n",
       "      <td>O</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>the</td>\n",
       "      <td>2017.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77.0</th>\n",
       "      <th>SOR/2012-256</th>\n",
       "      <td>amod</td>\n",
       "      <td>written</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBN</td>\n",
       "      <td>written</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>write</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>O</td>\n",
       "      <td>...</td>\n",
       "      <td>a</td>\n",
       "      <td>O</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>2012.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11.0</th>\n",
       "      <th>SOR/2007-128</th>\n",
       "      <td>dobj</td>\n",
       "      <td>document</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>document</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>document</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>O</td>\n",
       "      <td>...</td>\n",
       "      <td>a</td>\n",
       "      <td>O</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>board</td>\n",
       "      <td>O</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>on</td>\n",
       "      <td>2007.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <th>SOR/92-677</th>\n",
       "      <td>relcl</td>\n",
       "      <td>submitted</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBN</td>\n",
       "      <td>submit</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>submit</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>O</td>\n",
       "      <td>...</td>\n",
       "      <td>be</td>\n",
       "      <td>O</td>\n",
       "      <td>PART</td>\n",
       "      <td>TO</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>question</td>\n",
       "      <td>1992.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Dependency Original word   POS POS Tag  \\\n",
       "Sentence # instrumentNumber                                          \n",
       "10.0       SI/2017-41            advcl     submitted  VERB     VBN   \n",
       "           SI/2017-41            relcl     submitted  VERB     VBN   \n",
       "77.0       SOR/2012-256           amod       written  VERB     VBN   \n",
       "11.0       SOR/2007-128           dobj      document  NOUN      NN   \n",
       "1.0        SOR/92-677            relcl     submitted  VERB     VBN   \n",
       "\n",
       "                            Stemmed word  isCapital  label     lemma  \\\n",
       "Sentence # instrumentNumber                                            \n",
       "10.0       SI/2017-41             submit        0.0    0.0    submit   \n",
       "           SI/2017-41             submit        0.0    0.0    submit   \n",
       "77.0       SOR/2012-256          written        0.0    0.0     write   \n",
       "11.0       SOR/2007-128         document        0.0    0.0  document   \n",
       "1.0        SOR/92-677             submit        0.0    0.0    submit   \n",
       "\n",
       "                             modifiedyear previous 1 Entity tag  ...  \\\n",
       "Sentence # instrumentNumber                                      ...   \n",
       "10.0       SI/2017-41              2017.0                     O  ...   \n",
       "           SI/2017-41              2017.0                     O  ...   \n",
       "77.0       SOR/2012-256            2012.0                     O  ...   \n",
       "11.0       SOR/2007-128            2007.0                     O  ...   \n",
       "1.0        SOR/92-677              2008.0                     O  ...   \n",
       "\n",
       "                            previous 1 word previous 2 Entity tag  \\\n",
       "Sentence # instrumentNumber                                         \n",
       "10.0       SI/2017-41                    is                     O   \n",
       "           SI/2017-41                   was                     O   \n",
       "77.0       SOR/2012-256                   a                     O   \n",
       "11.0       SOR/2007-128                   a                     O   \n",
       "1.0        SOR/92-677                    be                     O   \n",
       "\n",
       "                            previous 2 POS previous 2 POS tag previous 2 word  \\\n",
       "Sentence # instrumentNumber                                                     \n",
       "10.0       SI/2017-41                 NOUN                 NN          review   \n",
       "           SI/2017-41                 NOUN                 NN         request   \n",
       "77.0       SOR/2012-256                ADP                 IN              in   \n",
       "11.0       SOR/2007-128               NOUN                 NN           board   \n",
       "1.0        SOR/92-677                 PART                 TO              to   \n",
       "\n",
       "                            previous 3 Entity tag previous 3 POS  \\\n",
       "Sentence # instrumentNumber                                        \n",
       "10.0       SI/2017-41                           O            ADP   \n",
       "           SI/2017-41                           O            DET   \n",
       "77.0       SOR/2012-256                         O          PUNCT   \n",
       "11.0       SOR/2007-128                         O            ADP   \n",
       "1.0        SOR/92-677                           O           NOUN   \n",
       "\n",
       "                            previous 3 POS tag previous 3 word  \\\n",
       "Sentence # instrumentNumber                                      \n",
       "10.0       SI/2017-41                       IN             for   \n",
       "           SI/2017-41                       DT             the   \n",
       "77.0       SOR/2012-256                      ,               ,   \n",
       "11.0       SOR/2007-128                     IN              on   \n",
       "1.0        SOR/92-677                       NN        question   \n",
       "\n",
       "                            registrationyear  \n",
       "Sentence # instrumentNumber                   \n",
       "10.0       SI/2017-41                 2017.0  \n",
       "           SI/2017-41                 2017.0  \n",
       "77.0       SOR/2012-256               2012.0  \n",
       "11.0       SOR/2007-128               2007.0  \n",
       "1.0        SOR/92-677                 1992.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(df):\n",
    "    \n",
    "    dic = df.to_dict(orient='records')\n",
    "    \n",
    "    train1 = [list() for i in range(5)]\n",
    "    test1 = [list() for i in range(5)]\n",
    "    trainFeature = [list() for i in range(5)]\n",
    "    trainLabel = [list() for i in range(5)]\n",
    "    testLabel = [list() for i in range(5)]\n",
    "    \n",
    "    trainInx = [list() for i in range(5)]\n",
    "    testInx = [list() for i in range(5)]\n",
    "    \n",
    "    lst = list(range(len(dic)))\n",
    "    tLen = round(len(dic)*0.2)\n",
    "    \n",
    "    for j in range(5):\n",
    "        \n",
    "        testInx[j] = lst[j*tLen:(j+1)*tLen]\n",
    "        trainInx[j] = lst[:j*tLen] + lst[(j+1)*tLen:]\n",
    "        \n",
    "        for t in testInx[j]:\n",
    "            features = dic[t].copy()\n",
    "            testLabel[j].append(dic[t]['label'])\n",
    "            features.pop('label')\n",
    "            test1[j].append(features)\n",
    "\n",
    "        for i in trainInx[j]:\n",
    "            features = dic[i].copy()\n",
    "            label = dic[i]['label']\n",
    "            trainLabel[j].append(label)\n",
    "            features.pop('label')\n",
    "            trainFeature[j].append(features)\n",
    "            train1[j].append((features, label))\n",
    "            \n",
    "    return train1, trainFeature, trainLabel, test1, testLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxEntTrain(df):\n",
    "    \n",
    "    train1, trainFeature, trainLabel, test1, testLabel = splitData(df)\n",
    "    acc_hist = []\n",
    "    \n",
    "    for j in range(5):\n",
    "        \n",
    "        classifier = maxent.MaxentClassifier.train(train1[j])\n",
    "        pred = classifier.classify_many(test1[j])\n",
    "        \n",
    "        match = sum([1 if pred[x] == testLabel[j][x] else 0 for x in range(len(pred))])\n",
    "        acc = match/len(pred)\n",
    "        acc_hist.append(acc)\n",
    "        print(\"\\nThe accuracy is {:.2%}\".format(acc))\n",
    "        \n",
    "    return acc_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.208\n",
      "             2          -0.36218        0.792\n",
      "             3          -0.32496        0.800\n",
      "             4          -0.29493        0.838\n",
      "             5          -0.27086        0.873\n",
      "             6          -0.25119        0.888\n",
      "             7          -0.23477        0.923\n",
      "             8          -0.22080        0.927\n",
      "             9          -0.20871        0.931\n",
      "            10          -0.19811        0.942\n",
      "            11          -0.18871        0.954\n",
      "            12          -0.18030        0.958\n",
      "            13          -0.17272        0.962\n",
      "            14          -0.16584        0.969\n",
      "            15          -0.15956        0.969\n",
      "            16          -0.15379        0.973\n",
      "            17          -0.14848        0.977\n",
      "            18          -0.14356        0.977\n",
      "            19          -0.13899        0.977\n",
      "            20          -0.13473        0.977\n",
      "            21          -0.13074        0.977\n",
      "            22          -0.12701        0.977\n",
      "            23          -0.12351        0.977\n",
      "            24          -0.12020        0.977\n",
      "            25          -0.11709        0.988\n",
      "            26          -0.11415        0.988\n",
      "            27          -0.11136        0.988\n",
      "            28          -0.10871        0.988\n",
      "            29          -0.10620        0.988\n",
      "            30          -0.10381        0.988\n",
      "            31          -0.10153        0.988\n",
      "            32          -0.09936        0.988\n",
      "            33          -0.09729        0.992\n",
      "            34          -0.09530        0.992\n",
      "            35          -0.09340        0.992\n",
      "            36          -0.09158        0.992\n",
      "            37          -0.08984        0.992\n",
      "            38          -0.08816        0.992\n",
      "            39          -0.08655        0.992\n",
      "            40          -0.08501        0.992\n",
      "            41          -0.08352        0.992\n",
      "            42          -0.08208        0.992\n",
      "            43          -0.08070        0.992\n",
      "            44          -0.07936        0.992\n",
      "            45          -0.07807        0.992\n",
      "            46          -0.07683        0.992\n",
      "            47          -0.07562        0.992\n",
      "            48          -0.07446        0.992\n",
      "            49          -0.07333        0.992\n",
      "            50          -0.07224        0.996\n",
      "            51          -0.07119        0.996\n",
      "            52          -0.07016        0.996\n",
      "            53          -0.06917        0.996\n",
      "            54          -0.06821        0.996\n",
      "            55          -0.06727        0.996\n",
      "            56          -0.06636        0.996\n",
      "            57          -0.06548        0.996\n",
      "            58          -0.06462        0.996\n",
      "            59          -0.06379        0.996\n",
      "            60          -0.06297        0.996\n",
      "            61          -0.06218        0.996\n",
      "            62          -0.06141        0.996\n",
      "            63          -0.06066        0.996\n",
      "            64          -0.05993        0.996\n",
      "            65          -0.05922        0.996\n",
      "            66          -0.05853        0.996\n",
      "            67          -0.05785        0.996\n",
      "            68          -0.05719        0.996\n",
      "            69          -0.05654        0.996\n",
      "            70          -0.05591        0.996\n",
      "            71          -0.05530        0.996\n",
      "            72          -0.05470        0.996\n",
      "            73          -0.05411        0.996\n",
      "            74          -0.05353        0.996\n",
      "            75          -0.05297        0.996\n",
      "            76          -0.05242        0.996\n",
      "            77          -0.05189        0.996\n",
      "            78          -0.05136        0.996\n",
      "            79          -0.05085        0.996\n",
      "            80          -0.05034        0.996\n",
      "            81          -0.04985        0.996\n",
      "            82          -0.04937        0.996\n",
      "            83          -0.04889        0.996\n",
      "            84          -0.04843        0.996\n",
      "            85          -0.04797        0.996\n",
      "            86          -0.04753        0.996\n",
      "            87          -0.04709        0.996\n",
      "            88          -0.04666        0.996\n",
      "            89          -0.04624        0.996\n",
      "            90          -0.04583        0.996\n",
      "            91          -0.04543        0.996\n",
      "            92          -0.04503        0.996\n",
      "            93          -0.04464        0.996\n",
      "            94          -0.04425        0.996\n",
      "            95          -0.04388        0.996\n",
      "            96          -0.04351        0.996\n",
      "            97          -0.04315        0.996\n",
      "            98          -0.04279        0.996\n",
      "            99          -0.04244        0.996\n",
      "         Final          -0.04209        0.996\n",
      "\n",
      "The accuracy is 63.08%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.169\n",
      "             2          -0.30356        0.831\n",
      "             3          -0.27420        0.831\n",
      "             4          -0.25046        0.865\n",
      "             5          -0.23169        0.885\n",
      "             6          -0.21647        0.896\n",
      "             7          -0.20375        0.915\n",
      "             8          -0.19287        0.927\n",
      "             9          -0.18337        0.931\n",
      "            10          -0.17496        0.931\n",
      "            11          -0.16742        0.938\n",
      "            12          -0.16061        0.950\n",
      "            13          -0.15440        0.946\n",
      "            14          -0.14871        0.950\n",
      "            15          -0.14346        0.954\n",
      "            16          -0.13861        0.969\n",
      "            17          -0.13410        0.969\n",
      "            18          -0.12990        0.969\n",
      "            19          -0.12597        0.973\n",
      "            20          -0.12229        0.973\n",
      "            21          -0.11883        0.977\n",
      "            22          -0.11558        0.985\n",
      "            23          -0.11251        0.992\n",
      "            24          -0.10960        0.992\n",
      "            25          -0.10685        0.992\n",
      "            26          -0.10425        0.992\n",
      "            27          -0.10177        0.992\n",
      "            28          -0.09941        0.992\n",
      "            29          -0.09717        0.992\n",
      "            30          -0.09503        0.996\n",
      "            31          -0.09298        0.996\n",
      "            32          -0.09103        0.996\n",
      "            33          -0.08915        0.996\n",
      "            34          -0.08736        0.996\n",
      "            35          -0.08564        0.996\n",
      "            36          -0.08399        0.996\n",
      "            37          -0.08240        0.996\n",
      "            38          -0.08088        0.996\n",
      "            39          -0.07941        0.996\n",
      "            40          -0.07799        0.996\n",
      "            41          -0.07663        0.996\n",
      "            42          -0.07531        0.996\n",
      "            43          -0.07404        0.996\n",
      "            44          -0.07281        1.000\n",
      "            45          -0.07163        1.000\n",
      "            46          -0.07048        1.000\n",
      "            47          -0.06937        1.000\n",
      "            48          -0.06829        1.000\n",
      "            49          -0.06725        1.000\n",
      "            50          -0.06624        1.000\n",
      "            51          -0.06526        1.000\n",
      "            52          -0.06431        1.000\n",
      "            53          -0.06339        1.000\n",
      "            54          -0.06249        1.000\n",
      "            55          -0.06162        1.000\n",
      "            56          -0.06077        1.000\n",
      "            57          -0.05995        1.000\n",
      "            58          -0.05915        1.000\n",
      "            59          -0.05837        1.000\n",
      "            60          -0.05761        1.000\n",
      "            61          -0.05687        1.000\n",
      "            62          -0.05614        1.000\n",
      "            63          -0.05544        1.000\n",
      "            64          -0.05476        1.000\n",
      "            65          -0.05409        1.000\n",
      "            66          -0.05344        1.000\n",
      "            67          -0.05280        1.000\n",
      "            68          -0.05218        1.000\n",
      "            69          -0.05157        1.000\n",
      "            70          -0.05098        1.000\n",
      "            71          -0.05040        1.000\n",
      "            72          -0.04983        1.000\n",
      "            73          -0.04928        1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            74          -0.04873        1.000\n",
      "            75          -0.04820        1.000\n",
      "            76          -0.04769        1.000\n",
      "            77          -0.04718        1.000\n",
      "            78          -0.04668        1.000\n",
      "            79          -0.04619        1.000\n",
      "            80          -0.04572        1.000\n",
      "            81          -0.04525        1.000\n",
      "            82          -0.04479        1.000\n",
      "            83          -0.04435        1.000\n",
      "            84          -0.04391        1.000\n",
      "            85          -0.04348        1.000\n",
      "            86          -0.04305        1.000\n",
      "            87          -0.04264        1.000\n",
      "            88          -0.04223        1.000\n",
      "            89          -0.04183        1.000\n",
      "            90          -0.04144        1.000\n",
      "            91          -0.04106        1.000\n",
      "            92          -0.04068        1.000\n",
      "            93          -0.04031        1.000\n",
      "            94          -0.03995        1.000\n",
      "            95          -0.03959        1.000\n",
      "            96          -0.03924        1.000\n",
      "            97          -0.03890        1.000\n",
      "            98          -0.03856        1.000\n",
      "            99          -0.03822        1.000\n",
      "         Final          -0.03790        1.000\n",
      "\n",
      "The accuracy is 52.31%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.258\n",
      "             2          -0.43673        0.742\n",
      "             3          -0.39387        0.773\n",
      "             4          -0.36006        0.808\n",
      "             5          -0.33287        0.838\n",
      "             6          -0.31042        0.869\n",
      "             7          -0.29145        0.885\n",
      "             8          -0.27512        0.915\n",
      "             9          -0.26083        0.931\n",
      "            10          -0.24819        0.931\n",
      "            11          -0.23689        0.935\n",
      "            12          -0.22670        0.935\n",
      "            13          -0.21746        0.950\n",
      "            14          -0.20903        0.954\n",
      "            15          -0.20129        0.965\n",
      "            16          -0.19415        0.965\n",
      "            17          -0.18756        0.977\n",
      "            18          -0.18143        0.981\n",
      "            19          -0.17573        0.985\n",
      "            20          -0.17040        0.985\n",
      "            21          -0.16542        0.985\n",
      "            22          -0.16073        0.985\n",
      "            23          -0.15633        0.985\n",
      "            24          -0.15218        0.985\n",
      "            25          -0.14826        0.985\n",
      "            26          -0.14455        0.985\n",
      "            27          -0.14104        0.985\n",
      "            28          -0.13770        0.988\n",
      "            29          -0.13453        0.988\n",
      "            30          -0.13151        0.988\n",
      "            31          -0.12863        0.988\n",
      "            32          -0.12589        0.988\n",
      "            33          -0.12326        0.988\n",
      "            34          -0.12075        0.988\n",
      "            35          -0.11835        0.988\n",
      "            36          -0.11605        0.988\n",
      "            37          -0.11384        0.988\n",
      "            38          -0.11172        0.992\n",
      "            39          -0.10968        0.992\n",
      "            40          -0.10772        0.992\n",
      "            41          -0.10583        0.992\n",
      "            42          -0.10401        0.992\n",
      "            43          -0.10226        0.992\n",
      "            44          -0.10057        0.992\n",
      "            45          -0.09894        0.992\n",
      "            46          -0.09736        0.992\n",
      "            47          -0.09584        0.992\n",
      "            48          -0.09436        0.992\n",
      "            49          -0.09294        0.992\n",
      "            50          -0.09155        0.992\n",
      "            51          -0.09022        0.992\n",
      "            52          -0.08892        0.992\n",
      "            53          -0.08766        0.992\n",
      "            54          -0.08644        0.992\n",
      "            55          -0.08525        0.996\n",
      "            56          -0.08410        0.996\n",
      "            57          -0.08298        0.996\n",
      "            58          -0.08190        0.996\n",
      "            59          -0.08084        0.996\n",
      "            60          -0.07981        0.996\n",
      "            61          -0.07881        0.996\n",
      "            62          -0.07784        0.996\n",
      "            63          -0.07689        0.996\n",
      "            64          -0.07596        0.996\n",
      "            65          -0.07506        0.996\n",
      "            66          -0.07418        0.996\n",
      "            67          -0.07332        0.996\n",
      "            68          -0.07249        0.996\n",
      "            69          -0.07167        0.996\n",
      "            70          -0.07087        0.996\n",
      "            71          -0.07009        0.996\n",
      "            72          -0.06933        0.996\n",
      "            73          -0.06859        0.996\n",
      "            74          -0.06786        0.996\n",
      "            75          -0.06715        0.996\n",
      "            76          -0.06646        0.996\n",
      "            77          -0.06578        0.996\n",
      "            78          -0.06512        0.996\n",
      "            79          -0.06447        0.996\n",
      "            80          -0.06383        0.996\n",
      "            81          -0.06320        0.996\n",
      "            82          -0.06259        0.996\n",
      "            83          -0.06200        0.996\n",
      "            84          -0.06141        0.996\n",
      "            85          -0.06084        0.996\n",
      "            86          -0.06027        0.996\n",
      "            87          -0.05972        0.996\n",
      "            88          -0.05918        0.996\n",
      "            89          -0.05865        0.996\n",
      "            90          -0.05813        0.996\n",
      "            91          -0.05761        0.996\n",
      "            92          -0.05711        0.996\n",
      "            93          -0.05662        0.996\n",
      "            94          -0.05614        0.996\n",
      "            95          -0.05566        0.996\n",
      "            96          -0.05519        0.996\n",
      "            97          -0.05474        0.996\n",
      "            98          -0.05429        0.996\n",
      "            99          -0.05384        0.996\n",
      "         Final          -0.05341        0.996\n",
      "\n",
      "The accuracy is 80.00%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.258\n",
      "             2          -0.43673        0.742\n",
      "             3          -0.39327        0.769\n",
      "             4          -0.35874        0.804\n",
      "             5          -0.33087        0.846\n",
      "             6          -0.30785        0.877\n",
      "             7          -0.28841        0.892\n",
      "             8          -0.27170        0.908\n",
      "             9          -0.25711        0.912\n",
      "            10          -0.24422        0.919\n",
      "            11          -0.23272        0.938\n",
      "            12          -0.22237        0.954\n",
      "            13          -0.21299        0.969\n",
      "            14          -0.20443        0.969\n",
      "            15          -0.19659        0.973\n",
      "            16          -0.18937        0.973\n",
      "            17          -0.18270        0.973\n",
      "            18          -0.17650        0.977\n",
      "            19          -0.17074        0.977\n",
      "            20          -0.16536        0.977\n",
      "            21          -0.16033        0.981\n",
      "            22          -0.15560        0.985\n",
      "            23          -0.15116        0.985\n",
      "            24          -0.14697        0.985\n",
      "            25          -0.14302        0.985\n",
      "            26          -0.13928        0.985\n",
      "            27          -0.13573        0.988\n",
      "            28          -0.13237        0.988\n",
      "            29          -0.12918        0.996\n",
      "            30          -0.12614        0.996\n",
      "            31          -0.12324        0.996\n",
      "            32          -0.12047        0.996\n",
      "            33          -0.11783        0.996\n",
      "            34          -0.11531        0.996\n",
      "            35          -0.11289        1.000\n",
      "            36          -0.11057        1.000\n",
      "            37          -0.10835        1.000\n",
      "            38          -0.10622        1.000\n",
      "            39          -0.10417        1.000\n",
      "            40          -0.10220        1.000\n",
      "            41          -0.10030        1.000\n",
      "            42          -0.09847        1.000\n",
      "            43          -0.09671        1.000\n",
      "            44          -0.09501        1.000\n",
      "            45          -0.09337        1.000\n",
      "            46          -0.09179        1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            47          -0.09026        1.000\n",
      "            48          -0.08878        1.000\n",
      "            49          -0.08735        1.000\n",
      "            50          -0.08596        1.000\n",
      "            51          -0.08462        1.000\n",
      "            52          -0.08332        1.000\n",
      "            53          -0.08206        1.000\n",
      "            54          -0.08083        1.000\n",
      "            55          -0.07965        1.000\n",
      "            56          -0.07849        1.000\n",
      "            57          -0.07737        1.000\n",
      "            58          -0.07629        1.000\n",
      "            59          -0.07523        1.000\n",
      "            60          -0.07420        1.000\n",
      "            61          -0.07320        1.000\n",
      "            62          -0.07222        1.000\n",
      "            63          -0.07127        1.000\n",
      "            64          -0.07035        1.000\n",
      "            65          -0.06945        1.000\n",
      "            66          -0.06857        1.000\n",
      "            67          -0.06771        1.000\n",
      "            68          -0.06688        1.000\n",
      "            69          -0.06606        1.000\n",
      "            70          -0.06527        1.000\n",
      "            71          -0.06449        1.000\n",
      "            72          -0.06373        1.000\n",
      "            73          -0.06299        1.000\n",
      "            74          -0.06227        1.000\n",
      "            75          -0.06156        1.000\n",
      "            76          -0.06087        1.000\n",
      "            77          -0.06020        1.000\n",
      "            78          -0.05954        1.000\n",
      "            79          -0.05889        1.000\n",
      "            80          -0.05826        1.000\n",
      "            81          -0.05764        1.000\n",
      "            82          -0.05703        1.000\n",
      "            83          -0.05644        1.000\n",
      "            84          -0.05585        1.000\n",
      "            85          -0.05528        1.000\n",
      "            86          -0.05473        1.000\n",
      "            87          -0.05418        1.000\n",
      "            88          -0.05364        1.000\n",
      "            89          -0.05312        1.000\n",
      "            90          -0.05260        1.000\n",
      "            91          -0.05209        1.000\n",
      "            92          -0.05160        1.000\n",
      "            93          -0.05111        1.000\n",
      "            94          -0.05063        1.000\n",
      "            95          -0.05016        1.000\n",
      "            96          -0.04970        1.000\n",
      "            97          -0.04925        1.000\n",
      "            98          -0.04881        1.000\n",
      "            99          -0.04837        1.000\n",
      "         Final          -0.04794        1.000\n",
      "\n",
      "The accuracy is 75.38%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.277\n",
      "             2          -0.45686        0.723\n",
      "             3          -0.41027        0.769\n",
      "             4          -0.37445        0.815\n",
      "             5          -0.34618        0.850\n",
      "             6          -0.32316        0.865\n",
      "             7          -0.30387        0.888\n",
      "             8          -0.28734        0.900\n",
      "             9          -0.27292        0.908\n",
      "            10          -0.26017        0.912\n",
      "            11          -0.24878        0.919\n",
      "            12          -0.23850        0.935\n",
      "            13          -0.22916        0.935\n",
      "            14          -0.22063        0.938\n",
      "            15          -0.21278        0.950\n",
      "            16          -0.20554        0.950\n",
      "            17          -0.19883        0.954\n",
      "            18          -0.19259        0.954\n",
      "            19          -0.18676        0.958\n",
      "            20          -0.18131        0.965\n",
      "            21          -0.17620        0.977\n",
      "            22          -0.17139        0.981\n",
      "            23          -0.16686        0.988\n",
      "            24          -0.16258        0.988\n",
      "            25          -0.15853        0.988\n",
      "            26          -0.15469        0.988\n",
      "            27          -0.15105        0.988\n",
      "            28          -0.14759        0.992\n",
      "            29          -0.14429        0.992\n",
      "            30          -0.14115        0.992\n",
      "            31          -0.13815        0.992\n",
      "            32          -0.13528        0.992\n",
      "            33          -0.13254        0.992\n",
      "            34          -0.12991        0.992\n",
      "            35          -0.12739        0.992\n",
      "            36          -0.12498        0.992\n",
      "            37          -0.12265        0.992\n",
      "            38          -0.12042        0.992\n",
      "            39          -0.11828        0.992\n",
      "            40          -0.11621        0.992\n",
      "            41          -0.11422        0.992\n",
      "            42          -0.11230        0.992\n",
      "            43          -0.11044        0.992\n",
      "            44          -0.10866        0.992\n",
      "            45          -0.10693        0.992\n",
      "            46          -0.10525        0.992\n",
      "            47          -0.10364        0.992\n",
      "            48          -0.10207        0.992\n",
      "            49          -0.10055        0.996\n",
      "            50          -0.09909        0.996\n",
      "            51          -0.09766        0.996\n",
      "            52          -0.09628        0.996\n",
      "            53          -0.09494        0.996\n",
      "            54          -0.09364        0.996\n",
      "            55          -0.09237        0.996\n",
      "            56          -0.09114        0.996\n",
      "            57          -0.08995        0.996\n",
      "            58          -0.08878        0.996\n",
      "            59          -0.08765        0.996\n",
      "            60          -0.08655        0.996\n",
      "            61          -0.08548        0.996\n",
      "            62          -0.08443        0.996\n",
      "            63          -0.08341        0.996\n",
      "            64          -0.08242        0.996\n",
      "            65          -0.08145        0.996\n",
      "            66          -0.08051        0.996\n",
      "            67          -0.07959        0.996\n",
      "            68          -0.07869        0.996\n",
      "            69          -0.07781        0.996\n",
      "            70          -0.07695        0.996\n",
      "            71          -0.07612        0.996\n",
      "            72          -0.07530        0.996\n",
      "            73          -0.07450        0.996\n",
      "            74          -0.07371        0.996\n",
      "            75          -0.07295        0.996\n",
      "            76          -0.07220        0.996\n",
      "            77          -0.07147        0.996\n",
      "            78          -0.07075        0.996\n",
      "            79          -0.07005        0.996\n",
      "            80          -0.06936        0.996\n",
      "            81          -0.06869        0.996\n",
      "            82          -0.06803        0.996\n",
      "            83          -0.06738        0.996\n",
      "            84          -0.06675        0.996\n",
      "            85          -0.06613        0.996\n",
      "            86          -0.06552        0.996\n",
      "            87          -0.06492        0.996\n",
      "            88          -0.06433        0.996\n",
      "            89          -0.06376        0.996\n",
      "            90          -0.06319        0.996\n",
      "            91          -0.06264        0.996\n",
      "            92          -0.06210        0.996\n",
      "            93          -0.06156        0.996\n",
      "            94          -0.06104        0.996\n",
      "            95          -0.06052        0.996\n",
      "            96          -0.06002        0.996\n",
      "            97          -0.05952        0.996\n",
      "            98          -0.05903        0.996\n",
      "            99          -0.05855        0.996\n",
      "         Final          -0.05808        0.996\n",
      "\n",
      "The accuracy is 81.54%\n"
     ]
    }
   ],
   "source": [
    "accHist = maxEntTrain(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6307692307692307, 0.5230769230769231, 0.8, 0.7538461538461538, 0.8153846153846154]\n",
      "\n",
      "The average accuracy with 5-Fold Cross-Validation is: 70.46%\n"
     ]
    }
   ],
   "source": [
    "print(accHist)\n",
    "print('\\nThe average accuracy with 5-Fold Cross-Validation is: {:.2%}'.format(np.sum(accHist)/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.208\n",
      "             2          -0.35993        0.792\n",
      "             3          -0.32396        0.804\n",
      "             4          -0.29545        0.846\n",
      "             5          -0.27290        0.873\n",
      "             6          -0.25464        0.892\n",
      "             7          -0.23949        0.908\n",
      "             8          -0.22663        0.915\n",
      "             9          -0.21553        0.927\n",
      "            10          -0.20581        0.927\n",
      "            11          -0.19719        0.935\n",
      "            12          -0.18948        0.935\n",
      "            13          -0.18253        0.942\n",
      "            14          -0.17621        0.950\n",
      "            15          -0.17043        0.954\n",
      "            16          -0.16513        0.965\n",
      "            17          -0.16023        0.965\n",
      "            18          -0.15569        0.969\n",
      "            19          -0.15146        0.969\n",
      "            20          -0.14752        0.969\n",
      "            21          -0.14383        0.969\n",
      "            22          -0.14036        0.969\n",
      "            23          -0.13710        0.973\n",
      "            24          -0.13402        0.973\n",
      "            25          -0.13111        0.973\n",
      "            26          -0.12835        0.973\n",
      "            27          -0.12573        0.973\n",
      "            28          -0.12324        0.973\n",
      "            29          -0.12087        0.973\n",
      "            30          -0.11861        0.973\n",
      "            31          -0.11645        0.977\n",
      "            32          -0.11438        0.981\n",
      "            33          -0.11240        0.981\n",
      "            34          -0.11050        0.981\n",
      "            35          -0.10868        0.981\n",
      "            36          -0.10693        0.981\n",
      "            37          -0.10525        0.981\n",
      "            38          -0.10363        0.981\n",
      "            39          -0.10207        0.985\n",
      "            40          -0.10057        0.985\n",
      "            41          -0.09912        0.985\n",
      "            42          -0.09772        0.985\n",
      "            43          -0.09636        0.985\n",
      "            44          -0.09505        0.985\n",
      "            45          -0.09379        0.988\n",
      "            46          -0.09256        0.988\n",
      "            47          -0.09137        0.988\n",
      "            48          -0.09022        0.988\n",
      "            49          -0.08910        0.988\n",
      "            50          -0.08802        0.988\n",
      "            51          -0.08696        0.988\n",
      "            52          -0.08594        0.988\n",
      "            53          -0.08494        0.988\n",
      "            54          -0.08398        0.988\n",
      "            55          -0.08303        0.988\n",
      "            56          -0.08212        0.988\n",
      "            57          -0.08123        0.988\n",
      "            58          -0.08036        0.988\n",
      "            59          -0.07951        0.988\n",
      "            60          -0.07868        0.988\n",
      "            61          -0.07788        0.988\n",
      "            62          -0.07709        0.988\n",
      "            63          -0.07633        0.988\n",
      "            64          -0.07558        0.988\n",
      "            65          -0.07485        0.988\n",
      "            66          -0.07413        0.988\n",
      "            67          -0.07343        0.988\n",
      "            68          -0.07275        0.988\n",
      "            69          -0.07209        0.988\n",
      "            70          -0.07143        0.988\n",
      "            71          -0.07079        0.988\n",
      "            72          -0.07017        0.988\n",
      "            73          -0.06956        0.988\n",
      "            74          -0.06896        0.988\n",
      "            75          -0.06837        0.988\n",
      "            76          -0.06780        0.988\n",
      "            77          -0.06723        0.988\n",
      "            78          -0.06668        0.988\n",
      "            79          -0.06614        0.988\n",
      "            80          -0.06561        0.988\n",
      "            81          -0.06509        0.988\n",
      "            82          -0.06458        0.988\n",
      "            83          -0.06407        0.988\n",
      "            84          -0.06358        0.988\n",
      "            85          -0.06310        0.988\n",
      "            86          -0.06262        0.988\n",
      "            87          -0.06216        0.988\n",
      "            88          -0.06170        0.988\n",
      "            89          -0.06125        0.992\n",
      "            90          -0.06081        0.992\n",
      "            91          -0.06037        0.992\n",
      "            92          -0.05995        0.992\n",
      "            93          -0.05953        0.992\n",
      "            94          -0.05911        0.992\n",
      "            95          -0.05871        0.992\n",
      "            96          -0.05831        0.992\n",
      "            97          -0.05791        0.992\n",
      "            98          -0.05753        0.992\n",
      "            99          -0.05715        0.992\n",
      "         Final          -0.05677        0.992\n",
      "\n",
      "The accuracy is 63.08%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.169\n",
      "             2          -0.29974        0.831\n",
      "             3          -0.27229        0.835\n",
      "             4          -0.25079        0.869\n",
      "             5          -0.23423        0.885\n",
      "             6          -0.22103        0.888\n",
      "             7          -0.21011        0.912\n",
      "             8          -0.20084        0.919\n",
      "             9          -0.19279        0.919\n",
      "            10          -0.18568        0.919\n",
      "            11          -0.17933        0.931\n",
      "            12          -0.17358        0.938\n",
      "            13          -0.16835        0.946\n",
      "            14          -0.16356        0.946\n",
      "            15          -0.15913        0.946\n",
      "            16          -0.15502        0.950\n",
      "            17          -0.15119        0.954\n",
      "            18          -0.14762        0.954\n",
      "            19          -0.14426        0.958\n",
      "            20          -0.14110        0.962\n",
      "            21          -0.13813        0.969\n",
      "            22          -0.13531        0.973\n",
      "            23          -0.13264        0.973\n",
      "            24          -0.13011        0.973\n",
      "            25          -0.12770        0.973\n",
      "            26          -0.12540        0.973\n",
      "            27          -0.12321        0.977\n",
      "            28          -0.12112        0.977\n",
      "            29          -0.11912        0.977\n",
      "            30          -0.11720        0.981\n",
      "            31          -0.11535        0.981\n",
      "            32          -0.11358        0.981\n",
      "            33          -0.11188        0.981\n",
      "            34          -0.11025        0.981\n",
      "            35          -0.10867        0.981\n",
      "            36          -0.10715        0.981\n",
      "            37          -0.10568        0.981\n",
      "            38          -0.10426        0.981\n",
      "            39          -0.10289        0.981\n",
      "            40          -0.10156        0.981\n",
      "            41          -0.10028        0.981\n",
      "            42          -0.09904        0.981\n",
      "            43          -0.09783        0.981\n",
      "            44          -0.09666        0.981\n",
      "            45          -0.09552        0.981\n",
      "            46          -0.09442        0.981\n",
      "            47          -0.09335        0.981\n",
      "            48          -0.09231        0.981\n",
      "            49          -0.09129        0.981\n",
      "            50          -0.09031        0.981\n",
      "            51          -0.08935        0.981\n",
      "            52          -0.08841        0.981\n",
      "            53          -0.08750        0.981\n",
      "            54          -0.08661        0.981\n",
      "            55          -0.08574        0.981\n",
      "            56          -0.08489        0.981\n",
      "            57          -0.08407        0.981\n",
      "            58          -0.08326        0.981\n",
      "            59          -0.08247        0.981\n",
      "            60          -0.08170        0.981\n",
      "            61          -0.08095        0.985\n",
      "            62          -0.08021        0.985\n",
      "            63          -0.07949        0.985\n",
      "            64          -0.07878        0.985\n",
      "            65          -0.07809        0.985\n",
      "            66          -0.07741        0.985\n",
      "            67          -0.07675        0.985\n",
      "            68          -0.07610        0.985\n",
      "            69          -0.07546        0.985\n",
      "            70          -0.07484        0.985\n",
      "            71          -0.07423        0.985\n",
      "            72          -0.07363        0.985\n",
      "            73          -0.07304        0.985\n",
      "            74          -0.07246        0.985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            75          -0.07190        0.985\n",
      "            76          -0.07134        0.985\n",
      "            77          -0.07079        0.985\n",
      "            78          -0.07026        0.985\n",
      "            79          -0.06973        0.985\n",
      "            80          -0.06921        0.985\n",
      "            81          -0.06870        0.985\n",
      "            82          -0.06820        0.985\n",
      "            83          -0.06771        0.985\n",
      "            84          -0.06723        0.985\n",
      "            85          -0.06675        0.985\n",
      "            86          -0.06628        0.985\n",
      "            87          -0.06582        0.985\n",
      "            88          -0.06537        0.985\n",
      "            89          -0.06492        0.985\n",
      "            90          -0.06448        0.985\n",
      "            91          -0.06405        0.985\n",
      "            92          -0.06362        0.985\n",
      "            93          -0.06320        0.985\n",
      "            94          -0.06279        0.985\n",
      "            95          -0.06238        0.985\n",
      "            96          -0.06198        0.985\n",
      "            97          -0.06159        0.985\n",
      "            98          -0.06120        0.985\n",
      "            99          -0.06082        0.985\n",
      "         Final          -0.06044        0.985\n",
      "\n",
      "The accuracy is 52.31%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.258\n",
      "             2          -0.43310        0.742\n",
      "             3          -0.39170        0.777\n",
      "             4          -0.35990        0.815\n",
      "             5          -0.33484        0.838\n",
      "             6          -0.31444        0.877\n",
      "             7          -0.29738        0.885\n",
      "             8          -0.28280        0.900\n",
      "             9          -0.27011        0.904\n",
      "            10          -0.25891        0.908\n",
      "            11          -0.24892        0.923\n",
      "            12          -0.23993        0.923\n",
      "            13          -0.23176        0.927\n",
      "            14          -0.22431        0.927\n",
      "            15          -0.21746        0.927\n",
      "            16          -0.21113        0.927\n",
      "            17          -0.20528        0.931\n",
      "            18          -0.19982        0.938\n",
      "            19          -0.19474        0.942\n",
      "            20          -0.18998        0.950\n",
      "            21          -0.18551        0.950\n",
      "            22          -0.18130        0.954\n",
      "            23          -0.17734        0.962\n",
      "            24          -0.17359        0.965\n",
      "            25          -0.17005        0.965\n",
      "            26          -0.16668        0.965\n",
      "            27          -0.16348        0.965\n",
      "            28          -0.16044        0.965\n",
      "            29          -0.15754        0.965\n",
      "            30          -0.15477        0.969\n",
      "            31          -0.15212        0.969\n",
      "            32          -0.14959        0.969\n",
      "            33          -0.14717        0.969\n",
      "            34          -0.14484        0.973\n",
      "            35          -0.14261        0.973\n",
      "            36          -0.14046        0.973\n",
      "            37          -0.13839        0.973\n",
      "            38          -0.13640        0.973\n",
      "            39          -0.13449        0.973\n",
      "            40          -0.13264        0.973\n",
      "            41          -0.13085        0.973\n",
      "            42          -0.12913        0.973\n",
      "            43          -0.12746        0.973\n",
      "            44          -0.12585        0.973\n",
      "            45          -0.12429        0.973\n",
      "            46          -0.12277        0.973\n",
      "            47          -0.12131        0.973\n",
      "            48          -0.11988        0.973\n",
      "            49          -0.11850        0.973\n",
      "            50          -0.11716        0.973\n",
      "            51          -0.11586        0.973\n",
      "            52          -0.11459        0.973\n",
      "            53          -0.11336        0.973\n",
      "            54          -0.11216        0.973\n",
      "            55          -0.11100        0.973\n",
      "            56          -0.10986        0.973\n",
      "            57          -0.10875        0.969\n",
      "            58          -0.10767        0.969\n",
      "            59          -0.10662        0.969\n",
      "            60          -0.10559        0.969\n",
      "            61          -0.10459        0.969\n",
      "            62          -0.10361        0.973\n",
      "            63          -0.10266        0.973\n",
      "            64          -0.10173        0.973\n",
      "            65          -0.10081        0.973\n",
      "            66          -0.09992        0.973\n",
      "            67          -0.09905        0.973\n",
      "            68          -0.09820        0.977\n",
      "            69          -0.09736        0.977\n",
      "            70          -0.09655        0.977\n",
      "            71          -0.09575        0.977\n",
      "            72          -0.09496        0.977\n",
      "            73          -0.09420        0.977\n",
      "            74          -0.09344        0.977\n",
      "            75          -0.09271        0.977\n",
      "            76          -0.09198        0.977\n",
      "            77          -0.09128        0.977\n",
      "            78          -0.09058        0.977\n",
      "            79          -0.08990        0.981\n",
      "            80          -0.08923        0.981\n",
      "            81          -0.08857        0.981\n",
      "            82          -0.08793        0.981\n",
      "            83          -0.08729        0.981\n",
      "            84          -0.08667        0.981\n",
      "            85          -0.08606        0.981\n",
      "            86          -0.08546        0.981\n",
      "            87          -0.08487        0.981\n",
      "            88          -0.08429        0.981\n",
      "            89          -0.08372        0.981\n",
      "            90          -0.08316        0.981\n",
      "            91          -0.08261        0.981\n",
      "            92          -0.08207        0.981\n",
      "            93          -0.08153        0.981\n",
      "            94          -0.08101        0.981\n",
      "            95          -0.08049        0.981\n",
      "            96          -0.07998        0.981\n",
      "            97          -0.07948        0.981\n",
      "            98          -0.07899        0.981\n",
      "            99          -0.07850        0.981\n",
      "         Final          -0.07803        0.981\n",
      "\n",
      "The accuracy is 83.08%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.258\n",
      "             2          -0.43316        0.742\n",
      "             3          -0.39147        0.777\n",
      "             4          -0.35916        0.812\n",
      "             5          -0.33356        0.846\n",
      "             6          -0.31269        0.862\n",
      "             7          -0.29524        0.877\n",
      "             8          -0.28034        0.896\n",
      "             9          -0.26740        0.900\n",
      "            10          -0.25601        0.904\n",
      "            11          -0.24587        0.927\n",
      "            12          -0.23675        0.927\n",
      "            13          -0.22849        0.927\n",
      "            14          -0.22096        0.931\n",
      "            15          -0.21405        0.935\n",
      "            16          -0.20768        0.938\n",
      "            17          -0.20179        0.942\n",
      "            18          -0.19630        0.950\n",
      "            19          -0.19119        0.958\n",
      "            20          -0.18640        0.958\n",
      "            21          -0.18191        0.958\n",
      "            22          -0.17769        0.958\n",
      "            23          -0.17370        0.962\n",
      "            24          -0.16994        0.962\n",
      "            25          -0.16637        0.962\n",
      "            26          -0.16299        0.962\n",
      "            27          -0.15977        0.969\n",
      "            28          -0.15671        0.969\n",
      "            29          -0.15379        0.969\n",
      "            30          -0.15100        0.969\n",
      "            31          -0.14833        0.969\n",
      "            32          -0.14578        0.969\n",
      "            33          -0.14333        0.969\n",
      "            34          -0.14098        0.977\n",
      "            35          -0.13873        0.977\n",
      "            36          -0.13656        0.981\n",
      "            37          -0.13447        0.981\n",
      "            38          -0.13246        0.981\n",
      "            39          -0.13052        0.981\n",
      "            40          -0.12865        0.981\n",
      "            41          -0.12684        0.981\n",
      "            42          -0.12509        0.981\n",
      "            43          -0.12340        0.981\n",
      "            44          -0.12177        0.981\n",
      "            45          -0.12018        0.981\n",
      "            46          -0.11865        0.981\n",
      "            47          -0.11716        0.981\n",
      "            48          -0.11572        0.981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            49          -0.11431        0.981\n",
      "            50          -0.11295        0.981\n",
      "            51          -0.11163        0.981\n",
      "            52          -0.11034        0.981\n",
      "            53          -0.10909        0.981\n",
      "            54          -0.10787        0.981\n",
      "            55          -0.10668        0.981\n",
      "            56          -0.10552        0.981\n",
      "            57          -0.10439        0.981\n",
      "            58          -0.10329        0.981\n",
      "            59          -0.10222        0.981\n",
      "            60          -0.10117        0.981\n",
      "            61          -0.10015        0.981\n",
      "            62          -0.09915        0.981\n",
      "            63          -0.09818        0.981\n",
      "            64          -0.09723        0.981\n",
      "            65          -0.09629        0.981\n",
      "            66          -0.09538        0.981\n",
      "            67          -0.09449        0.981\n",
      "            68          -0.09362        0.981\n",
      "            69          -0.09277        0.981\n",
      "            70          -0.09193        0.985\n",
      "            71          -0.09111        0.985\n",
      "            72          -0.09031        0.985\n",
      "            73          -0.08952        0.985\n",
      "            74          -0.08875        0.985\n",
      "            75          -0.08800        0.985\n",
      "            76          -0.08726        0.985\n",
      "            77          -0.08653        0.988\n",
      "            78          -0.08582        0.988\n",
      "            79          -0.08512        0.988\n",
      "            80          -0.08443        0.988\n",
      "            81          -0.08375        0.988\n",
      "            82          -0.08309        0.988\n",
      "            83          -0.08244        0.988\n",
      "            84          -0.08180        0.988\n",
      "            85          -0.08117        0.988\n",
      "            86          -0.08055        0.988\n",
      "            87          -0.07995        0.988\n",
      "            88          -0.07935        0.988\n",
      "            89          -0.07876        0.988\n",
      "            90          -0.07819        0.988\n",
      "            91          -0.07762        0.988\n",
      "            92          -0.07706        0.988\n",
      "            93          -0.07651        0.988\n",
      "            94          -0.07597        0.988\n",
      "            95          -0.07544        0.988\n",
      "            96          -0.07491        0.988\n",
      "            97          -0.07439        0.988\n",
      "            98          -0.07388        0.988\n",
      "            99          -0.07338        0.988\n",
      "         Final          -0.07289        0.988\n",
      "\n",
      "The accuracy is 73.85%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.277\n",
      "             2          -0.45335        0.723\n",
      "             3          -0.40788        0.765\n",
      "             4          -0.37397        0.815\n",
      "             5          -0.34776        0.835\n",
      "             6          -0.32667        0.842\n",
      "             7          -0.30915        0.854\n",
      "             8          -0.29422        0.858\n",
      "             9          -0.28125        0.862\n",
      "            10          -0.26981        0.877\n",
      "            11          -0.25959        0.888\n",
      "            12          -0.25038        0.904\n",
      "            13          -0.24202        0.915\n",
      "            14          -0.23437        0.927\n",
      "            15          -0.22734        0.931\n",
      "            16          -0.22084        0.935\n",
      "            17          -0.21481        0.938\n",
      "            18          -0.20919        0.942\n",
      "            19          -0.20394        0.942\n",
      "            20          -0.19901        0.942\n",
      "            21          -0.19439        0.946\n",
      "            22          -0.19002        0.950\n",
      "            23          -0.18591        0.950\n",
      "            24          -0.18201        0.958\n",
      "            25          -0.17831        0.958\n",
      "            26          -0.17480        0.958\n",
      "            27          -0.17145        0.958\n",
      "            28          -0.16827        0.958\n",
      "            29          -0.16523        0.962\n",
      "            30          -0.16232        0.962\n",
      "            31          -0.15954        0.962\n",
      "            32          -0.15687        0.962\n",
      "            33          -0.15432        0.962\n",
      "            34          -0.15187        0.962\n",
      "            35          -0.14951        0.965\n",
      "            36          -0.14724        0.973\n",
      "            37          -0.14505        0.973\n",
      "            38          -0.14295        0.973\n",
      "            39          -0.14092        0.973\n",
      "            40          -0.13896        0.973\n",
      "            41          -0.13706        0.977\n",
      "            42          -0.13523        0.977\n",
      "            43          -0.13346        0.977\n",
      "            44          -0.13175        0.977\n",
      "            45          -0.13009        0.977\n",
      "            46          -0.12848        0.977\n",
      "            47          -0.12692        0.977\n",
      "            48          -0.12541        0.977\n",
      "            49          -0.12394        0.977\n",
      "            50          -0.12251        0.977\n",
      "            51          -0.12112        0.977\n",
      "            52          -0.11978        0.977\n",
      "            53          -0.11847        0.977\n",
      "            54          -0.11719        0.977\n",
      "            55          -0.11595        0.977\n",
      "            56          -0.11474        0.977\n",
      "            57          -0.11356        0.977\n",
      "            58          -0.11241        0.977\n",
      "            59          -0.11129        0.977\n",
      "            60          -0.11020        0.977\n",
      "            61          -0.10913        0.977\n",
      "            62          -0.10809        0.977\n",
      "            63          -0.10707        0.977\n",
      "            64          -0.10608        0.977\n",
      "            65          -0.10511        0.977\n",
      "            66          -0.10416        0.977\n",
      "            67          -0.10324        0.977\n",
      "            68          -0.10233        0.977\n",
      "            69          -0.10144        0.977\n",
      "            70          -0.10058        0.981\n",
      "            71          -0.09973        0.981\n",
      "            72          -0.09890        0.981\n",
      "            73          -0.09808        0.981\n",
      "            74          -0.09729        0.981\n",
      "            75          -0.09650        0.981\n",
      "            76          -0.09574        0.981\n",
      "            77          -0.09499        0.981\n",
      "            78          -0.09425        0.981\n",
      "            79          -0.09353        0.981\n",
      "            80          -0.09282        0.981\n",
      "            81          -0.09213        0.981\n",
      "            82          -0.09145        0.981\n",
      "            83          -0.09078        0.985\n",
      "            84          -0.09012        0.985\n",
      "            85          -0.08948        0.985\n",
      "            86          -0.08885        0.985\n",
      "            87          -0.08822        0.985\n",
      "            88          -0.08761        0.985\n",
      "            89          -0.08701        0.985\n",
      "            90          -0.08642        0.985\n",
      "            91          -0.08584        0.985\n",
      "            92          -0.08527        0.985\n",
      "            93          -0.08471        0.985\n",
      "            94          -0.08416        0.985\n",
      "            95          -0.08362        0.985\n",
      "            96          -0.08308        0.985\n",
      "            97          -0.08256        0.985\n",
      "            98          -0.08204        0.985\n",
      "            99          -0.08154        0.985\n",
      "         Final          -0.08103        0.985\n",
      "\n",
      "The accuracy is 80.00%\n"
     ]
    }
   ],
   "source": [
    "# Train and test without previous 3rd word features\n",
    "data2 = data.drop(['previous 3 Entity tag','previous 3 POS','previous 3 POS tag','previous 3 word'], axis=1)\n",
    "accHist2 = maxEntTrain(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6307692307692307, 0.5230769230769231, 0.8307692307692308, 0.7384615384615385, 0.8]\n",
      "\n",
      "The average accuracy after removing previous 3rd word is: 70.46%\n"
     ]
    }
   ],
   "source": [
    "print(accHist2)\n",
    "print('\\nThe average accuracy after removing previous 3rd word is: {:.2%}'.format(np.sum(accHist2)/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.208\n",
      "             2          -0.35983        0.792\n",
      "             3          -0.32659        0.808\n",
      "             4          -0.30071        0.842\n",
      "             5          -0.28052        0.854\n",
      "             6          -0.26434        0.881\n",
      "             7          -0.25098        0.885\n",
      "             8          -0.23970        0.892\n",
      "             9          -0.23000        0.904\n",
      "            10          -0.22151        0.915\n",
      "            11          -0.21400        0.919\n",
      "            12          -0.20729        0.919\n",
      "            13          -0.20124        0.919\n",
      "            14          -0.19573        0.919\n",
      "            15          -0.19070        0.927\n",
      "            16          -0.18607        0.927\n",
      "            17          -0.18179        0.927\n",
      "            18          -0.17782        0.927\n",
      "            19          -0.17412        0.931\n",
      "            20          -0.17065        0.931\n",
      "            21          -0.16740        0.938\n",
      "            22          -0.16434        0.942\n",
      "            23          -0.16146        0.942\n",
      "            24          -0.15873        0.942\n",
      "            25          -0.15614        0.946\n",
      "            26          -0.15368        0.946\n",
      "            27          -0.15134        0.946\n",
      "            28          -0.14911        0.946\n",
      "            29          -0.14698        0.950\n",
      "            30          -0.14494        0.954\n",
      "            31          -0.14299        0.958\n",
      "            32          -0.14112        0.958\n",
      "            33          -0.13932        0.958\n",
      "            34          -0.13759        0.958\n",
      "            35          -0.13592        0.958\n",
      "            36          -0.13432        0.958\n",
      "            37          -0.13277        0.962\n",
      "            38          -0.13128        0.962\n",
      "            39          -0.12984        0.962\n",
      "            40          -0.12844        0.962\n",
      "            41          -0.12709        0.962\n",
      "            42          -0.12579        0.962\n",
      "            43          -0.12452        0.962\n",
      "            44          -0.12329        0.962\n",
      "            45          -0.12210        0.962\n",
      "            46          -0.12094        0.962\n",
      "            47          -0.11981        0.962\n",
      "            48          -0.11872        0.962\n",
      "            49          -0.11765        0.962\n",
      "            50          -0.11661        0.965\n",
      "            51          -0.11560        0.969\n",
      "            52          -0.11462        0.969\n",
      "            53          -0.11366        0.969\n",
      "            54          -0.11272        0.969\n",
      "            55          -0.11181        0.973\n",
      "            56          -0.11092        0.973\n",
      "            57          -0.11005        0.973\n",
      "            58          -0.10920        0.973\n",
      "            59          -0.10837        0.973\n",
      "            60          -0.10756        0.973\n",
      "            61          -0.10676        0.973\n",
      "            62          -0.10599        0.973\n",
      "            63          -0.10523        0.973\n",
      "            64          -0.10448        0.977\n",
      "            65          -0.10375        0.977\n",
      "            66          -0.10304        0.977\n",
      "            67          -0.10234        0.977\n",
      "            68          -0.10166        0.977\n",
      "            69          -0.10098        0.977\n",
      "            70          -0.10032        0.977\n",
      "            71          -0.09968        0.977\n",
      "            72          -0.09904        0.977\n",
      "            73          -0.09842        0.977\n",
      "            74          -0.09781        0.977\n",
      "            75          -0.09721        0.981\n",
      "            76          -0.09662        0.981\n",
      "            77          -0.09604        0.981\n",
      "            78          -0.09547        0.981\n",
      "            79          -0.09491        0.981\n",
      "            80          -0.09437        0.981\n",
      "            81          -0.09383        0.981\n",
      "            82          -0.09329        0.981\n",
      "            83          -0.09277        0.981\n",
      "            84          -0.09226        0.981\n",
      "            85          -0.09175        0.981\n",
      "            86          -0.09126        0.981\n",
      "            87          -0.09077        0.981\n",
      "            88          -0.09028        0.981\n",
      "            89          -0.08981        0.981\n",
      "            90          -0.08934        0.981\n",
      "            91          -0.08888        0.981\n",
      "            92          -0.08843        0.981\n",
      "            93          -0.08798        0.981\n",
      "            94          -0.08754        0.981\n",
      "            95          -0.08711        0.981\n",
      "            96          -0.08668        0.981\n",
      "            97          -0.08626        0.981\n",
      "            98          -0.08585        0.981\n",
      "            99          -0.08544        0.981\n",
      "         Final          -0.08503        0.981\n",
      "\n",
      "The accuracy is 55.38%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.169\n",
      "             2          -0.30043        0.831\n",
      "             3          -0.27521        0.835\n",
      "             4          -0.25592        0.862\n",
      "             5          -0.24144        0.865\n",
      "             6          -0.23011        0.865\n",
      "             7          -0.22087        0.881\n",
      "             8          -0.21308        0.885\n",
      "             9          -0.20633        0.885\n",
      "            10          -0.20037        0.896\n",
      "            11          -0.19503        0.904\n",
      "            12          -0.19019        0.904\n",
      "            13          -0.18577        0.904\n",
      "            14          -0.18170        0.904\n",
      "            15          -0.17793        0.908\n",
      "            16          -0.17442        0.908\n",
      "            17          -0.17115        0.912\n",
      "            18          -0.16808        0.912\n",
      "            19          -0.16520        0.912\n",
      "            20          -0.16248        0.919\n",
      "            21          -0.15991        0.919\n",
      "            22          -0.15748        0.923\n",
      "            23          -0.15517        0.923\n",
      "            24          -0.15298        0.927\n",
      "            25          -0.15089        0.927\n",
      "            26          -0.14889        0.931\n",
      "            27          -0.14699        0.931\n",
      "            28          -0.14517        0.935\n",
      "            29          -0.14342        0.935\n",
      "            30          -0.14174        0.935\n",
      "            31          -0.14014        0.938\n",
      "            32          -0.13859        0.938\n",
      "            33          -0.13710        0.938\n",
      "            34          -0.13566        0.938\n",
      "            35          -0.13427        0.942\n",
      "            36          -0.13294        0.942\n",
      "            37          -0.13164        0.942\n",
      "            38          -0.13039        0.942\n",
      "            39          -0.12918        0.942\n",
      "            40          -0.12800        0.946\n",
      "            41          -0.12686        0.950\n",
      "            42          -0.12576        0.950\n",
      "            43          -0.12469        0.950\n",
      "            44          -0.12364        0.950\n",
      "            45          -0.12263        0.950\n",
      "            46          -0.12164        0.950\n",
      "            47          -0.12068        0.950\n",
      "            48          -0.11974        0.950\n",
      "            49          -0.11883        0.950\n",
      "            50          -0.11794        0.950\n",
      "            51          -0.11707        0.954\n",
      "            52          -0.11623        0.954\n",
      "            53          -0.11540        0.954\n",
      "            54          -0.11459        0.954\n",
      "            55          -0.11380        0.954\n",
      "            56          -0.11303        0.954\n",
      "            57          -0.11227        0.954\n",
      "            58          -0.11153        0.954\n",
      "            59          -0.11081        0.954\n",
      "            60          -0.11010        0.954\n",
      "            61          -0.10941        0.954\n",
      "            62          -0.10872        0.958\n",
      "            63          -0.10806        0.962\n",
      "            64          -0.10740        0.962\n",
      "            65          -0.10676        0.962\n",
      "            66          -0.10613        0.962\n",
      "            67          -0.10551        0.962\n",
      "            68          -0.10491        0.962\n",
      "            69          -0.10431        0.962\n",
      "            70          -0.10373        0.962\n",
      "            71          -0.10315        0.962\n",
      "            72          -0.10258        0.965\n",
      "            73          -0.10203        0.965\n",
      "            74          -0.10148        0.965\n",
      "            75          -0.10095        0.965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            76          -0.10042        0.965\n",
      "            77          -0.09990        0.965\n",
      "            78          -0.09939        0.965\n",
      "            79          -0.09888        0.965\n",
      "            80          -0.09839        0.965\n",
      "            81          -0.09790        0.965\n",
      "            82          -0.09742        0.965\n",
      "            83          -0.09695        0.965\n",
      "            84          -0.09648        0.965\n",
      "            85          -0.09602        0.965\n",
      "            86          -0.09557        0.965\n",
      "            87          -0.09512        0.965\n",
      "            88          -0.09468        0.965\n",
      "            89          -0.09425        0.965\n",
      "            90          -0.09382        0.965\n",
      "            91          -0.09340        0.965\n",
      "            92          -0.09298        0.965\n",
      "            93          -0.09257        0.965\n",
      "            94          -0.09217        0.965\n",
      "            95          -0.09177        0.965\n",
      "            96          -0.09137        0.965\n",
      "            97          -0.09098        0.965\n",
      "            98          -0.09060        0.965\n",
      "            99          -0.09022        0.965\n",
      "         Final          -0.08985        0.965\n",
      "\n",
      "The accuracy is 53.85%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.258\n",
      "             2          -0.43401        0.742\n",
      "             3          -0.39543        0.773\n",
      "             4          -0.36639        0.800\n",
      "             5          -0.34387        0.815\n",
      "             6          -0.32577        0.838\n",
      "             7          -0.31076        0.858\n",
      "             8          -0.29801        0.862\n",
      "             9          -0.28697        0.869\n",
      "            10          -0.27726        0.896\n",
      "            11          -0.26861        0.896\n",
      "            12          -0.26083        0.908\n",
      "            13          -0.25378        0.908\n",
      "            14          -0.24733        0.912\n",
      "            15          -0.24141        0.915\n",
      "            16          -0.23593        0.919\n",
      "            17          -0.23085        0.919\n",
      "            18          -0.22612        0.923\n",
      "            19          -0.22169        0.919\n",
      "            20          -0.21753        0.919\n",
      "            21          -0.21361        0.919\n",
      "            22          -0.20992        0.919\n",
      "            23          -0.20643        0.919\n",
      "            24          -0.20311        0.919\n",
      "            25          -0.19997        0.919\n",
      "            26          -0.19697        0.919\n",
      "            27          -0.19411        0.923\n",
      "            28          -0.19139        0.923\n",
      "            29          -0.18878        0.923\n",
      "            30          -0.18628        0.927\n",
      "            31          -0.18389        0.927\n",
      "            32          -0.18159        0.927\n",
      "            33          -0.17938        0.927\n",
      "            34          -0.17725        0.927\n",
      "            35          -0.17521        0.931\n",
      "            36          -0.17324        0.938\n",
      "            37          -0.17134        0.938\n",
      "            38          -0.16950        0.938\n",
      "            39          -0.16773        0.938\n",
      "            40          -0.16601        0.942\n",
      "            41          -0.16436        0.946\n",
      "            42          -0.16275        0.946\n",
      "            43          -0.16120        0.950\n",
      "            44          -0.15969        0.950\n",
      "            45          -0.15823        0.950\n",
      "            46          -0.15681        0.950\n",
      "            47          -0.15543        0.950\n",
      "            48          -0.15409        0.950\n",
      "            49          -0.15279        0.950\n",
      "            50          -0.15152        0.950\n",
      "            51          -0.15029        0.950\n",
      "            52          -0.14909        0.950\n",
      "            53          -0.14792        0.950\n",
      "            54          -0.14679        0.950\n",
      "            55          -0.14568        0.950\n",
      "            56          -0.14460        0.954\n",
      "            57          -0.14354        0.954\n",
      "            58          -0.14251        0.954\n",
      "            59          -0.14151        0.954\n",
      "            60          -0.14053        0.954\n",
      "            61          -0.13957        0.954\n",
      "            62          -0.13863        0.958\n",
      "            63          -0.13771        0.958\n",
      "            64          -0.13682        0.958\n",
      "            65          -0.13594        0.958\n",
      "            66          -0.13508        0.958\n",
      "            67          -0.13425        0.958\n",
      "            68          -0.13342        0.958\n",
      "            69          -0.13262        0.958\n",
      "            70          -0.13183        0.958\n",
      "            71          -0.13106        0.958\n",
      "            72          -0.13030        0.958\n",
      "            73          -0.12956        0.958\n",
      "            74          -0.12883        0.958\n",
      "            75          -0.12812        0.958\n",
      "            76          -0.12742        0.958\n",
      "            77          -0.12673        0.958\n",
      "            78          -0.12605        0.958\n",
      "            79          -0.12539        0.958\n",
      "            80          -0.12474        0.954\n",
      "            81          -0.12410        0.954\n",
      "            82          -0.12347        0.954\n",
      "            83          -0.12286        0.954\n",
      "            84          -0.12225        0.954\n",
      "            85          -0.12165        0.954\n",
      "            86          -0.12107        0.958\n",
      "            87          -0.12049        0.958\n",
      "            88          -0.11993        0.958\n",
      "            89          -0.11937        0.958\n",
      "            90          -0.11882        0.958\n",
      "            91          -0.11828        0.958\n",
      "            92          -0.11775        0.958\n",
      "            93          -0.11723        0.958\n",
      "            94          -0.11671        0.958\n",
      "            95          -0.11621        0.958\n",
      "            96          -0.11571        0.958\n",
      "            97          -0.11521        0.958\n",
      "            98          -0.11473        0.958\n",
      "            99          -0.11425        0.958\n",
      "         Final          -0.11378        0.958\n",
      "\n",
      "The accuracy is 81.54%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.258\n",
      "             2          -0.43097        0.742\n",
      "             3          -0.39250        0.781\n",
      "             4          -0.36331        0.796\n",
      "             5          -0.34050        0.827\n",
      "             6          -0.32205        0.850\n",
      "             7          -0.30669        0.858\n",
      "             8          -0.29359        0.865\n",
      "             9          -0.28222        0.877\n",
      "            10          -0.27221        0.885\n",
      "            11          -0.26331        0.896\n",
      "            12          -0.25530        0.904\n",
      "            13          -0.24805        0.912\n",
      "            14          -0.24144        0.927\n",
      "            15          -0.23538        0.931\n",
      "            16          -0.22979        0.935\n",
      "            17          -0.22461        0.935\n",
      "            18          -0.21979        0.935\n",
      "            19          -0.21530        0.935\n",
      "            20          -0.21109        0.935\n",
      "            21          -0.20714        0.938\n",
      "            22          -0.20342        0.942\n",
      "            23          -0.19991        0.942\n",
      "            24          -0.19659        0.942\n",
      "            25          -0.19344        0.942\n",
      "            26          -0.19045        0.942\n",
      "            27          -0.18760        0.942\n",
      "            28          -0.18488        0.942\n",
      "            29          -0.18229        0.946\n",
      "            30          -0.17981        0.946\n",
      "            31          -0.17744        0.946\n",
      "            32          -0.17516        0.946\n",
      "            33          -0.17298        0.946\n",
      "            34          -0.17088        0.946\n",
      "            35          -0.16885        0.946\n",
      "            36          -0.16691        0.946\n",
      "            37          -0.16503        0.946\n",
      "            38          -0.16322        0.946\n",
      "            39          -0.16147        0.946\n",
      "            40          -0.15978        0.946\n",
      "            41          -0.15815        0.946\n",
      "            42          -0.15657        0.946\n",
      "            43          -0.15503        0.946\n",
      "            44          -0.15355        0.946\n",
      "            45          -0.15210        0.946\n",
      "            46          -0.15071        0.946\n",
      "            47          -0.14935        0.946\n",
      "            48          -0.14803        0.946\n",
      "            49          -0.14674        0.946\n",
      "            50          -0.14549        0.946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            51          -0.14428        0.946\n",
      "            52          -0.14309        0.946\n",
      "            53          -0.14194        0.946\n",
      "            54          -0.14082        0.946\n",
      "            55          -0.13972        0.946\n",
      "            56          -0.13865        0.946\n",
      "            57          -0.13761        0.946\n",
      "            58          -0.13659        0.946\n",
      "            59          -0.13560        0.946\n",
      "            60          -0.13462        0.946\n",
      "            61          -0.13367        0.946\n",
      "            62          -0.13275        0.946\n",
      "            63          -0.13184        0.946\n",
      "            64          -0.13095        0.950\n",
      "            65          -0.13008        0.950\n",
      "            66          -0.12923        0.950\n",
      "            67          -0.12839        0.950\n",
      "            68          -0.12758        0.954\n",
      "            69          -0.12678        0.954\n",
      "            70          -0.12599        0.954\n",
      "            71          -0.12522        0.954\n",
      "            72          -0.12447        0.954\n",
      "            73          -0.12373        0.954\n",
      "            74          -0.12300        0.954\n",
      "            75          -0.12229        0.954\n",
      "            76          -0.12159        0.954\n",
      "            77          -0.12090        0.954\n",
      "            78          -0.12023        0.954\n",
      "            79          -0.11956        0.954\n",
      "            80          -0.11891        0.954\n",
      "            81          -0.11827        0.954\n",
      "            82          -0.11764        0.954\n",
      "            83          -0.11703        0.958\n",
      "            84          -0.11642        0.958\n",
      "            85          -0.11582        0.958\n",
      "            86          -0.11523        0.958\n",
      "            87          -0.11465        0.958\n",
      "            88          -0.11408        0.958\n",
      "            89          -0.11352        0.958\n",
      "            90          -0.11297        0.958\n",
      "            91          -0.11243        0.958\n",
      "            92          -0.11189        0.958\n",
      "            93          -0.11137        0.958\n",
      "            94          -0.11085        0.958\n",
      "            95          -0.11033        0.958\n",
      "            96          -0.10983        0.962\n",
      "            97          -0.10933        0.962\n",
      "            98          -0.10884        0.962\n",
      "            99          -0.10836        0.962\n",
      "         Final          -0.10789        0.962\n",
      "\n",
      "The accuracy is 70.77%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.277\n",
      "             2          -0.45293        0.723\n",
      "             3          -0.41110        0.769\n",
      "             4          -0.38070        0.788\n",
      "             5          -0.35763        0.808\n",
      "             6          -0.33930        0.838\n",
      "             7          -0.32420        0.850\n",
      "             8          -0.31141        0.858\n",
      "             9          -0.30034        0.865\n",
      "            10          -0.29060        0.877\n",
      "            11          -0.28193        0.877\n",
      "            12          -0.27411        0.888\n",
      "            13          -0.26702        0.896\n",
      "            14          -0.26052        0.900\n",
      "            15          -0.25454        0.900\n",
      "            16          -0.24900        0.908\n",
      "            17          -0.24385        0.908\n",
      "            18          -0.23904        0.912\n",
      "            19          -0.23453        0.912\n",
      "            20          -0.23029        0.912\n",
      "            21          -0.22630        0.912\n",
      "            22          -0.22253        0.912\n",
      "            23          -0.21896        0.912\n",
      "            24          -0.21557        0.912\n",
      "            25          -0.21234        0.912\n",
      "            26          -0.20928        0.912\n",
      "            27          -0.20635        0.912\n",
      "            28          -0.20355        0.912\n",
      "            29          -0.20088        0.912\n",
      "            30          -0.19832        0.912\n",
      "            31          -0.19587        0.919\n",
      "            32          -0.19351        0.931\n",
      "            33          -0.19124        0.931\n",
      "            34          -0.18907        0.935\n",
      "            35          -0.18697        0.938\n",
      "            36          -0.18495        0.938\n",
      "            37          -0.18300        0.938\n",
      "            38          -0.18112        0.938\n",
      "            39          -0.17930        0.942\n",
      "            40          -0.17755        0.946\n",
      "            41          -0.17585        0.946\n",
      "            42          -0.17420        0.946\n",
      "            43          -0.17261        0.946\n",
      "            44          -0.17106        0.946\n",
      "            45          -0.16956        0.946\n",
      "            46          -0.16811        0.946\n",
      "            47          -0.16669        0.946\n",
      "            48          -0.16532        0.946\n",
      "            49          -0.16398        0.946\n",
      "            50          -0.16268        0.946\n",
      "            51          -0.16142        0.946\n",
      "            52          -0.16019        0.946\n",
      "            53          -0.15899        0.946\n",
      "            54          -0.15782        0.946\n",
      "            55          -0.15669        0.946\n",
      "            56          -0.15557        0.946\n",
      "            57          -0.15449        0.946\n",
      "            58          -0.15343        0.946\n",
      "            59          -0.15240        0.946\n",
      "            60          -0.15139        0.946\n",
      "            61          -0.15040        0.946\n",
      "            62          -0.14944        0.946\n",
      "            63          -0.14850        0.946\n",
      "            64          -0.14757        0.946\n",
      "            65          -0.14667        0.946\n",
      "            66          -0.14579        0.946\n",
      "            67          -0.14492        0.946\n",
      "            68          -0.14408        0.946\n",
      "            69          -0.14325        0.946\n",
      "            70          -0.14243        0.946\n",
      "            71          -0.14163        0.946\n",
      "            72          -0.14085        0.946\n",
      "            73          -0.14009        0.946\n",
      "            74          -0.13933        0.946\n",
      "            75          -0.13859        0.946\n",
      "            76          -0.13787        0.946\n",
      "            77          -0.13716        0.946\n",
      "            78          -0.13646        0.946\n",
      "            79          -0.13577        0.946\n",
      "            80          -0.13510        0.950\n",
      "            81          -0.13444        0.950\n",
      "            82          -0.13379        0.954\n",
      "            83          -0.13315        0.954\n",
      "            84          -0.13252        0.954\n",
      "            85          -0.13190        0.954\n",
      "            86          -0.13129        0.954\n",
      "            87          -0.13069        0.954\n",
      "            88          -0.13010        0.954\n",
      "            89          -0.12952        0.954\n",
      "            90          -0.12895        0.954\n",
      "            91          -0.12839        0.954\n",
      "            92          -0.12784        0.954\n",
      "            93          -0.12729        0.954\n",
      "            94          -0.12675        0.954\n",
      "            95          -0.12622        0.954\n",
      "            96          -0.12570        0.954\n",
      "            97          -0.12519        0.954\n",
      "            98          -0.12468        0.954\n",
      "            99          -0.12419        0.954\n",
      "         Final          -0.12369        0.954\n",
      "\n",
      "The accuracy is 80.00%\n"
     ]
    }
   ],
   "source": [
    "# Train and test without previous 2nd & 3rd word features\n",
    "data3 = data.drop(['previous 3 Entity tag','previous 3 POS','previous 3 POS tag','previous 3 word','previous 2 Entity tag',\n",
    "              'previous 2 POS','previous 2 POS tag','previous 2 word'], axis=1)\n",
    "accHist3 = maxEntTrain(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5538461538461539, 0.5384615384615384, 0.8153846153846154, 0.7076923076923077, 0.8]\n",
      "\n",
      "The average accuracy after removing previous 2nd&3rd words is: 68.31%\n"
     ]
    }
   ],
   "source": [
    "print(accHist3)\n",
    "print('\\nThe average accuracy after removing previous 2nd&3rd words is: {:.2%}'.format(np.sum(accHist3)/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.208\n",
      "             2          -0.36486        0.792\n",
      "             3          -0.33124        0.796\n",
      "             4          -0.30411        0.831\n",
      "             5          -0.28231        0.862\n",
      "             6          -0.26443        0.888\n",
      "             7          -0.24942        0.900\n",
      "             8          -0.23656        0.908\n",
      "             9          -0.22534        0.919\n",
      "            10          -0.21543        0.923\n",
      "            11          -0.20658        0.927\n",
      "            12          -0.19859        0.938\n",
      "            13          -0.19134        0.946\n",
      "            14          -0.18471        0.946\n",
      "            15          -0.17860        0.950\n",
      "            16          -0.17297        0.950\n",
      "            17          -0.16773        0.958\n",
      "            18          -0.16286        0.958\n",
      "            19          -0.15831        0.962\n",
      "            20          -0.15404        0.962\n",
      "            21          -0.15003        0.962\n",
      "            22          -0.14625        0.962\n",
      "            23          -0.14268        0.965\n",
      "            24          -0.13931        0.973\n",
      "            25          -0.13611        0.973\n",
      "            26          -0.13308        0.973\n",
      "            27          -0.13019        0.977\n",
      "            28          -0.12744        0.977\n",
      "            29          -0.12483        0.977\n",
      "            30          -0.12233        0.977\n",
      "            31          -0.11994        0.977\n",
      "            32          -0.11765        0.977\n",
      "            33          -0.11547        0.977\n",
      "            34          -0.11337        0.977\n",
      "            35          -0.11136        0.977\n",
      "            36          -0.10942        0.977\n",
      "            37          -0.10756        0.977\n",
      "            38          -0.10578        0.977\n",
      "            39          -0.10406        0.977\n",
      "            40          -0.10240        0.977\n",
      "            41          -0.10080        0.977\n",
      "            42          -0.09926        0.981\n",
      "            43          -0.09777        0.981\n",
      "            44          -0.09633        0.981\n",
      "            45          -0.09494        0.981\n",
      "            46          -0.09360        0.985\n",
      "            47          -0.09230        0.985\n",
      "            48          -0.09104        0.985\n",
      "            49          -0.08982        0.985\n",
      "            50          -0.08864        0.985\n",
      "            51          -0.08749        0.985\n",
      "            52          -0.08638        0.985\n",
      "            53          -0.08530        0.985\n",
      "            54          -0.08425        0.985\n",
      "            55          -0.08323        0.985\n",
      "            56          -0.08224        0.985\n",
      "            57          -0.08127        0.985\n",
      "            58          -0.08034        0.985\n",
      "            59          -0.07943        0.985\n",
      "            60          -0.07854        0.988\n",
      "            61          -0.07768        0.988\n",
      "            62          -0.07683        0.988\n",
      "            63          -0.07601        0.988\n",
      "            64          -0.07521        0.988\n",
      "            65          -0.07443        0.988\n",
      "            66          -0.07367        0.988\n",
      "            67          -0.07293        0.988\n",
      "            68          -0.07221        0.988\n",
      "            69          -0.07150        0.988\n",
      "            70          -0.07081        0.988\n",
      "            71          -0.07013        0.988\n",
      "            72          -0.06947        0.988\n",
      "            73          -0.06883        0.988\n",
      "            74          -0.06820        0.988\n",
      "            75          -0.06758        0.988\n",
      "            76          -0.06698        0.988\n",
      "            77          -0.06639        0.988\n",
      "            78          -0.06581        0.988\n",
      "            79          -0.06525        0.988\n",
      "            80          -0.06469        0.988\n",
      "            81          -0.06415        0.988\n",
      "            82          -0.06362        0.988\n",
      "            83          -0.06310        0.988\n",
      "            84          -0.06259        0.988\n",
      "            85          -0.06209        0.988\n",
      "            86          -0.06160        0.988\n",
      "            87          -0.06112        0.988\n",
      "            88          -0.06065        0.988\n",
      "            89          -0.06019        0.988\n",
      "            90          -0.05973        0.988\n",
      "            91          -0.05929        0.988\n",
      "            92          -0.05885        0.988\n",
      "            93          -0.05842        0.988\n",
      "            94          -0.05800        0.988\n",
      "            95          -0.05759        0.988\n",
      "            96          -0.05718        0.988\n",
      "            97          -0.05678        0.988\n",
      "            98          -0.05639        0.988\n",
      "            99          -0.05601        0.988\n",
      "         Final          -0.05563        0.988\n",
      "\n",
      "The accuracy is 67.69%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.169\n",
      "             2          -0.30121        0.831\n",
      "             3          -0.27261        0.835\n",
      "             4          -0.24955        0.865\n",
      "             5          -0.23135        0.888\n",
      "             6          -0.21661        0.908\n",
      "             7          -0.20432        0.923\n",
      "             8          -0.19380        0.927\n",
      "             9          -0.18464        0.935\n",
      "            10          -0.17653        0.942\n",
      "            11          -0.16926        0.946\n",
      "            12          -0.16269        0.946\n",
      "            13          -0.15671        0.954\n",
      "            14          -0.15122        0.954\n",
      "            15          -0.14616        0.954\n",
      "            16          -0.14148        0.954\n",
      "            17          -0.13713        0.958\n",
      "            18          -0.13307        0.958\n",
      "            19          -0.12927        0.962\n",
      "            20          -0.12571        0.962\n",
      "            21          -0.12237        0.962\n",
      "            22          -0.11921        0.965\n",
      "            23          -0.11624        0.969\n",
      "            24          -0.11342        0.973\n",
      "            25          -0.11076        0.973\n",
      "            26          -0.10823        0.977\n",
      "            27          -0.10582        0.985\n",
      "            28          -0.10354        0.988\n",
      "            29          -0.10136        0.988\n",
      "            30          -0.09928        0.988\n",
      "            31          -0.09729        0.992\n",
      "            32          -0.09539        0.992\n",
      "            33          -0.09357        0.992\n",
      "            34          -0.09183        0.992\n",
      "            35          -0.09015        0.992\n",
      "            36          -0.08855        0.992\n",
      "            37          -0.08700        0.992\n",
      "            38          -0.08552        0.992\n",
      "            39          -0.08409        0.992\n",
      "            40          -0.08271        0.992\n",
      "            41          -0.08138        0.992\n",
      "            42          -0.08010        0.992\n",
      "            43          -0.07887        0.992\n",
      "            44          -0.07767        0.992\n",
      "            45          -0.07652        0.992\n",
      "            46          -0.07540        0.992\n",
      "            47          -0.07432        0.992\n",
      "            48          -0.07327        0.992\n",
      "            49          -0.07225        0.992\n",
      "            50          -0.07127        0.992\n",
      "            51          -0.07032        0.992\n",
      "            52          -0.06939        0.992\n",
      "            53          -0.06849        0.992\n",
      "            54          -0.06761        0.992\n",
      "            55          -0.06677        0.992\n",
      "            56          -0.06594        0.992\n",
      "            57          -0.06514        0.992\n",
      "            58          -0.06435        0.992\n",
      "            59          -0.06359        0.992\n",
      "            60          -0.06285        0.996\n",
      "            61          -0.06213        0.996\n",
      "            62          -0.06143        0.996\n",
      "            63          -0.06074        0.996\n",
      "            64          -0.06007        0.996\n",
      "            65          -0.05942        0.996\n",
      "            66          -0.05878        0.996\n",
      "            67          -0.05816        0.996\n",
      "            68          -0.05755        0.996\n",
      "            69          -0.05696        0.996\n",
      "            70          -0.05638        0.996\n",
      "            71          -0.05581        0.996\n",
      "            72          -0.05525        0.996\n",
      "            73          -0.05471        0.996\n",
      "            74          -0.05418        0.996\n",
      "            75          -0.05366        0.996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            76          -0.05316        0.996\n",
      "            77          -0.05266        0.996\n",
      "            78          -0.05217        0.996\n",
      "            79          -0.05169        0.996\n",
      "            80          -0.05123        0.996\n",
      "            81          -0.05077        0.996\n",
      "            82          -0.05032        0.996\n",
      "            83          -0.04988        0.996\n",
      "            84          -0.04945        0.996\n",
      "            85          -0.04903        0.996\n",
      "            86          -0.04861        0.996\n",
      "            87          -0.04821        0.996\n",
      "            88          -0.04781        0.996\n",
      "            89          -0.04741        0.996\n",
      "            90          -0.04703        0.996\n",
      "            91          -0.04665        0.996\n",
      "            92          -0.04628        0.996\n",
      "            93          -0.04592        0.996\n",
      "            94          -0.04556        0.996\n",
      "            95          -0.04521        0.996\n",
      "            96          -0.04486        0.996\n",
      "            97          -0.04452        0.996\n",
      "            98          -0.04419        0.996\n",
      "            99          -0.04386        0.996\n",
      "         Final          -0.04353        0.996\n",
      "\n",
      "The accuracy is 50.77%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.258\n",
      "             2          -0.43546        0.742\n",
      "             3          -0.39429        0.769\n",
      "             4          -0.36217        0.819\n",
      "             5          -0.33655        0.835\n",
      "             6          -0.31551        0.865\n",
      "             7          -0.29778        0.885\n",
      "             8          -0.28251        0.900\n",
      "             9          -0.26915        0.915\n",
      "            10          -0.25729        0.919\n",
      "            11          -0.24666        0.923\n",
      "            12          -0.23704        0.931\n",
      "            13          -0.22827        0.935\n",
      "            14          -0.22023        0.938\n",
      "            15          -0.21282        0.950\n",
      "            16          -0.20596        0.950\n",
      "            17          -0.19959        0.969\n",
      "            18          -0.19364        0.969\n",
      "            19          -0.18808        0.969\n",
      "            20          -0.18286        0.973\n",
      "            21          -0.17795        0.977\n",
      "            22          -0.17333        0.977\n",
      "            23          -0.16896        0.977\n",
      "            24          -0.16482        0.981\n",
      "            25          -0.16091        0.981\n",
      "            26          -0.15718        0.981\n",
      "            27          -0.15365        0.981\n",
      "            28          -0.15028        0.981\n",
      "            29          -0.14706        0.981\n",
      "            30          -0.14400        0.981\n",
      "            31          -0.14107        0.985\n",
      "            32          -0.13826        0.985\n",
      "            33          -0.13557        0.985\n",
      "            34          -0.13299        0.985\n",
      "            35          -0.13052        0.988\n",
      "            36          -0.12815        0.988\n",
      "            37          -0.12586        0.988\n",
      "            38          -0.12366        0.988\n",
      "            39          -0.12155        0.988\n",
      "            40          -0.11951        0.988\n",
      "            41          -0.11754        0.988\n",
      "            42          -0.11564        0.988\n",
      "            43          -0.11380        0.988\n",
      "            44          -0.11203        0.988\n",
      "            45          -0.11032        0.988\n",
      "            46          -0.10866        0.988\n",
      "            47          -0.10705        0.988\n",
      "            48          -0.10550        0.988\n",
      "            49          -0.10399        0.988\n",
      "            50          -0.10253        0.988\n",
      "            51          -0.10111        0.988\n",
      "            52          -0.09974        0.988\n",
      "            53          -0.09840        0.988\n",
      "            54          -0.09710        0.988\n",
      "            55          -0.09584        0.992\n",
      "            56          -0.09461        0.992\n",
      "            57          -0.09342        0.992\n",
      "            58          -0.09226        0.992\n",
      "            59          -0.09113        0.992\n",
      "            60          -0.09003        0.996\n",
      "            61          -0.08896        0.996\n",
      "            62          -0.08791        0.996\n",
      "            63          -0.08689        0.996\n",
      "            64          -0.08590        0.996\n",
      "            65          -0.08493        0.996\n",
      "            66          -0.08398        0.996\n",
      "            67          -0.08306        0.996\n",
      "            68          -0.08216        0.996\n",
      "            69          -0.08128        0.996\n",
      "            70          -0.08042        0.996\n",
      "            71          -0.07958        0.996\n",
      "            72          -0.07876        0.996\n",
      "            73          -0.07795        0.996\n",
      "            74          -0.07717        0.996\n",
      "            75          -0.07640        0.996\n",
      "            76          -0.07564        0.996\n",
      "            77          -0.07491        0.996\n",
      "            78          -0.07419        0.996\n",
      "            79          -0.07348        0.996\n",
      "            80          -0.07279        0.996\n",
      "            81          -0.07211        0.996\n",
      "            82          -0.07145        0.996\n",
      "            83          -0.07080        0.996\n",
      "            84          -0.07016        0.996\n",
      "            85          -0.06953        0.996\n",
      "            86          -0.06892        0.996\n",
      "            87          -0.06832        0.996\n",
      "            88          -0.06772        0.996\n",
      "            89          -0.06715        0.996\n",
      "            90          -0.06658        0.996\n",
      "            91          -0.06602        0.996\n",
      "            92          -0.06547        0.996\n",
      "            93          -0.06493        0.996\n",
      "            94          -0.06440        0.996\n",
      "            95          -0.06388        0.996\n",
      "            96          -0.06337        0.996\n",
      "            97          -0.06287        0.996\n",
      "            98          -0.06238        0.996\n",
      "            99          -0.06189        0.996\n",
      "         Final          -0.06142        0.996\n",
      "\n",
      "The accuracy is 80.00%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.258\n",
      "             2          -0.43732        0.742\n",
      "             3          -0.39593        0.762\n",
      "             4          -0.36363        0.819\n",
      "             5          -0.33794        0.850\n",
      "             6          -0.31695        0.877\n",
      "             7          -0.29935        0.888\n",
      "             8          -0.28427        0.888\n",
      "             9          -0.27110        0.896\n",
      "            10          -0.25944        0.904\n",
      "            11          -0.24900        0.908\n",
      "            12          -0.23956        0.915\n",
      "            13          -0.23095        0.915\n",
      "            14          -0.22305        0.927\n",
      "            15          -0.21577        0.938\n",
      "            16          -0.20901        0.938\n",
      "            17          -0.20273        0.946\n",
      "            18          -0.19686        0.950\n",
      "            19          -0.19137        0.950\n",
      "            20          -0.18621        0.950\n",
      "            21          -0.18135        0.958\n",
      "            22          -0.17676        0.958\n",
      "            23          -0.17243        0.962\n",
      "            24          -0.16832        0.973\n",
      "            25          -0.16442        0.973\n",
      "            26          -0.16072        0.973\n",
      "            27          -0.15719        0.973\n",
      "            28          -0.15383        0.973\n",
      "            29          -0.15063        0.973\n",
      "            30          -0.14757        0.973\n",
      "            31          -0.14464        0.977\n",
      "            32          -0.14183        0.977\n",
      "            33          -0.13914        0.977\n",
      "            34          -0.13657        0.981\n",
      "            35          -0.13409        0.981\n",
      "            36          -0.13171        0.981\n",
      "            37          -0.12942        0.981\n",
      "            38          -0.12721        0.981\n",
      "            39          -0.12509        0.981\n",
      "            40          -0.12304        0.981\n",
      "            41          -0.12107        0.981\n",
      "            42          -0.11916        0.981\n",
      "            43          -0.11732        0.981\n",
      "            44          -0.11554        0.985\n",
      "            45          -0.11381        0.988\n",
      "            46          -0.11215        0.988\n",
      "            47          -0.11053        0.988\n",
      "            48          -0.10897        0.988\n",
      "            49          -0.10745        0.988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            50          -0.10598        0.988\n",
      "            51          -0.10456        0.992\n",
      "            52          -0.10317        0.992\n",
      "            53          -0.10183        0.992\n",
      "            54          -0.10052        0.992\n",
      "            55          -0.09925        0.992\n",
      "            56          -0.09801        0.992\n",
      "            57          -0.09681        0.992\n",
      "            58          -0.09564        0.992\n",
      "            59          -0.09451        0.992\n",
      "            60          -0.09340        0.992\n",
      "            61          -0.09232        0.992\n",
      "            62          -0.09126        0.992\n",
      "            63          -0.09024        0.992\n",
      "            64          -0.08924        0.992\n",
      "            65          -0.08826        0.992\n",
      "            66          -0.08731        0.992\n",
      "            67          -0.08638        0.992\n",
      "            68          -0.08547        0.992\n",
      "            69          -0.08458        0.992\n",
      "            70          -0.08371        0.992\n",
      "            71          -0.08286        0.992\n",
      "            72          -0.08204        0.992\n",
      "            73          -0.08123        0.992\n",
      "            74          -0.08043        0.992\n",
      "            75          -0.07966        0.992\n",
      "            76          -0.07890        0.992\n",
      "            77          -0.07816        0.992\n",
      "            78          -0.07743        0.992\n",
      "            79          -0.07672        0.992\n",
      "            80          -0.07602        0.992\n",
      "            81          -0.07534        0.992\n",
      "            82          -0.07467        0.992\n",
      "            83          -0.07401        0.992\n",
      "            84          -0.07337        0.992\n",
      "            85          -0.07274        0.992\n",
      "            86          -0.07212        0.992\n",
      "            87          -0.07152        0.992\n",
      "            88          -0.07092        0.992\n",
      "            89          -0.07034        0.992\n",
      "            90          -0.06976        0.992\n",
      "            91          -0.06920        0.992\n",
      "            92          -0.06865        0.992\n",
      "            93          -0.06811        0.992\n",
      "            94          -0.06757        0.992\n",
      "            95          -0.06705        0.992\n",
      "            96          -0.06654        0.992\n",
      "            97          -0.06603        0.992\n",
      "            98          -0.06554        0.992\n",
      "            99          -0.06505        0.992\n",
      "         Final          -0.06457        0.992\n",
      "\n",
      "The accuracy is 78.46%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.277\n",
      "             2          -0.45516        0.723\n",
      "             3          -0.40946        0.777\n",
      "             4          -0.37473        0.819\n",
      "             5          -0.34759        0.846\n",
      "             6          -0.32562        0.865\n",
      "             7          -0.30731        0.885\n",
      "             8          -0.29167        0.896\n",
      "             9          -0.27806        0.896\n",
      "            10          -0.26603        0.904\n",
      "            11          -0.25528        0.912\n",
      "            12          -0.24557        0.919\n",
      "            13          -0.23674        0.919\n",
      "            14          -0.22865        0.923\n",
      "            15          -0.22121        0.931\n",
      "            16          -0.21432        0.935\n",
      "            17          -0.20792        0.946\n",
      "            18          -0.20195        0.946\n",
      "            19          -0.19637        0.954\n",
      "            20          -0.19114        0.958\n",
      "            21          -0.18622        0.958\n",
      "            22          -0.18158        0.958\n",
      "            23          -0.17721        0.962\n",
      "            24          -0.17306        0.969\n",
      "            25          -0.16914        0.969\n",
      "            26          -0.16541        0.969\n",
      "            27          -0.16186        0.973\n",
      "            28          -0.15848        0.973\n",
      "            29          -0.15526        0.973\n",
      "            30          -0.15219        0.973\n",
      "            31          -0.14925        0.973\n",
      "            32          -0.14643        0.973\n",
      "            33          -0.14374        0.977\n",
      "            34          -0.14116        0.977\n",
      "            35          -0.13867        0.977\n",
      "            36          -0.13629        0.977\n",
      "            37          -0.13400        0.977\n",
      "            38          -0.13179        0.981\n",
      "            39          -0.12967        0.981\n",
      "            40          -0.12762        0.981\n",
      "            41          -0.12565        0.981\n",
      "            42          -0.12374        0.981\n",
      "            43          -0.12190        0.981\n",
      "            44          -0.12012        0.981\n",
      "            45          -0.11840        0.981\n",
      "            46          -0.11673        0.981\n",
      "            47          -0.11512        0.981\n",
      "            48          -0.11356        0.981\n",
      "            49          -0.11205        0.981\n",
      "            50          -0.11058        0.985\n",
      "            51          -0.10915        0.985\n",
      "            52          -0.10777        0.985\n",
      "            53          -0.10643        0.985\n",
      "            54          -0.10512        0.985\n",
      "            55          -0.10385        0.985\n",
      "            56          -0.10262        0.985\n",
      "            57          -0.10142        0.985\n",
      "            58          -0.10025        0.985\n",
      "            59          -0.09911        0.985\n",
      "            60          -0.09801        0.985\n",
      "            61          -0.09693        0.985\n",
      "            62          -0.09588        0.985\n",
      "            63          -0.09485        0.985\n",
      "            64          -0.09385        0.985\n",
      "            65          -0.09288        0.985\n",
      "            66          -0.09192        0.985\n",
      "            67          -0.09099        0.985\n",
      "            68          -0.09009        0.985\n",
      "            69          -0.08920        0.985\n",
      "            70          -0.08833        0.985\n",
      "            71          -0.08748        0.988\n",
      "            72          -0.08666        0.988\n",
      "            73          -0.08585        0.988\n",
      "            74          -0.08505        0.988\n",
      "            75          -0.08428        0.988\n",
      "            76          -0.08352        0.988\n",
      "            77          -0.08278        0.988\n",
      "            78          -0.08205        0.988\n",
      "            79          -0.08134        0.988\n",
      "            80          -0.08064        0.988\n",
      "            81          -0.07996        0.988\n",
      "            82          -0.07929        0.988\n",
      "            83          -0.07863        0.988\n",
      "            84          -0.07799        0.988\n",
      "            85          -0.07736        0.988\n",
      "            86          -0.07674        0.988\n",
      "            87          -0.07613        0.988\n",
      "            88          -0.07554        0.988\n",
      "            89          -0.07495        0.988\n",
      "            90          -0.07438        0.988\n",
      "            91          -0.07381        0.988\n",
      "            92          -0.07326        0.988\n",
      "            93          -0.07272        0.988\n",
      "            94          -0.07218        0.988\n",
      "            95          -0.07166        0.988\n",
      "            96          -0.07114        0.988\n",
      "            97          -0.07064        0.988\n",
      "            98          -0.07014        0.988\n",
      "            99          -0.06965        0.988\n",
      "         Final          -0.06917        0.988\n",
      "\n",
      "The accuracy is 76.92%\n"
     ]
    }
   ],
   "source": [
    "# Train and test without year features\n",
    "data4 = data.drop(['modifiedyear', 'registrationyear'], axis=1)\n",
    "accHist4 = maxEntTrain(data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.676923076923077, 0.5076923076923077, 0.8, 0.7846153846153846, 0.7692307692307693]\n",
      "\n",
      "The average accuracy after removing year features is: 70.77%\n"
     ]
    }
   ],
   "source": [
    "print(accHist4)\n",
    "print('\\nThe average accuracy after removing year features is: {:.2%}'.format(np.sum(accHist4)/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.208\n",
      "             2          -0.37195        0.792\n",
      "             3          -0.33781        0.792\n",
      "             4          -0.30909        0.812\n",
      "             5          -0.28515        0.850\n",
      "             6          -0.26505        0.877\n",
      "             7          -0.24796        0.896\n",
      "             8          -0.23323        0.915\n",
      "             9          -0.22037        0.931\n",
      "            10          -0.20904        0.942\n",
      "            11          -0.19896        0.958\n",
      "            12          -0.18991        0.962\n",
      "            13          -0.18175        0.969\n",
      "            14          -0.17434        0.973\n",
      "            15          -0.16756        0.973\n",
      "            16          -0.16135        0.977\n",
      "            17          -0.15563        0.977\n",
      "            18          -0.15033        0.977\n",
      "            19          -0.14542        0.985\n",
      "            20          -0.14084        0.985\n",
      "            21          -0.13657        0.985\n",
      "            22          -0.13258        0.988\n",
      "            23          -0.12883        0.988\n",
      "            24          -0.12530        0.988\n",
      "            25          -0.12197        0.988\n",
      "            26          -0.11884        0.988\n",
      "            27          -0.11587        0.988\n",
      "            28          -0.11306        0.988\n",
      "            29          -0.11039        0.988\n",
      "            30          -0.10786        0.988\n",
      "            31          -0.10544        0.988\n",
      "            32          -0.10314        0.992\n",
      "            33          -0.10095        0.992\n",
      "            34          -0.09886        0.992\n",
      "            35          -0.09685        0.992\n",
      "            36          -0.09493        0.992\n",
      "            37          -0.09310        0.992\n",
      "            38          -0.09133        0.992\n",
      "            39          -0.08964        0.992\n",
      "            40          -0.08801        0.992\n",
      "            41          -0.08645        0.992\n",
      "            42          -0.08494        0.988\n",
      "            43          -0.08349        0.992\n",
      "            44          -0.08210        0.992\n",
      "            45          -0.08075        0.992\n",
      "            46          -0.07945        0.992\n",
      "            47          -0.07819        0.992\n",
      "            48          -0.07697        0.992\n",
      "            49          -0.07580        0.992\n",
      "            50          -0.07466        0.992\n",
      "            51          -0.07356        0.992\n",
      "            52          -0.07249        0.992\n",
      "            53          -0.07145        0.992\n",
      "            54          -0.07045        0.992\n",
      "            55          -0.06948        0.992\n",
      "            56          -0.06853        0.992\n",
      "            57          -0.06761        0.992\n",
      "            58          -0.06672        0.992\n",
      "            59          -0.06585        0.992\n",
      "            60          -0.06501        0.992\n",
      "            61          -0.06419        0.992\n",
      "            62          -0.06339        0.992\n",
      "            63          -0.06261        0.992\n",
      "            64          -0.06185        0.992\n",
      "            65          -0.06111        0.992\n",
      "            66          -0.06039        0.992\n",
      "            67          -0.05969        0.992\n",
      "            68          -0.05900        0.992\n",
      "            69          -0.05833        0.992\n",
      "            70          -0.05768        0.992\n",
      "            71          -0.05704        0.992\n",
      "            72          -0.05642        0.992\n",
      "            73          -0.05581        0.992\n",
      "            74          -0.05522        0.992\n",
      "            75          -0.05464        0.992\n",
      "            76          -0.05407        0.996\n",
      "            77          -0.05351        0.996\n",
      "            78          -0.05297        0.996\n",
      "            79          -0.05244        0.996\n",
      "            80          -0.05192        0.996\n",
      "            81          -0.05141        0.996\n",
      "            82          -0.05091        0.996\n",
      "            83          -0.05042        0.996\n",
      "            84          -0.04994        0.996\n",
      "            85          -0.04947        0.996\n",
      "            86          -0.04901        0.996\n",
      "            87          -0.04856        0.996\n",
      "            88          -0.04811        0.996\n",
      "            89          -0.04768        0.996\n",
      "            90          -0.04725        0.996\n",
      "            91          -0.04683        0.996\n",
      "            92          -0.04642        0.996\n",
      "            93          -0.04602        0.996\n",
      "            94          -0.04562        0.996\n",
      "            95          -0.04524        0.996\n",
      "            96          -0.04485        0.996\n",
      "            97          -0.04448        0.996\n",
      "            98          -0.04411        0.996\n",
      "            99          -0.04375        0.996\n",
      "         Final          -0.04339        0.996\n",
      "\n",
      "The accuracy is 67.69%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.169\n",
      "             2          -0.31704        0.831\n",
      "             3          -0.29022        0.831\n",
      "             4          -0.26721        0.850\n",
      "             5          -0.24800        0.865\n",
      "             6          -0.23183        0.888\n",
      "             7          -0.21801        0.900\n",
      "             8          -0.20603        0.904\n",
      "             9          -0.19548        0.908\n",
      "            10          -0.18611        0.923\n",
      "            11          -0.17770        0.931\n",
      "            12          -0.17009        0.935\n",
      "            13          -0.16317        0.946\n",
      "            14          -0.15684        0.950\n",
      "            15          -0.15102        0.958\n",
      "            16          -0.14564        0.958\n",
      "            17          -0.14066        0.958\n",
      "            18          -0.13603        0.969\n",
      "            19          -0.13172        0.973\n",
      "            20          -0.12768        0.988\n",
      "            21          -0.12389        0.992\n",
      "            22          -0.12033        0.992\n",
      "            23          -0.11698        0.992\n",
      "            24          -0.11382        0.992\n",
      "            25          -0.11084        0.996\n",
      "            26          -0.10801        0.996\n",
      "            27          -0.10532        0.996\n",
      "            28          -0.10277        0.996\n",
      "            29          -0.10035        0.996\n",
      "            30          -0.09804        0.996\n",
      "            31          -0.09584        0.996\n",
      "            32          -0.09373        0.996\n",
      "            33          -0.09172        0.996\n",
      "            34          -0.08980        0.996\n",
      "            35          -0.08796        0.996\n",
      "            36          -0.08619        0.996\n",
      "            37          -0.08449        0.996\n",
      "            38          -0.08286        0.996\n",
      "            39          -0.08130        0.996\n",
      "            40          -0.07979        0.996\n",
      "            41          -0.07834        1.000\n",
      "            42          -0.07694        1.000\n",
      "            43          -0.07559        1.000\n",
      "            44          -0.07429        1.000\n",
      "            45          -0.07303        1.000\n",
      "            46          -0.07182        1.000\n",
      "            47          -0.07064        1.000\n",
      "            48          -0.06950        1.000\n",
      "            49          -0.06840        1.000\n",
      "            50          -0.06734        1.000\n",
      "            51          -0.06631        1.000\n",
      "            52          -0.06531        1.000\n",
      "            53          -0.06434        1.000\n",
      "            54          -0.06339        1.000\n",
      "            55          -0.06248        1.000\n",
      "            56          -0.06159        1.000\n",
      "            57          -0.06073        1.000\n",
      "            58          -0.05989        1.000\n",
      "            59          -0.05907        1.000\n",
      "            60          -0.05828        1.000\n",
      "            61          -0.05751        1.000\n",
      "            62          -0.05675        1.000\n",
      "            63          -0.05602        1.000\n",
      "            64          -0.05531        1.000\n",
      "            65          -0.05461        1.000\n",
      "            66          -0.05393        1.000\n",
      "            67          -0.05327        1.000\n",
      "            68          -0.05262        1.000\n",
      "            69          -0.05199        1.000\n",
      "            70          -0.05138        1.000\n",
      "            71          -0.05078        1.000\n",
      "            72          -0.05019        1.000\n",
      "            73          -0.04962        1.000\n",
      "            74          -0.04906        1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            75          -0.04851        1.000\n",
      "            76          -0.04797        1.000\n",
      "            77          -0.04745        1.000\n",
      "            78          -0.04694        1.000\n",
      "            79          -0.04644        1.000\n",
      "            80          -0.04595        1.000\n",
      "            81          -0.04546        1.000\n",
      "            82          -0.04499        1.000\n",
      "            83          -0.04453        1.000\n",
      "            84          -0.04408        1.000\n",
      "            85          -0.04364        1.000\n",
      "            86          -0.04321        1.000\n",
      "            87          -0.04278        1.000\n",
      "            88          -0.04236        1.000\n",
      "            89          -0.04195        1.000\n",
      "            90          -0.04155        1.000\n",
      "            91          -0.04116        1.000\n",
      "            92          -0.04077        1.000\n",
      "            93          -0.04040        1.000\n",
      "            94          -0.04002        1.000\n",
      "            95          -0.03966        1.000\n",
      "            96          -0.03930        1.000\n",
      "            97          -0.03895        1.000\n",
      "            98          -0.03860        1.000\n",
      "            99          -0.03826        1.000\n",
      "         Final          -0.03793        1.000\n",
      "\n",
      "The accuracy is 47.69%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.258\n",
      "             2          -0.44938        0.742\n",
      "             3          -0.41077        0.746\n",
      "             4          -0.37874        0.785\n",
      "             5          -0.35191        0.831\n",
      "             6          -0.32913        0.850\n",
      "             7          -0.30952        0.885\n",
      "             8          -0.29241        0.888\n",
      "             9          -0.27732        0.915\n",
      "            10          -0.26388        0.923\n",
      "            11          -0.25182        0.931\n",
      "            12          -0.24091        0.938\n",
      "            13          -0.23100        0.950\n",
      "            14          -0.22194        0.958\n",
      "            15          -0.21362        0.962\n",
      "            16          -0.20595        0.973\n",
      "            17          -0.19885        0.977\n",
      "            18          -0.19226        0.981\n",
      "            19          -0.18613        0.981\n",
      "            20          -0.18040        0.981\n",
      "            21          -0.17503        0.981\n",
      "            22          -0.17000        0.985\n",
      "            23          -0.16526        0.985\n",
      "            24          -0.16080        0.988\n",
      "            25          -0.15659        0.988\n",
      "            26          -0.15261        0.988\n",
      "            27          -0.14883        0.988\n",
      "            28          -0.14525        0.988\n",
      "            29          -0.14185        0.988\n",
      "            30          -0.13861        0.988\n",
      "            31          -0.13553        0.988\n",
      "            32          -0.13259        0.988\n",
      "            33          -0.12978        0.988\n",
      "            34          -0.12710        0.988\n",
      "            35          -0.12453        0.988\n",
      "            36          -0.12207        0.988\n",
      "            37          -0.11971        0.992\n",
      "            38          -0.11744        0.992\n",
      "            39          -0.11527        0.992\n",
      "            40          -0.11317        0.992\n",
      "            41          -0.11116        0.992\n",
      "            42          -0.10922        0.992\n",
      "            43          -0.10736        0.992\n",
      "            44          -0.10556        0.992\n",
      "            45          -0.10382        0.992\n",
      "            46          -0.10214        0.992\n",
      "            47          -0.10052        0.996\n",
      "            48          -0.09895        0.996\n",
      "            49          -0.09743        0.996\n",
      "            50          -0.09597        0.996\n",
      "            51          -0.09455        0.996\n",
      "            52          -0.09317        0.996\n",
      "            53          -0.09183        0.996\n",
      "            54          -0.09054        0.996\n",
      "            55          -0.08928        0.996\n",
      "            56          -0.08806        0.996\n",
      "            57          -0.08688        0.996\n",
      "            58          -0.08572        0.996\n",
      "            59          -0.08461        0.996\n",
      "            60          -0.08352        0.996\n",
      "            61          -0.08246        0.996\n",
      "            62          -0.08143        0.996\n",
      "            63          -0.08042        0.996\n",
      "            64          -0.07944        0.996\n",
      "            65          -0.07849        0.996\n",
      "            66          -0.07756        0.996\n",
      "            67          -0.07666        0.996\n",
      "            68          -0.07577        0.996\n",
      "            69          -0.07491        0.996\n",
      "            70          -0.07407        0.996\n",
      "            71          -0.07325        0.996\n",
      "            72          -0.07245        0.996\n",
      "            73          -0.07166        0.996\n",
      "            74          -0.07090        0.996\n",
      "            75          -0.07015        0.996\n",
      "            76          -0.06942        0.996\n",
      "            77          -0.06870        0.996\n",
      "            78          -0.06800        0.996\n",
      "            79          -0.06732        0.996\n",
      "            80          -0.06665        0.996\n",
      "            81          -0.06599        0.996\n",
      "            82          -0.06535        0.996\n",
      "            83          -0.06472        0.996\n",
      "            84          -0.06410        0.996\n",
      "            85          -0.06350        0.996\n",
      "            86          -0.06291        0.996\n",
      "            87          -0.06233        0.996\n",
      "            88          -0.06176        0.996\n",
      "            89          -0.06120        0.996\n",
      "            90          -0.06065        0.996\n",
      "            91          -0.06011        0.996\n",
      "            92          -0.05959        0.996\n",
      "            93          -0.05907        0.996\n",
      "            94          -0.05856        0.996\n",
      "            95          -0.05806        0.996\n",
      "            96          -0.05757        0.996\n",
      "            97          -0.05709        0.996\n",
      "            98          -0.05662        0.996\n",
      "            99          -0.05615        0.996\n",
      "         Final          -0.05570        0.996\n",
      "\n",
      "The accuracy is 84.62%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.258\n",
      "             2          -0.44543        0.742\n",
      "             3          -0.40448        0.746\n",
      "             4          -0.37083        0.800\n",
      "             5          -0.34288        0.831\n",
      "             6          -0.31932        0.846\n",
      "             7          -0.29915        0.877\n",
      "             8          -0.28165        0.900\n",
      "             9          -0.26628        0.919\n",
      "            10          -0.25265        0.931\n",
      "            11          -0.24046        0.938\n",
      "            12          -0.22949        0.954\n",
      "            13          -0.21954        0.965\n",
      "            14          -0.21048        0.973\n",
      "            15          -0.20218        0.977\n",
      "            16          -0.19455        0.981\n",
      "            17          -0.18751        0.981\n",
      "            18          -0.18099        0.981\n",
      "            19          -0.17492        0.981\n",
      "            20          -0.16927        0.985\n",
      "            21          -0.16399        0.985\n",
      "            22          -0.15904        0.988\n",
      "            23          -0.15439        0.988\n",
      "            24          -0.15002        0.988\n",
      "            25          -0.14589        0.988\n",
      "            26          -0.14199        0.992\n",
      "            27          -0.13831        0.992\n",
      "            28          -0.13481        0.996\n",
      "            29          -0.13149        0.996\n",
      "            30          -0.12833        0.996\n",
      "            31          -0.12533        0.996\n",
      "            32          -0.12247        0.996\n",
      "            33          -0.11973        0.996\n",
      "            34          -0.11712        1.000\n",
      "            35          -0.11462        1.000\n",
      "            36          -0.11223        1.000\n",
      "            37          -0.10994        1.000\n",
      "            38          -0.10774        1.000\n",
      "            39          -0.10562        1.000\n",
      "            40          -0.10359        1.000\n",
      "            41          -0.10164        1.000\n",
      "            42          -0.09976        1.000\n",
      "            43          -0.09795        1.000\n",
      "            44          -0.09621        1.000\n",
      "            45          -0.09452        1.000\n",
      "            46          -0.09290        1.000\n",
      "            47          -0.09133        1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            48          -0.08981        1.000\n",
      "            49          -0.08834        1.000\n",
      "            50          -0.08692        1.000\n",
      "            51          -0.08555        1.000\n",
      "            52          -0.08421        1.000\n",
      "            53          -0.08292        1.000\n",
      "            54          -0.08167        1.000\n",
      "            55          -0.08046        1.000\n",
      "            56          -0.07928        1.000\n",
      "            57          -0.07813        1.000\n",
      "            58          -0.07702        1.000\n",
      "            59          -0.07594        1.000\n",
      "            60          -0.07489        1.000\n",
      "            61          -0.07387        1.000\n",
      "            62          -0.07287        1.000\n",
      "            63          -0.07191        1.000\n",
      "            64          -0.07096        1.000\n",
      "            65          -0.07004        1.000\n",
      "            66          -0.06915        1.000\n",
      "            67          -0.06828        1.000\n",
      "            68          -0.06743        1.000\n",
      "            69          -0.06660        1.000\n",
      "            70          -0.06579        1.000\n",
      "            71          -0.06500        1.000\n",
      "            72          -0.06423        1.000\n",
      "            73          -0.06347        1.000\n",
      "            74          -0.06274        1.000\n",
      "            75          -0.06202        1.000\n",
      "            76          -0.06132        1.000\n",
      "            77          -0.06063        1.000\n",
      "            78          -0.05996        1.000\n",
      "            79          -0.05930        1.000\n",
      "            80          -0.05866        1.000\n",
      "            81          -0.05803        1.000\n",
      "            82          -0.05741        1.000\n",
      "            83          -0.05681        1.000\n",
      "            84          -0.05622        1.000\n",
      "            85          -0.05564        1.000\n",
      "            86          -0.05508        1.000\n",
      "            87          -0.05452        1.000\n",
      "            88          -0.05398        1.000\n",
      "            89          -0.05344        1.000\n",
      "            90          -0.05292        1.000\n",
      "            91          -0.05241        1.000\n",
      "            92          -0.05191        1.000\n",
      "            93          -0.05141        1.000\n",
      "            94          -0.05093        1.000\n",
      "            95          -0.05045        1.000\n",
      "            96          -0.04999        1.000\n",
      "            97          -0.04953        1.000\n",
      "            98          -0.04908        1.000\n",
      "            99          -0.04864        1.000\n",
      "         Final          -0.04820        1.000\n",
      "\n",
      "The accuracy is 72.31%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.277\n",
      "             2          -0.47157        0.723\n",
      "             3          -0.42992        0.742\n",
      "             4          -0.39600        0.788\n",
      "             5          -0.36800        0.815\n",
      "             6          -0.34447        0.846\n",
      "             7          -0.32436        0.862\n",
      "             8          -0.30689        0.877\n",
      "             9          -0.29152        0.888\n",
      "            10          -0.27786        0.912\n",
      "            11          -0.26560        0.931\n",
      "            12          -0.25452        0.938\n",
      "            13          -0.24444        0.946\n",
      "            14          -0.23521        0.950\n",
      "            15          -0.22673        0.962\n",
      "            16          -0.21890        0.965\n",
      "            17          -0.21165        0.977\n",
      "            18          -0.20490        0.977\n",
      "            19          -0.19861        0.977\n",
      "            20          -0.19273        0.977\n",
      "            21          -0.18721        0.981\n",
      "            22          -0.18202        0.981\n",
      "            23          -0.17713        0.981\n",
      "            24          -0.17252        0.988\n",
      "            25          -0.16816        0.992\n",
      "            26          -0.16403        0.992\n",
      "            27          -0.16011        0.992\n",
      "            28          -0.15638        0.992\n",
      "            29          -0.15284        0.992\n",
      "            30          -0.14946        0.992\n",
      "            31          -0.14624        0.992\n",
      "            32          -0.14316        0.992\n",
      "            33          -0.14022        0.992\n",
      "            34          -0.13740        0.992\n",
      "            35          -0.13470        0.992\n",
      "            36          -0.13211        0.992\n",
      "            37          -0.12962        0.992\n",
      "            38          -0.12723        0.992\n",
      "            39          -0.12493        0.992\n",
      "            40          -0.12272        0.992\n",
      "            41          -0.12058        0.992\n",
      "            42          -0.11853        0.992\n",
      "            43          -0.11654        0.996\n",
      "            44          -0.11463        0.996\n",
      "            45          -0.11278        0.996\n",
      "            46          -0.11099        0.996\n",
      "            47          -0.10927        0.996\n",
      "            48          -0.10759        0.996\n",
      "            49          -0.10597        0.996\n",
      "            50          -0.10441        0.996\n",
      "            51          -0.10288        0.996\n",
      "            52          -0.10141        0.996\n",
      "            53          -0.09998        0.996\n",
      "            54          -0.09859        0.996\n",
      "            55          -0.09724        0.996\n",
      "            56          -0.09593        0.996\n",
      "            57          -0.09466        0.996\n",
      "            58          -0.09342        0.996\n",
      "            59          -0.09221        0.996\n",
      "            60          -0.09104        0.996\n",
      "            61          -0.08990        0.996\n",
      "            62          -0.08878        0.996\n",
      "            63          -0.08770        0.996\n",
      "            64          -0.08664        0.996\n",
      "            65          -0.08561        0.996\n",
      "            66          -0.08461        0.996\n",
      "            67          -0.08363        0.996\n",
      "            68          -0.08267        0.996\n",
      "            69          -0.08174        0.996\n",
      "            70          -0.08083        0.996\n",
      "            71          -0.07994        0.996\n",
      "            72          -0.07907        0.996\n",
      "            73          -0.07822        0.996\n",
      "            74          -0.07739        0.996\n",
      "            75          -0.07658        0.996\n",
      "            76          -0.07578        0.996\n",
      "            77          -0.07500        0.996\n",
      "            78          -0.07424        0.996\n",
      "            79          -0.07350        0.996\n",
      "            80          -0.07277        0.996\n",
      "            81          -0.07205        0.996\n",
      "            82          -0.07136        0.996\n",
      "            83          -0.07067        0.996\n",
      "            84          -0.07000        0.996\n",
      "            85          -0.06934        0.996\n",
      "            86          -0.06869        0.996\n",
      "            87          -0.06806        0.996\n",
      "            88          -0.06744        0.996\n",
      "            89          -0.06683        0.996\n",
      "            90          -0.06623        0.996\n",
      "            91          -0.06565        0.996\n",
      "            92          -0.06507        0.996\n",
      "            93          -0.06451        0.996\n",
      "            94          -0.06395        0.996\n",
      "            95          -0.06341        0.996\n",
      "            96          -0.06287        0.996\n",
      "            97          -0.06235        0.996\n",
      "            98          -0.06183        0.996\n",
      "            99          -0.06132        0.996\n",
      "         Final          -0.06083        0.996\n",
      "\n",
      "The accuracy is 81.54%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.208\n",
      "             2          -0.36019        0.792\n",
      "             3          -0.32173        0.804\n",
      "             4          -0.29109        0.842\n",
      "             5          -0.26673        0.877\n",
      "             6          -0.24695        0.896\n",
      "             7          -0.23049        0.927\n",
      "             8          -0.21652        0.931\n",
      "             9          -0.20445        0.935\n",
      "            10          -0.19389        0.946\n",
      "            11          -0.18455        0.954\n",
      "            12          -0.17619        0.962\n",
      "            13          -0.16867        0.969\n",
      "            14          -0.16185        0.969\n",
      "            15          -0.15562        0.969\n",
      "            16          -0.14992        0.977\n",
      "            17          -0.14466        0.977\n",
      "            18          -0.13979        0.977\n",
      "            19          -0.13528        0.977\n",
      "            20          -0.13108        0.977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            21          -0.12715        0.977\n",
      "            22          -0.12347        0.977\n",
      "            23          -0.12002        0.981\n",
      "            24          -0.11677        0.988\n",
      "            25          -0.11371        0.988\n",
      "            26          -0.11081        0.988\n",
      "            27          -0.10807        0.988\n",
      "            28          -0.10548        0.988\n",
      "            29          -0.10301        0.988\n",
      "            30          -0.10067        0.988\n",
      "            31          -0.09843        0.988\n",
      "            32          -0.09630        0.992\n",
      "            33          -0.09427        0.992\n",
      "            34          -0.09233        0.992\n",
      "            35          -0.09047        0.992\n",
      "            36          -0.08869        0.992\n",
      "            37          -0.08698        0.992\n",
      "            38          -0.08534        0.992\n",
      "            39          -0.08377        0.992\n",
      "            40          -0.08226        0.992\n",
      "            41          -0.08080        0.992\n",
      "            42          -0.07940        0.992\n",
      "            43          -0.07805        0.992\n",
      "            44          -0.07675        0.992\n",
      "            45          -0.07549        0.992\n",
      "            46          -0.07428        0.992\n",
      "            47          -0.07310        0.992\n",
      "            48          -0.07197        0.996\n",
      "            49          -0.07087        0.996\n",
      "            50          -0.06981        0.996\n",
      "            51          -0.06878        0.996\n",
      "            52          -0.06778        0.996\n",
      "            53          -0.06681        0.996\n",
      "            54          -0.06588        0.996\n",
      "            55          -0.06497        0.996\n",
      "            56          -0.06408        0.996\n",
      "            57          -0.06322        0.996\n",
      "            58          -0.06239        0.996\n",
      "            59          -0.06158        0.996\n",
      "            60          -0.06079        0.996\n",
      "            61          -0.06002        0.996\n",
      "            62          -0.05927        0.996\n",
      "            63          -0.05854        0.996\n",
      "            64          -0.05783        0.996\n",
      "            65          -0.05714        0.996\n",
      "            66          -0.05647        0.996\n",
      "            67          -0.05581        0.996\n",
      "            68          -0.05517        0.996\n",
      "            69          -0.05454        0.996\n",
      "            70          -0.05393        0.996\n",
      "            71          -0.05333        0.996\n",
      "            72          -0.05275        0.996\n",
      "            73          -0.05218        0.996\n",
      "            74          -0.05162        0.996\n",
      "            75          -0.05108        0.996\n",
      "            76          -0.05055        0.996\n",
      "            77          -0.05002        0.996\n",
      "            78          -0.04952        0.996\n",
      "            79          -0.04902        0.996\n",
      "            80          -0.04853        0.996\n",
      "            81          -0.04805        0.996\n",
      "            82          -0.04758        0.996\n",
      "            83          -0.04712        0.996\n",
      "            84          -0.04667        0.996\n",
      "            85          -0.04623        0.996\n",
      "            86          -0.04580        0.996\n",
      "            87          -0.04538        0.996\n",
      "            88          -0.04496        0.996\n",
      "            89          -0.04456        0.996\n",
      "            90          -0.04416        0.996\n",
      "            91          -0.04376        0.996\n",
      "            92          -0.04338        0.996\n",
      "            93          -0.04300        0.996\n",
      "            94          -0.04263        0.996\n",
      "            95          -0.04227        0.996\n",
      "            96          -0.04191        0.996\n",
      "            97          -0.04156        0.996\n",
      "            98          -0.04121        0.996\n",
      "            99          -0.04087        0.996\n",
      "         Final          -0.04054        0.996\n",
      "\n",
      "The accuracy is 63.08%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.169\n",
      "             2          -0.30319        0.831\n",
      "             3          -0.27286        0.831\n",
      "             4          -0.24854        0.869\n",
      "             5          -0.22943        0.885\n",
      "             6          -0.21399        0.900\n",
      "             7          -0.20114        0.923\n",
      "             8          -0.19017        0.927\n",
      "             9          -0.18061        0.931\n",
      "            10          -0.17215        0.931\n",
      "            11          -0.16459        0.938\n",
      "            12          -0.15776        0.950\n",
      "            13          -0.15154        0.950\n",
      "            14          -0.14585        0.954\n",
      "            15          -0.14061        0.962\n",
      "            16          -0.13577        0.969\n",
      "            17          -0.13128        0.969\n",
      "            18          -0.12710        0.973\n",
      "            19          -0.12319        0.973\n",
      "            20          -0.11954        0.977\n",
      "            21          -0.11610        0.985\n",
      "            22          -0.11287        0.988\n",
      "            23          -0.10983        0.988\n",
      "            24          -0.10695        0.992\n",
      "            25          -0.10423        0.992\n",
      "            26          -0.10165        0.992\n",
      "            27          -0.09920        0.992\n",
      "            28          -0.09688        0.996\n",
      "            29          -0.09466        0.996\n",
      "            30          -0.09255        0.996\n",
      "            31          -0.09053        0.996\n",
      "            32          -0.08860        0.996\n",
      "            33          -0.08676        0.996\n",
      "            34          -0.08499        0.996\n",
      "            35          -0.08330        0.996\n",
      "            36          -0.08167        0.996\n",
      "            37          -0.08011        0.996\n",
      "            38          -0.07861        0.996\n",
      "            39          -0.07717        0.996\n",
      "            40          -0.07577        0.996\n",
      "            41          -0.07443        0.996\n",
      "            42          -0.07314        1.000\n",
      "            43          -0.07189        1.000\n",
      "            44          -0.07069        1.000\n",
      "            45          -0.06953        1.000\n",
      "            46          -0.06840        1.000\n",
      "            47          -0.06731        1.000\n",
      "            48          -0.06626        1.000\n",
      "            49          -0.06524        1.000\n",
      "            50          -0.06425        1.000\n",
      "            51          -0.06329        1.000\n",
      "            52          -0.06235        1.000\n",
      "            53          -0.06145        1.000\n",
      "            54          -0.06057        1.000\n",
      "            55          -0.05972        1.000\n",
      "            56          -0.05889        1.000\n",
      "            57          -0.05809        1.000\n",
      "            58          -0.05730        1.000\n",
      "            59          -0.05654        1.000\n",
      "            60          -0.05580        1.000\n",
      "            61          -0.05507        1.000\n",
      "            62          -0.05437        1.000\n",
      "            63          -0.05368        1.000\n",
      "            64          -0.05301        1.000\n",
      "            65          -0.05236        1.000\n",
      "            66          -0.05172        1.000\n",
      "            67          -0.05110        1.000\n",
      "            68          -0.05049        1.000\n",
      "            69          -0.04990        1.000\n",
      "            70          -0.04932        1.000\n",
      "            71          -0.04876        1.000\n",
      "            72          -0.04820        1.000\n",
      "            73          -0.04766        1.000\n",
      "            74          -0.04713        1.000\n",
      "            75          -0.04662        1.000\n",
      "            76          -0.04611        1.000\n",
      "            77          -0.04562        1.000\n",
      "            78          -0.04513        1.000\n",
      "            79          -0.04466        1.000\n",
      "            80          -0.04419        1.000\n",
      "            81          -0.04374        1.000\n",
      "            82          -0.04329        1.000\n",
      "            83          -0.04286        1.000\n",
      "            84          -0.04243        1.000\n",
      "            85          -0.04201        1.000\n",
      "            86          -0.04160        1.000\n",
      "            87          -0.04120        1.000\n",
      "            88          -0.04080        1.000\n",
      "            89          -0.04041        1.000\n",
      "            90          -0.04003        1.000\n",
      "            91          -0.03966        1.000\n",
      "            92          -0.03929        1.000\n",
      "            93          -0.03893        1.000\n",
      "            94          -0.03858        1.000\n",
      "            95          -0.03823        1.000\n",
      "            96          -0.03789        1.000\n",
      "            97          -0.03756        1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            98          -0.03723        1.000\n",
      "            99          -0.03690        1.000\n",
      "         Final          -0.03658        1.000\n",
      "\n",
      "The accuracy is 52.31%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.258\n",
      "             2          -0.43449        0.742\n",
      "             3          -0.39038        0.773\n",
      "             4          -0.35593        0.823\n",
      "             5          -0.32838        0.850\n",
      "             6          -0.30573        0.877\n",
      "             7          -0.28664        0.892\n",
      "             8          -0.27023        0.919\n",
      "             9          -0.25591        0.931\n",
      "            10          -0.24326        0.931\n",
      "            11          -0.23197        0.935\n",
      "            12          -0.22180        0.942\n",
      "            13          -0.21259        0.954\n",
      "            14          -0.20419        0.958\n",
      "            15          -0.19649        0.965\n",
      "            16          -0.18941        0.973\n",
      "            17          -0.18286        0.981\n",
      "            18          -0.17679        0.985\n",
      "            19          -0.17114        0.985\n",
      "            20          -0.16588        0.985\n",
      "            21          -0.16095        0.985\n",
      "            22          -0.15632        0.985\n",
      "            23          -0.15198        0.985\n",
      "            24          -0.14789        0.985\n",
      "            25          -0.14402        0.985\n",
      "            26          -0.14037        0.985\n",
      "            27          -0.13691        0.988\n",
      "            28          -0.13363        0.988\n",
      "            29          -0.13052        0.988\n",
      "            30          -0.12755        0.988\n",
      "            31          -0.12473        0.988\n",
      "            32          -0.12203        0.988\n",
      "            33          -0.11946        0.988\n",
      "            34          -0.11700        0.988\n",
      "            35          -0.11465        0.988\n",
      "            36          -0.11239        0.992\n",
      "            37          -0.11023        0.992\n",
      "            38          -0.10816        0.992\n",
      "            39          -0.10616        0.992\n",
      "            40          -0.10425        0.992\n",
      "            41          -0.10240        0.992\n",
      "            42          -0.10063        0.992\n",
      "            43          -0.09892        0.992\n",
      "            44          -0.09727        0.992\n",
      "            45          -0.09567        0.992\n",
      "            46          -0.09414        0.992\n",
      "            47          -0.09265        0.992\n",
      "            48          -0.09121        0.992\n",
      "            49          -0.08982        0.992\n",
      "            50          -0.08847        0.992\n",
      "            51          -0.08717        0.992\n",
      "            52          -0.08591        0.996\n",
      "            53          -0.08468        0.996\n",
      "            54          -0.08349        0.996\n",
      "            55          -0.08234        0.996\n",
      "            56          -0.08122        0.996\n",
      "            57          -0.08013        0.996\n",
      "            58          -0.07908        0.996\n",
      "            59          -0.07805        0.996\n",
      "            60          -0.07705        0.996\n",
      "            61          -0.07607        0.996\n",
      "            62          -0.07513        0.996\n",
      "            63          -0.07421        0.996\n",
      "            64          -0.07331        0.996\n",
      "            65          -0.07243        0.996\n",
      "            66          -0.07158        0.996\n",
      "            67          -0.07075        0.996\n",
      "            68          -0.06993        0.996\n",
      "            69          -0.06914        0.996\n",
      "            70          -0.06837        0.996\n",
      "            71          -0.06761        0.996\n",
      "            72          -0.06687        0.996\n",
      "            73          -0.06615        0.996\n",
      "            74          -0.06545        0.996\n",
      "            75          -0.06476        0.996\n",
      "            76          -0.06409        0.996\n",
      "            77          -0.06343        0.996\n",
      "            78          -0.06278        0.996\n",
      "            79          -0.06215        0.996\n",
      "            80          -0.06154        0.996\n",
      "            81          -0.06093        0.996\n",
      "            82          -0.06034        0.996\n",
      "            83          -0.05976        0.996\n",
      "            84          -0.05919        0.996\n",
      "            85          -0.05864        0.996\n",
      "            86          -0.05809        0.996\n",
      "            87          -0.05756        0.996\n",
      "            88          -0.05703        0.996\n",
      "            89          -0.05652        0.996\n",
      "            90          -0.05601        0.996\n",
      "            91          -0.05552        0.996\n",
      "            92          -0.05503        0.996\n",
      "            93          -0.05455        0.996\n",
      "            94          -0.05409        0.996\n",
      "            95          -0.05363        0.996\n",
      "            96          -0.05317        0.996\n",
      "            97          -0.05273        0.996\n",
      "            98          -0.05230        0.996\n",
      "            99          -0.05187        0.996\n",
      "         Final          -0.05145        0.996\n",
      "\n",
      "The accuracy is 80.00%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.258\n",
      "             2          -0.43489        0.742\n",
      "             3          -0.39011        0.777\n",
      "             4          -0.35483        0.815\n",
      "             5          -0.32651        0.850\n",
      "             6          -0.30323        0.881\n",
      "             7          -0.28363        0.892\n",
      "             8          -0.26683        0.912\n",
      "             9          -0.25219        0.915\n",
      "            10          -0.23927        0.927\n",
      "            11          -0.22777        0.954\n",
      "            12          -0.21743        0.965\n",
      "            13          -0.20807        0.969\n",
      "            14          -0.19955        0.969\n",
      "            15          -0.19175        0.973\n",
      "            16          -0.18458        0.973\n",
      "            17          -0.17796        0.973\n",
      "            18          -0.17182        0.977\n",
      "            19          -0.16611        0.977\n",
      "            20          -0.16079        0.981\n",
      "            21          -0.15581        0.985\n",
      "            22          -0.15114        0.985\n",
      "            23          -0.14676        0.985\n",
      "            24          -0.14263        0.985\n",
      "            25          -0.13873        0.985\n",
      "            26          -0.13505        0.988\n",
      "            27          -0.13156        0.996\n",
      "            28          -0.12825        0.996\n",
      "            29          -0.12511        0.996\n",
      "            30          -0.12213        0.996\n",
      "            31          -0.11928        0.996\n",
      "            32          -0.11657        0.996\n",
      "            33          -0.11398        0.996\n",
      "            34          -0.11151        1.000\n",
      "            35          -0.10914        1.000\n",
      "            36          -0.10687        1.000\n",
      "            37          -0.10469        1.000\n",
      "            38          -0.10261        1.000\n",
      "            39          -0.10060        1.000\n",
      "            40          -0.09868        1.000\n",
      "            41          -0.09682        1.000\n",
      "            42          -0.09504        1.000\n",
      "            43          -0.09332        1.000\n",
      "            44          -0.09166        1.000\n",
      "            45          -0.09006        1.000\n",
      "            46          -0.08852        1.000\n",
      "            47          -0.08703        1.000\n",
      "            48          -0.08559        1.000\n",
      "            49          -0.08419        1.000\n",
      "            50          -0.08284        1.000\n",
      "            51          -0.08153        1.000\n",
      "            52          -0.08027        1.000\n",
      "            53          -0.07904        1.000\n",
      "            54          -0.07785        1.000\n",
      "            55          -0.07670        1.000\n",
      "            56          -0.07557        1.000\n",
      "            57          -0.07449        1.000\n",
      "            58          -0.07343        1.000\n",
      "            59          -0.07240        1.000\n",
      "            60          -0.07140        1.000\n",
      "            61          -0.07043        1.000\n",
      "            62          -0.06948        1.000\n",
      "            63          -0.06856        1.000\n",
      "            64          -0.06766        1.000\n",
      "            65          -0.06679        1.000\n",
      "            66          -0.06594        1.000\n",
      "            67          -0.06511        1.000\n",
      "            68          -0.06430        1.000\n",
      "            69          -0.06351        1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            70          -0.06274        1.000\n",
      "            71          -0.06198        1.000\n",
      "            72          -0.06125        1.000\n",
      "            73          -0.06053        1.000\n",
      "            74          -0.05983        1.000\n",
      "            75          -0.05915        1.000\n",
      "            76          -0.05848        1.000\n",
      "            77          -0.05782        1.000\n",
      "            78          -0.05718        1.000\n",
      "            79          -0.05656        1.000\n",
      "            80          -0.05595        1.000\n",
      "            81          -0.05535        1.000\n",
      "            82          -0.05476        1.000\n",
      "            83          -0.05418        1.000\n",
      "            84          -0.05362        1.000\n",
      "            85          -0.05307        1.000\n",
      "            86          -0.05253        1.000\n",
      "            87          -0.05200        1.000\n",
      "            88          -0.05148        1.000\n",
      "            89          -0.05098        1.000\n",
      "            90          -0.05048        1.000\n",
      "            91          -0.04999        1.000\n",
      "            92          -0.04951        1.000\n",
      "            93          -0.04904        1.000\n",
      "            94          -0.04858        1.000\n",
      "            95          -0.04812        1.000\n",
      "            96          -0.04768        1.000\n",
      "            97          -0.04724        1.000\n",
      "            98          -0.04681        1.000\n",
      "            99          -0.04639        1.000\n",
      "         Final          -0.04598        1.000\n",
      "\n",
      "The accuracy is 75.38%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.277\n",
      "             2          -0.45482        0.723\n",
      "             3          -0.40698        0.773\n",
      "             4          -0.37054        0.819\n",
      "             5          -0.34195        0.854\n",
      "             6          -0.31872        0.873\n",
      "             7          -0.29931        0.892\n",
      "             8          -0.28269        0.900\n",
      "             9          -0.26822        0.908\n",
      "            10          -0.25545        0.919\n",
      "            11          -0.24404        0.919\n",
      "            12          -0.23376        0.935\n",
      "            13          -0.22444        0.938\n",
      "            14          -0.21592        0.942\n",
      "            15          -0.20811        0.950\n",
      "            16          -0.20090        0.950\n",
      "            17          -0.19423        0.954\n",
      "            18          -0.18803        0.954\n",
      "            19          -0.18224        0.962\n",
      "            20          -0.17684        0.981\n",
      "            21          -0.17177        0.985\n",
      "            22          -0.16701        0.988\n",
      "            23          -0.16253        0.988\n",
      "            24          -0.15830        0.988\n",
      "            25          -0.15430        0.988\n",
      "            26          -0.15052        0.988\n",
      "            27          -0.14692        0.992\n",
      "            28          -0.14351        0.992\n",
      "            29          -0.14026        0.992\n",
      "            30          -0.13717        0.992\n",
      "            31          -0.13422        0.992\n",
      "            32          -0.13140        0.992\n",
      "            33          -0.12870        0.992\n",
      "            34          -0.12612        0.992\n",
      "            35          -0.12365        0.992\n",
      "            36          -0.12128        0.992\n",
      "            37          -0.11900        0.992\n",
      "            38          -0.11681        0.992\n",
      "            39          -0.11471        0.992\n",
      "            40          -0.11268        0.992\n",
      "            41          -0.11073        0.992\n",
      "            42          -0.10885        0.992\n",
      "            43          -0.10704        0.992\n",
      "            44          -0.10529        0.992\n",
      "            45          -0.10360        0.992\n",
      "            46          -0.10196        0.992\n",
      "            47          -0.10038        0.996\n",
      "            48          -0.09885        0.996\n",
      "            49          -0.09737        0.996\n",
      "            50          -0.09594        0.996\n",
      "            51          -0.09455        0.996\n",
      "            52          -0.09320        0.996\n",
      "            53          -0.09189        0.996\n",
      "            54          -0.09062        0.996\n",
      "            55          -0.08939        0.996\n",
      "            56          -0.08819        0.996\n",
      "            57          -0.08702        0.996\n",
      "            58          -0.08589        0.996\n",
      "            59          -0.08479        0.996\n",
      "            60          -0.08372        0.996\n",
      "            61          -0.08267        0.996\n",
      "            62          -0.08165        0.996\n",
      "            63          -0.08066        0.996\n",
      "            64          -0.07970        0.996\n",
      "            65          -0.07876        0.996\n",
      "            66          -0.07784        0.996\n",
      "            67          -0.07694        0.996\n",
      "            68          -0.07607        0.996\n",
      "            69          -0.07521        0.996\n",
      "            70          -0.07438        0.996\n",
      "            71          -0.07357        0.996\n",
      "            72          -0.07277        0.996\n",
      "            73          -0.07199        0.996\n",
      "            74          -0.07123        0.996\n",
      "            75          -0.07049        0.996\n",
      "            76          -0.06976        0.996\n",
      "            77          -0.06905        0.996\n",
      "            78          -0.06835        0.996\n",
      "            79          -0.06767        0.996\n",
      "            80          -0.06700        0.996\n",
      "            81          -0.06635        0.996\n",
      "            82          -0.06571        0.996\n",
      "            83          -0.06508        0.996\n",
      "            84          -0.06447        0.996\n",
      "            85          -0.06386        0.996\n",
      "            86          -0.06327        0.996\n",
      "            87          -0.06269        0.996\n",
      "            88          -0.06212        0.996\n",
      "            89          -0.06157        0.996\n",
      "            90          -0.06102        0.996\n",
      "            91          -0.06048        0.996\n",
      "            92          -0.05995        0.996\n",
      "            93          -0.05944        0.996\n",
      "            94          -0.05893        0.996\n",
      "            95          -0.05843        0.996\n",
      "            96          -0.05794        0.996\n",
      "            97          -0.05746        0.996\n",
      "            98          -0.05698        0.996\n",
      "            99          -0.05652        0.996\n",
      "         Final          -0.05606        0.996\n",
      "\n",
      "The accuracy is 81.54%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.208\n",
      "             2          -0.36067        0.792\n",
      "             3          -0.32257        0.804\n",
      "             4          -0.29218        0.838\n",
      "             5          -0.26798        0.881\n",
      "             6          -0.24831        0.892\n",
      "             7          -0.23193        0.927\n",
      "             8          -0.21800        0.931\n",
      "             9          -0.20595        0.931\n",
      "            10          -0.19539        0.942\n",
      "            11          -0.18604        0.954\n",
      "            12          -0.17766        0.962\n",
      "            13          -0.17011        0.969\n",
      "            14          -0.16325        0.969\n",
      "            15          -0.15699        0.969\n",
      "            16          -0.15124        0.977\n",
      "            17          -0.14595        0.977\n",
      "            18          -0.14104        0.977\n",
      "            19          -0.13649        0.977\n",
      "            20          -0.13225        0.977\n",
      "            21          -0.12828        0.977\n",
      "            22          -0.12457        0.977\n",
      "            23          -0.12108        0.977\n",
      "            24          -0.11780        0.985\n",
      "            25          -0.11470        0.988\n",
      "            26          -0.11178        0.988\n",
      "            27          -0.10901        0.988\n",
      "            28          -0.10638        0.988\n",
      "            29          -0.10389        0.988\n",
      "            30          -0.10152        0.988\n",
      "            31          -0.09926        0.992\n",
      "            32          -0.09711        0.992\n",
      "            33          -0.09505        0.992\n",
      "            34          -0.09309        0.992\n",
      "            35          -0.09121        0.992\n",
      "            36          -0.08941        0.992\n",
      "            37          -0.08768        0.992\n",
      "            38          -0.08603        0.992\n",
      "            39          -0.08444        0.992\n",
      "            40          -0.08291        0.992\n",
      "            41          -0.08144        0.992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            42          -0.08002        0.992\n",
      "            43          -0.07866        0.992\n",
      "            44          -0.07734        0.992\n",
      "            45          -0.07607        0.992\n",
      "            46          -0.07484        0.992\n",
      "            47          -0.07366        0.996\n",
      "            48          -0.07251        0.996\n",
      "            49          -0.07140        0.996\n",
      "            50          -0.07033        0.996\n",
      "            51          -0.06929        0.996\n",
      "            52          -0.06828        0.996\n",
      "            53          -0.06730        0.996\n",
      "            54          -0.06636        0.996\n",
      "            55          -0.06544        0.996\n",
      "            56          -0.06454        0.996\n",
      "            57          -0.06368        0.996\n",
      "            58          -0.06283        0.996\n",
      "            59          -0.06201        0.996\n",
      "            60          -0.06122        0.996\n",
      "            61          -0.06044        0.996\n",
      "            62          -0.05968        0.996\n",
      "            63          -0.05895        0.996\n",
      "            64          -0.05823        0.996\n",
      "            65          -0.05753        0.996\n",
      "            66          -0.05685        0.996\n",
      "            67          -0.05619        0.996\n",
      "            68          -0.05554        0.996\n",
      "            69          -0.05491        0.996\n",
      "            70          -0.05429        0.996\n",
      "            71          -0.05369        0.996\n",
      "            72          -0.05310        0.996\n",
      "            73          -0.05252        0.996\n",
      "            74          -0.05196        0.996\n",
      "            75          -0.05141        0.996\n",
      "            76          -0.05087        0.996\n",
      "            77          -0.05035        0.996\n",
      "            78          -0.04983        0.996\n",
      "            79          -0.04933        0.996\n",
      "            80          -0.04884        0.996\n",
      "            81          -0.04836        0.996\n",
      "            82          -0.04788        0.996\n",
      "            83          -0.04742        0.996\n",
      "            84          -0.04697        0.996\n",
      "            85          -0.04652        0.996\n",
      "            86          -0.04609        0.996\n",
      "            87          -0.04566        0.996\n",
      "            88          -0.04524        0.996\n",
      "            89          -0.04483        0.996\n",
      "            90          -0.04443        0.996\n",
      "            91          -0.04403        0.996\n",
      "            92          -0.04364        0.996\n",
      "            93          -0.04326        0.996\n",
      "            94          -0.04289        0.996\n",
      "            95          -0.04252        0.996\n",
      "            96          -0.04216        0.996\n",
      "            97          -0.04180        0.996\n",
      "            98          -0.04145        0.996\n",
      "            99          -0.04111        0.996\n",
      "         Final          -0.04077        0.996\n",
      "\n",
      "The accuracy is 63.08%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.169\n",
      "             2          -0.30386        0.831\n",
      "             3          -0.27385        0.831\n",
      "             4          -0.24970        0.869\n",
      "             5          -0.23067        0.885\n",
      "             6          -0.21529        0.896\n",
      "             7          -0.20247        0.919\n",
      "             8          -0.19153        0.923\n",
      "             9          -0.18199        0.931\n",
      "            10          -0.17356        0.935\n",
      "            11          -0.16602        0.942\n",
      "            12          -0.15921        0.950\n",
      "            13          -0.15301        0.954\n",
      "            14          -0.14734        0.958\n",
      "            15          -0.14212        0.965\n",
      "            16          -0.13729        0.969\n",
      "            17          -0.13281        0.969\n",
      "            18          -0.12864        0.973\n",
      "            19          -0.12474        0.973\n",
      "            20          -0.12109        0.977\n",
      "            21          -0.11766        0.981\n",
      "            22          -0.11444        0.988\n",
      "            23          -0.11140        0.988\n",
      "            24          -0.10852        0.988\n",
      "            25          -0.10580        0.988\n",
      "            26          -0.10322        0.988\n",
      "            27          -0.10077        0.988\n",
      "            28          -0.09844        0.988\n",
      "            29          -0.09623        0.992\n",
      "            30          -0.09411        0.992\n",
      "            31          -0.09209        0.992\n",
      "            32          -0.09016        0.992\n",
      "            33          -0.08831        0.992\n",
      "            34          -0.08654        0.992\n",
      "            35          -0.08484        0.992\n",
      "            36          -0.08321        0.992\n",
      "            37          -0.08164        0.992\n",
      "            38          -0.08014        0.996\n",
      "            39          -0.07869        0.996\n",
      "            40          -0.07729        0.996\n",
      "            41          -0.07595        0.996\n",
      "            42          -0.07465        1.000\n",
      "            43          -0.07339        1.000\n",
      "            44          -0.07218        1.000\n",
      "            45          -0.07101        1.000\n",
      "            46          -0.06988        1.000\n",
      "            47          -0.06878        1.000\n",
      "            48          -0.06772        1.000\n",
      "            49          -0.06669        1.000\n",
      "            50          -0.06570        1.000\n",
      "            51          -0.06473        1.000\n",
      "            52          -0.06379        1.000\n",
      "            53          -0.06288        1.000\n",
      "            54          -0.06200        1.000\n",
      "            55          -0.06114        1.000\n",
      "            56          -0.06030        1.000\n",
      "            57          -0.05949        1.000\n",
      "            58          -0.05870        1.000\n",
      "            59          -0.05793        1.000\n",
      "            60          -0.05718        1.000\n",
      "            61          -0.05645        1.000\n",
      "            62          -0.05573        1.000\n",
      "            63          -0.05504        1.000\n",
      "            64          -0.05436        1.000\n",
      "            65          -0.05370        1.000\n",
      "            66          -0.05306        1.000\n",
      "            67          -0.05243        1.000\n",
      "            68          -0.05182        1.000\n",
      "            69          -0.05122        1.000\n",
      "            70          -0.05063        1.000\n",
      "            71          -0.05006        1.000\n",
      "            72          -0.04950        1.000\n",
      "            73          -0.04895        1.000\n",
      "            74          -0.04842        1.000\n",
      "            75          -0.04789        1.000\n",
      "            76          -0.04738        1.000\n",
      "            77          -0.04688        1.000\n",
      "            78          -0.04639        1.000\n",
      "            79          -0.04591        1.000\n",
      "            80          -0.04543        1.000\n",
      "            81          -0.04497        1.000\n",
      "            82          -0.04452        1.000\n",
      "            83          -0.04408        1.000\n",
      "            84          -0.04364        1.000\n",
      "            85          -0.04322        1.000\n",
      "            86          -0.04280        1.000\n",
      "            87          -0.04239        1.000\n",
      "            88          -0.04199        1.000\n",
      "            89          -0.04159        1.000\n",
      "            90          -0.04121        1.000\n",
      "            91          -0.04083        1.000\n",
      "            92          -0.04045        1.000\n",
      "            93          -0.04009        1.000\n",
      "            94          -0.03973        1.000\n",
      "            95          -0.03937        1.000\n",
      "            96          -0.03903        1.000\n",
      "            97          -0.03869        1.000\n",
      "            98          -0.03835        1.000\n",
      "            99          -0.03802        1.000\n",
      "         Final          -0.03770        1.000\n",
      "\n",
      "The accuracy is 52.31%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.258\n",
      "             2          -0.43513        0.742\n",
      "             3          -0.39156        0.777\n",
      "             4          -0.35750        0.819\n",
      "             5          -0.33023        0.842\n",
      "             6          -0.30780        0.877\n",
      "             7          -0.28887        0.896\n",
      "             8          -0.27259        0.923\n",
      "             9          -0.25836        0.927\n",
      "            10          -0.24576        0.931\n",
      "            11          -0.23451        0.938\n",
      "            12          -0.22437        0.938\n",
      "            13          -0.21517        0.950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            14          -0.20678        0.954\n",
      "            15          -0.19908        0.958\n",
      "            16          -0.19198        0.962\n",
      "            17          -0.18543        0.973\n",
      "            18          -0.17934        0.981\n",
      "            19          -0.17368        0.981\n",
      "            20          -0.16839        0.981\n",
      "            21          -0.16344        0.981\n",
      "            22          -0.15879        0.981\n",
      "            23          -0.15443        0.981\n",
      "            24          -0.15031        0.981\n",
      "            25          -0.14643        0.981\n",
      "            26          -0.14275        0.985\n",
      "            27          -0.13927        0.988\n",
      "            28          -0.13597        0.988\n",
      "            29          -0.13283        0.988\n",
      "            30          -0.12985        0.988\n",
      "            31          -0.12700        0.988\n",
      "            32          -0.12429        0.988\n",
      "            33          -0.12170        0.988\n",
      "            34          -0.11922        0.988\n",
      "            35          -0.11685        0.988\n",
      "            36          -0.11458        0.988\n",
      "            37          -0.11240        0.988\n",
      "            38          -0.11030        0.992\n",
      "            39          -0.10829        0.992\n",
      "            40          -0.10636        0.992\n",
      "            41          -0.10450        0.992\n",
      "            42          -0.10271        0.992\n",
      "            43          -0.10098        0.992\n",
      "            44          -0.09932        0.992\n",
      "            45          -0.09771        0.992\n",
      "            46          -0.09615        0.992\n",
      "            47          -0.09465        0.992\n",
      "            48          -0.09320        0.992\n",
      "            49          -0.09180        0.992\n",
      "            50          -0.09044        0.992\n",
      "            51          -0.08912        0.992\n",
      "            52          -0.08784        0.992\n",
      "            53          -0.08661        0.996\n",
      "            54          -0.08541        0.996\n",
      "            55          -0.08424        0.996\n",
      "            56          -0.08311        0.996\n",
      "            57          -0.08201        0.996\n",
      "            58          -0.08094        0.996\n",
      "            59          -0.07990        0.996\n",
      "            60          -0.07889        0.996\n",
      "            61          -0.07790        0.996\n",
      "            62          -0.07694        0.996\n",
      "            63          -0.07601        0.996\n",
      "            64          -0.07510        0.996\n",
      "            65          -0.07422        0.996\n",
      "            66          -0.07335        0.996\n",
      "            67          -0.07251        0.996\n",
      "            68          -0.07169        0.996\n",
      "            69          -0.07088        0.996\n",
      "            70          -0.07010        0.996\n",
      "            71          -0.06933        0.996\n",
      "            72          -0.06859        0.996\n",
      "            73          -0.06786        0.996\n",
      "            74          -0.06714        0.996\n",
      "            75          -0.06644        0.996\n",
      "            76          -0.06576        0.996\n",
      "            77          -0.06509        0.996\n",
      "            78          -0.06444        0.996\n",
      "            79          -0.06380        0.996\n",
      "            80          -0.06317        0.996\n",
      "            81          -0.06256        0.996\n",
      "            82          -0.06196        0.996\n",
      "            83          -0.06137        0.996\n",
      "            84          -0.06080        0.996\n",
      "            85          -0.06023        0.996\n",
      "            86          -0.05968        0.996\n",
      "            87          -0.05913        0.996\n",
      "            88          -0.05860        0.996\n",
      "            89          -0.05808        0.996\n",
      "            90          -0.05757        0.996\n",
      "            91          -0.05706        0.996\n",
      "            92          -0.05657        0.996\n",
      "            93          -0.05608        0.996\n",
      "            94          -0.05561        0.996\n",
      "            95          -0.05514        0.996\n",
      "            96          -0.05468        0.996\n",
      "            97          -0.05423        0.996\n",
      "            98          -0.05379        0.996\n",
      "            99          -0.05335        0.996\n",
      "         Final          -0.05292        0.996\n",
      "\n",
      "The accuracy is 80.00%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.258\n",
      "             2          -0.43561        0.742\n",
      "             3          -0.39134        0.773\n",
      "             4          -0.35640        0.808\n",
      "             5          -0.32832        0.850\n",
      "             6          -0.30520        0.885\n",
      "             7          -0.28571        0.892\n",
      "             8          -0.26896        0.900\n",
      "             9          -0.25436        0.912\n",
      "            10          -0.24147        0.931\n",
      "            11          -0.22996        0.946\n",
      "            12          -0.21961        0.965\n",
      "            13          -0.21024        0.969\n",
      "            14          -0.20169        0.969\n",
      "            15          -0.19386        0.973\n",
      "            16          -0.18666        0.973\n",
      "            17          -0.18000        0.973\n",
      "            18          -0.17383        0.977\n",
      "            19          -0.16809        0.977\n",
      "            20          -0.16273        0.985\n",
      "            21          -0.15772        0.985\n",
      "            22          -0.15302        0.985\n",
      "            23          -0.14860        0.985\n",
      "            24          -0.14444        0.985\n",
      "            25          -0.14052        0.988\n",
      "            26          -0.13681        0.992\n",
      "            27          -0.13329        0.992\n",
      "            28          -0.12995        0.992\n",
      "            29          -0.12679        0.992\n",
      "            30          -0.12377        0.996\n",
      "            31          -0.12090        0.996\n",
      "            32          -0.11816        0.996\n",
      "            33          -0.11555        1.000\n",
      "            34          -0.11305        1.000\n",
      "            35          -0.11066        1.000\n",
      "            36          -0.10837        1.000\n",
      "            37          -0.10617        1.000\n",
      "            38          -0.10406        1.000\n",
      "            39          -0.10204        1.000\n",
      "            40          -0.10009        1.000\n",
      "            41          -0.09822        1.000\n",
      "            42          -0.09642        1.000\n",
      "            43          -0.09468        1.000\n",
      "            44          -0.09300        1.000\n",
      "            45          -0.09139        1.000\n",
      "            46          -0.08982        1.000\n",
      "            47          -0.08832        1.000\n",
      "            48          -0.08686        1.000\n",
      "            49          -0.08545        1.000\n",
      "            50          -0.08408        1.000\n",
      "            51          -0.08276        1.000\n",
      "            52          -0.08148        1.000\n",
      "            53          -0.08024        1.000\n",
      "            54          -0.07903        1.000\n",
      "            55          -0.07786        1.000\n",
      "            56          -0.07673        1.000\n",
      "            57          -0.07562        1.000\n",
      "            58          -0.07455        1.000\n",
      "            59          -0.07351        1.000\n",
      "            60          -0.07250        1.000\n",
      "            61          -0.07152        1.000\n",
      "            62          -0.07056        1.000\n",
      "            63          -0.06963        1.000\n",
      "            64          -0.06872        1.000\n",
      "            65          -0.06783        1.000\n",
      "            66          -0.06697        1.000\n",
      "            67          -0.06613        1.000\n",
      "            68          -0.06531        1.000\n",
      "            69          -0.06451        1.000\n",
      "            70          -0.06373        1.000\n",
      "            71          -0.06296        1.000\n",
      "            72          -0.06222        1.000\n",
      "            73          -0.06149        1.000\n",
      "            74          -0.06078        1.000\n",
      "            75          -0.06009        1.000\n",
      "            76          -0.05941        1.000\n",
      "            77          -0.05875        1.000\n",
      "            78          -0.05810        1.000\n",
      "            79          -0.05747        1.000\n",
      "            80          -0.05684        1.000\n",
      "            81          -0.05624        1.000\n",
      "            82          -0.05564        1.000\n",
      "            83          -0.05506        1.000\n",
      "            84          -0.05449        1.000\n",
      "            85          -0.05393        1.000\n",
      "            86          -0.05338        1.000\n",
      "            87          -0.05285        1.000\n",
      "            88          -0.05232        1.000\n",
      "            89          -0.05181        1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            90          -0.05130        1.000\n",
      "            91          -0.05080        1.000\n",
      "            92          -0.05032        1.000\n",
      "            93          -0.04984        1.000\n",
      "            94          -0.04937        1.000\n",
      "            95          -0.04891        1.000\n",
      "            96          -0.04846        1.000\n",
      "            97          -0.04802        1.000\n",
      "            98          -0.04758        1.000\n",
      "            99          -0.04716        1.000\n",
      "         Final          -0.04674        1.000\n",
      "\n",
      "The accuracy is 75.38%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.277\n",
      "             2          -0.45550        0.723\n",
      "             3          -0.40819        0.773\n",
      "             4          -0.37215        0.815\n",
      "             5          -0.34388        0.854\n",
      "             6          -0.32093        0.881\n",
      "             7          -0.30174        0.892\n",
      "             8          -0.28531        0.900\n",
      "             9          -0.27099        0.904\n",
      "            10          -0.25833        0.912\n",
      "            11          -0.24701        0.923\n",
      "            12          -0.23680        0.927\n",
      "            13          -0.22753        0.931\n",
      "            14          -0.21905        0.942\n",
      "            15          -0.21126        0.950\n",
      "            16          -0.20406        0.950\n",
      "            17          -0.19740        0.954\n",
      "            18          -0.19120        0.954\n",
      "            19          -0.18542        0.962\n",
      "            20          -0.18001        0.969\n",
      "            21          -0.17494        0.977\n",
      "            22          -0.17017        0.981\n",
      "            23          -0.16568        0.981\n",
      "            24          -0.16144        0.985\n",
      "            25          -0.15743        0.985\n",
      "            26          -0.15363        0.985\n",
      "            27          -0.15002        0.988\n",
      "            28          -0.14660        0.988\n",
      "            29          -0.14333        0.988\n",
      "            30          -0.14023        0.988\n",
      "            31          -0.13726        0.988\n",
      "            32          -0.13443        0.988\n",
      "            33          -0.13172        0.988\n",
      "            34          -0.12912        0.988\n",
      "            35          -0.12663        0.988\n",
      "            36          -0.12425        0.988\n",
      "            37          -0.12196        0.988\n",
      "            38          -0.11976        0.988\n",
      "            39          -0.11764        0.988\n",
      "            40          -0.11560        0.988\n",
      "            41          -0.11364        0.988\n",
      "            42          -0.11174        0.988\n",
      "            43          -0.10992        0.988\n",
      "            44          -0.10815        0.988\n",
      "            45          -0.10645        0.988\n",
      "            46          -0.10480        0.988\n",
      "            47          -0.10321        0.992\n",
      "            48          -0.10167        0.992\n",
      "            49          -0.10017        0.992\n",
      "            50          -0.09873        0.992\n",
      "            51          -0.09732        0.996\n",
      "            52          -0.09596        0.996\n",
      "            53          -0.09464        0.996\n",
      "            54          -0.09336        0.996\n",
      "            55          -0.09212        0.996\n",
      "            56          -0.09091        0.996\n",
      "            57          -0.08973        0.996\n",
      "            58          -0.08859        0.996\n",
      "            59          -0.08747        0.996\n",
      "            60          -0.08639        0.996\n",
      "            61          -0.08533        0.996\n",
      "            62          -0.08431        0.996\n",
      "            63          -0.08331        0.996\n",
      "            64          -0.08233        0.996\n",
      "            65          -0.08138        0.996\n",
      "            66          -0.08045        0.996\n",
      "            67          -0.07954        0.996\n",
      "            68          -0.07866        0.996\n",
      "            69          -0.07779        0.996\n",
      "            70          -0.07695        0.996\n",
      "            71          -0.07613        0.996\n",
      "            72          -0.07532        0.996\n",
      "            73          -0.07453        0.996\n",
      "            74          -0.07376        0.996\n",
      "            75          -0.07301        0.996\n",
      "            76          -0.07227        0.996\n",
      "            77          -0.07155        0.996\n",
      "            78          -0.07085        0.996\n",
      "            79          -0.07016        0.996\n",
      "            80          -0.06948        0.996\n",
      "            81          -0.06882        0.996\n",
      "            82          -0.06817        0.996\n",
      "            83          -0.06753        0.996\n",
      "            84          -0.06691        0.996\n",
      "            85          -0.06630        0.996\n",
      "            86          -0.06570        0.996\n",
      "            87          -0.06511        0.996\n",
      "            88          -0.06453        0.996\n",
      "            89          -0.06397        0.996\n",
      "            90          -0.06341        0.996\n",
      "            91          -0.06287        0.996\n",
      "            92          -0.06233        0.996\n",
      "            93          -0.06181        0.996\n",
      "            94          -0.06129        0.996\n",
      "            95          -0.06078        0.996\n",
      "            96          -0.06028        0.996\n",
      "            97          -0.05979        0.996\n",
      "            98          -0.05931        0.996\n",
      "            99          -0.05884        0.996\n",
      "         Final          -0.05838        0.996\n",
      "\n",
      "The accuracy is 81.54%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.208\n",
      "             2          -0.36131        0.792\n",
      "             3          -0.32362        0.804\n",
      "             4          -0.29336        0.842\n",
      "             5          -0.26918        0.877\n",
      "             6          -0.24948        0.888\n",
      "             7          -0.23306        0.923\n",
      "             8          -0.21911        0.923\n",
      "             9          -0.20705        0.931\n",
      "            10          -0.19649        0.942\n",
      "            11          -0.18714        0.954\n",
      "            12          -0.17877        0.954\n",
      "            13          -0.17124        0.962\n",
      "            14          -0.16440        0.969\n",
      "            15          -0.15816        0.969\n",
      "            16          -0.15244        0.977\n",
      "            17          -0.14716        0.977\n",
      "            18          -0.14228        0.977\n",
      "            19          -0.13775        0.977\n",
      "            20          -0.13352        0.977\n",
      "            21          -0.12957        0.977\n",
      "            22          -0.12587        0.977\n",
      "            23          -0.12240        0.977\n",
      "            24          -0.11913        0.977\n",
      "            25          -0.11604        0.985\n",
      "            26          -0.11312        0.985\n",
      "            27          -0.11036        0.988\n",
      "            28          -0.10774        0.988\n",
      "            29          -0.10525        0.988\n",
      "            30          -0.10288        0.988\n",
      "            31          -0.10063        0.992\n",
      "            32          -0.09848        0.992\n",
      "            33          -0.09642        0.992\n",
      "            34          -0.09445        0.992\n",
      "            35          -0.09257        0.992\n",
      "            36          -0.09077        0.992\n",
      "            37          -0.08904        0.992\n",
      "            38          -0.08738        0.992\n",
      "            39          -0.08578        0.992\n",
      "            40          -0.08425        0.992\n",
      "            41          -0.08277        0.992\n",
      "            42          -0.08135        0.992\n",
      "            43          -0.07998        0.992\n",
      "            44          -0.07866        0.992\n",
      "            45          -0.07738        0.992\n",
      "            46          -0.07614        0.992\n",
      "            47          -0.07495        0.992\n",
      "            48          -0.07380        0.996\n",
      "            49          -0.07268        0.996\n",
      "            50          -0.07160        0.996\n",
      "            51          -0.07055        0.996\n",
      "            52          -0.06954        0.996\n",
      "            53          -0.06855        0.996\n",
      "            54          -0.06760        0.996\n",
      "            55          -0.06667        0.996\n",
      "            56          -0.06577        0.996\n",
      "            57          -0.06489        0.996\n",
      "            58          -0.06404        0.996\n",
      "            59          -0.06321        0.996\n",
      "            60          -0.06241        0.996\n",
      "            61          -0.06162        0.996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            62          -0.06086        0.996\n",
      "            63          -0.06012        0.996\n",
      "            64          -0.05939        0.996\n",
      "            65          -0.05868        0.996\n",
      "            66          -0.05799        0.996\n",
      "            67          -0.05732        0.996\n",
      "            68          -0.05667        0.996\n",
      "            69          -0.05603        0.996\n",
      "            70          -0.05540        0.996\n",
      "            71          -0.05479        0.996\n",
      "            72          -0.05419        0.996\n",
      "            73          -0.05361        0.996\n",
      "            74          -0.05304        0.996\n",
      "            75          -0.05248        0.996\n",
      "            76          -0.05194        0.996\n",
      "            77          -0.05141        0.996\n",
      "            78          -0.05088        0.996\n",
      "            79          -0.05037        0.996\n",
      "            80          -0.04987        0.996\n",
      "            81          -0.04938        0.996\n",
      "            82          -0.04890        0.996\n",
      "            83          -0.04843        0.996\n",
      "            84          -0.04797        0.996\n",
      "            85          -0.04752        0.996\n",
      "            86          -0.04708        0.996\n",
      "            87          -0.04665        0.996\n",
      "            88          -0.04622        0.996\n",
      "            89          -0.04580        0.996\n",
      "            90          -0.04539        0.996\n",
      "            91          -0.04499        0.996\n",
      "            92          -0.04460        0.996\n",
      "            93          -0.04421        0.996\n",
      "            94          -0.04383        0.996\n",
      "            95          -0.04345        0.996\n",
      "            96          -0.04309        0.996\n",
      "            97          -0.04273        0.996\n",
      "            98          -0.04237        0.996\n",
      "            99          -0.04202        0.996\n",
      "         Final          -0.04168        0.996\n",
      "\n",
      "The accuracy is 66.15%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.169\n",
      "             2          -0.30345        0.831\n",
      "             3          -0.27396        0.835\n",
      "             4          -0.25004        0.865\n",
      "             5          -0.23109        0.888\n",
      "             6          -0.21572        0.904\n",
      "             7          -0.20291        0.912\n",
      "             8          -0.19197        0.927\n",
      "             9          -0.18243        0.931\n",
      "            10          -0.17401        0.935\n",
      "            11          -0.16646        0.938\n",
      "            12          -0.15965        0.946\n",
      "            13          -0.15345        0.954\n",
      "            14          -0.14777        0.958\n",
      "            15          -0.14254        0.958\n",
      "            16          -0.13770        0.958\n",
      "            17          -0.13321        0.962\n",
      "            18          -0.12903        0.965\n",
      "            19          -0.12512        0.965\n",
      "            20          -0.12146        0.969\n",
      "            21          -0.11802        0.977\n",
      "            22          -0.11478        0.988\n",
      "            23          -0.11173        0.988\n",
      "            24          -0.10885        0.988\n",
      "            25          -0.10612        0.992\n",
      "            26          -0.10352        0.992\n",
      "            27          -0.10106        0.996\n",
      "            28          -0.09872        0.996\n",
      "            29          -0.09649        0.996\n",
      "            30          -0.09437        0.996\n",
      "            31          -0.09234        0.996\n",
      "            32          -0.09040        0.996\n",
      "            33          -0.08854        0.996\n",
      "            34          -0.08676        0.996\n",
      "            35          -0.08505        0.996\n",
      "            36          -0.08341        0.996\n",
      "            37          -0.08184        0.996\n",
      "            38          -0.08032        0.996\n",
      "            39          -0.07887        0.996\n",
      "            40          -0.07746        0.996\n",
      "            41          -0.07611        0.996\n",
      "            42          -0.07480        1.000\n",
      "            43          -0.07354        1.000\n",
      "            44          -0.07233        1.000\n",
      "            45          -0.07115        1.000\n",
      "            46          -0.07001        1.000\n",
      "            47          -0.06891        1.000\n",
      "            48          -0.06784        1.000\n",
      "            49          -0.06681        1.000\n",
      "            50          -0.06581        1.000\n",
      "            51          -0.06484        1.000\n",
      "            52          -0.06389        1.000\n",
      "            53          -0.06298        1.000\n",
      "            54          -0.06209        1.000\n",
      "            55          -0.06122        1.000\n",
      "            56          -0.06038        1.000\n",
      "            57          -0.05957        1.000\n",
      "            58          -0.05877        1.000\n",
      "            59          -0.05800        1.000\n",
      "            60          -0.05725        1.000\n",
      "            61          -0.05651        1.000\n",
      "            62          -0.05580        1.000\n",
      "            63          -0.05510        1.000\n",
      "            64          -0.05442        1.000\n",
      "            65          -0.05376        1.000\n",
      "            66          -0.05311        1.000\n",
      "            67          -0.05248        1.000\n",
      "            68          -0.05186        1.000\n",
      "            69          -0.05126        1.000\n",
      "            70          -0.05067        1.000\n",
      "            71          -0.05010        1.000\n",
      "            72          -0.04954        1.000\n",
      "            73          -0.04899        1.000\n",
      "            74          -0.04845        1.000\n",
      "            75          -0.04793        1.000\n",
      "            76          -0.04741        1.000\n",
      "            77          -0.04691        1.000\n",
      "            78          -0.04642        1.000\n",
      "            79          -0.04593        1.000\n",
      "            80          -0.04546        1.000\n",
      "            81          -0.04500        1.000\n",
      "            82          -0.04454        1.000\n",
      "            83          -0.04410        1.000\n",
      "            84          -0.04367        1.000\n",
      "            85          -0.04324        1.000\n",
      "            86          -0.04282        1.000\n",
      "            87          -0.04241        1.000\n",
      "            88          -0.04201        1.000\n",
      "            89          -0.04161        1.000\n",
      "            90          -0.04122        1.000\n",
      "            91          -0.04084        1.000\n",
      "            92          -0.04047        1.000\n",
      "            93          -0.04010        1.000\n",
      "            94          -0.03974        1.000\n",
      "            95          -0.03939        1.000\n",
      "            96          -0.03904        1.000\n",
      "            97          -0.03870        1.000\n",
      "            98          -0.03836        1.000\n",
      "            99          -0.03803        1.000\n",
      "         Final          -0.03771        1.000\n",
      "\n",
      "The accuracy is 50.77%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.258\n",
      "             2          -0.43596        0.742\n",
      "             3          -0.39252        0.765\n",
      "             4          -0.35830        0.815\n",
      "             5          -0.33081        0.842\n",
      "             6          -0.30816        0.885\n",
      "             7          -0.28904        0.892\n",
      "             8          -0.27261        0.919\n",
      "             9          -0.25826        0.931\n",
      "            10          -0.24558        0.931\n",
      "            11          -0.23425        0.931\n",
      "            12          -0.22406        0.938\n",
      "            13          -0.21481        0.954\n",
      "            14          -0.20638        0.954\n",
      "            15          -0.19866        0.965\n",
      "            16          -0.19154        0.969\n",
      "            17          -0.18497        0.977\n",
      "            18          -0.17886        0.977\n",
      "            19          -0.17319        0.985\n",
      "            20          -0.16789        0.985\n",
      "            21          -0.16293        0.985\n",
      "            22          -0.15827        0.985\n",
      "            23          -0.15390        0.985\n",
      "            24          -0.14977        0.985\n",
      "            25          -0.14588        0.985\n",
      "            26          -0.14220        0.985\n",
      "            27          -0.13872        0.988\n",
      "            28          -0.13541        0.988\n",
      "            29          -0.13227        0.988\n",
      "            30          -0.12927        0.988\n",
      "            31          -0.12643        0.988\n",
      "            32          -0.12371        0.988\n",
      "            33          -0.12111        0.988\n",
      "            34          -0.11863        0.988\n",
      "            35          -0.11625        0.988\n",
      "            36          -0.11398        0.988\n",
      "            37          -0.11179        0.988\n",
      "            38          -0.10970        0.988\n",
      "            39          -0.10768        0.992\n",
      "            40          -0.10575        0.992\n",
      "            41          -0.10389        0.992\n",
      "            42          -0.10209        0.992\n",
      "            43          -0.10036        0.992\n",
      "            44          -0.09869        0.992\n",
      "            45          -0.09709        0.992\n",
      "            46          -0.09553        0.992\n",
      "            47          -0.09403        0.992\n",
      "            48          -0.09258        0.992\n",
      "            49          -0.09117        0.992\n",
      "            50          -0.08981        0.992\n",
      "            51          -0.08849        0.992\n",
      "            52          -0.08722        0.992\n",
      "            53          -0.08598        0.996\n",
      "            54          -0.08478        0.996\n",
      "            55          -0.08361        0.996\n",
      "            56          -0.08248        0.996\n",
      "            57          -0.08138        0.996\n",
      "            58          -0.08031        0.996\n",
      "            59          -0.07927        0.996\n",
      "            60          -0.07826        0.996\n",
      "            61          -0.07728        0.996\n",
      "            62          -0.07632        0.996\n",
      "            63          -0.07539        0.996\n",
      "            64          -0.07448        0.996\n",
      "            65          -0.07359        0.996\n",
      "            66          -0.07273        0.996\n",
      "            67          -0.07189        0.996\n",
      "            68          -0.07107        0.996\n",
      "            69          -0.07026        0.996\n",
      "            70          -0.06948        0.996\n",
      "            71          -0.06872        0.996\n",
      "            72          -0.06797        0.996\n",
      "            73          -0.06724        0.996\n",
      "            74          -0.06653        0.996\n",
      "            75          -0.06584        0.996\n",
      "            76          -0.06515        0.996\n",
      "            77          -0.06449        0.996\n",
      "            78          -0.06384        0.996\n",
      "            79          -0.06320        0.996\n",
      "            80          -0.06258        0.996\n",
      "            81          -0.06196        0.996\n",
      "            82          -0.06137        0.996\n",
      "            83          -0.06078        0.996\n",
      "            84          -0.06021        0.996\n",
      "            85          -0.05964        0.996\n",
      "            86          -0.05909        0.996\n",
      "            87          -0.05855        0.996\n",
      "            88          -0.05802        0.996\n",
      "            89          -0.05750        0.996\n",
      "            90          -0.05699        0.996\n",
      "            91          -0.05649        0.996\n",
      "            92          -0.05600        0.996\n",
      "            93          -0.05552        0.996\n",
      "            94          -0.05504        0.996\n",
      "            95          -0.05458        0.996\n",
      "            96          -0.05412        0.996\n",
      "            97          -0.05367        0.996\n",
      "            98          -0.05323        0.996\n",
      "            99          -0.05280        0.996\n",
      "         Final          -0.05237        0.996\n",
      "\n",
      "The accuracy is 83.08%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.258\n",
      "             2          -0.43709        0.742\n",
      "             3          -0.39336        0.769\n",
      "             4          -0.35861        0.808\n",
      "             5          -0.33056        0.846\n",
      "             6          -0.30741        0.877\n",
      "             7          -0.28787        0.885\n",
      "             8          -0.27109        0.908\n",
      "             9          -0.25646        0.915\n",
      "            10          -0.24354        0.919\n",
      "            11          -0.23202        0.935\n",
      "            12          -0.22165        0.946\n",
      "            13          -0.21227        0.954\n",
      "            14          -0.20371        0.965\n",
      "            15          -0.19588        0.965\n",
      "            16          -0.18866        0.965\n",
      "            17          -0.18200        0.965\n",
      "            18          -0.17581        0.969\n",
      "            19          -0.17006        0.969\n",
      "            20          -0.16469        0.969\n",
      "            21          -0.15966        0.977\n",
      "            22          -0.15494        0.981\n",
      "            23          -0.15051        0.985\n",
      "            24          -0.14633        0.985\n",
      "            25          -0.14238        0.985\n",
      "            26          -0.13865        0.988\n",
      "            27          -0.13511        0.988\n",
      "            28          -0.13175        0.988\n",
      "            29          -0.12856        1.000\n",
      "            30          -0.12553        1.000\n",
      "            31          -0.12264        1.000\n",
      "            32          -0.11988        1.000\n",
      "            33          -0.11724        1.000\n",
      "            34          -0.11472        1.000\n",
      "            35          -0.11231        1.000\n",
      "            36          -0.10999        1.000\n",
      "            37          -0.10778        1.000\n",
      "            38          -0.10565        1.000\n",
      "            39          -0.10360        1.000\n",
      "            40          -0.10163        1.000\n",
      "            41          -0.09974        1.000\n",
      "            42          -0.09792        1.000\n",
      "            43          -0.09616        1.000\n",
      "            44          -0.09446        1.000\n",
      "            45          -0.09283        1.000\n",
      "            46          -0.09125        1.000\n",
      "            47          -0.08972        1.000\n",
      "            48          -0.08825        1.000\n",
      "            49          -0.08682        1.000\n",
      "            50          -0.08543        1.000\n",
      "            51          -0.08409        1.000\n",
      "            52          -0.08280        1.000\n",
      "            53          -0.08154        1.000\n",
      "            54          -0.08032        1.000\n",
      "            55          -0.07913        1.000\n",
      "            56          -0.07798        1.000\n",
      "            57          -0.07687        1.000\n",
      "            58          -0.07578        1.000\n",
      "            59          -0.07472        1.000\n",
      "            60          -0.07370        1.000\n",
      "            61          -0.07270        1.000\n",
      "            62          -0.07173        1.000\n",
      "            63          -0.07078        1.000\n",
      "            64          -0.06986        1.000\n",
      "            65          -0.06896        1.000\n",
      "            66          -0.06809        1.000\n",
      "            67          -0.06723        1.000\n",
      "            68          -0.06640        1.000\n",
      "            69          -0.06559        1.000\n",
      "            70          -0.06480        1.000\n",
      "            71          -0.06402        1.000\n",
      "            72          -0.06327        1.000\n",
      "            73          -0.06253        1.000\n",
      "            74          -0.06181        1.000\n",
      "            75          -0.06110        1.000\n",
      "            76          -0.06041        1.000\n",
      "            77          -0.05974        1.000\n",
      "            78          -0.05908        1.000\n",
      "            79          -0.05844        1.000\n",
      "            80          -0.05781        1.000\n",
      "            81          -0.05719        1.000\n",
      "            82          -0.05659        1.000\n",
      "            83          -0.05600        1.000\n",
      "            84          -0.05542        1.000\n",
      "            85          -0.05485        1.000\n",
      "            86          -0.05429        1.000\n",
      "            87          -0.05375        1.000\n",
      "            88          -0.05321        1.000\n",
      "            89          -0.05269        1.000\n",
      "            90          -0.05218        1.000\n",
      "            91          -0.05167        1.000\n",
      "            92          -0.05118        1.000\n",
      "            93          -0.05069        1.000\n",
      "            94          -0.05022        1.000\n",
      "            95          -0.04975        1.000\n",
      "            96          -0.04929        1.000\n",
      "            97          -0.04884        1.000\n",
      "            98          -0.04840        1.000\n",
      "            99          -0.04797        1.000\n",
      "         Final          -0.04754        1.000\n",
      "\n",
      "The accuracy is 73.85%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.277\n",
      "             2          -0.45752        0.723\n",
      "             3          -0.41087        0.769\n",
      "             4          -0.37476        0.815\n",
      "             5          -0.34614        0.846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             6          -0.32278        0.869\n",
      "             7          -0.30321        0.892\n",
      "             8          -0.28644        0.904\n",
      "             9          -0.27184        0.912\n",
      "            10          -0.25895        0.915\n",
      "            11          -0.24745        0.919\n",
      "            12          -0.23709        0.927\n",
      "            13          -0.22769        0.931\n",
      "            14          -0.21911        0.931\n",
      "            15          -0.21123        0.942\n",
      "            16          -0.20397        0.950\n",
      "            17          -0.19724        0.950\n",
      "            18          -0.19100        0.958\n",
      "            19          -0.18517        0.958\n",
      "            20          -0.17973        0.958\n",
      "            21          -0.17462        0.973\n",
      "            22          -0.16982        0.981\n",
      "            23          -0.16530        0.981\n",
      "            24          -0.16104        0.981\n",
      "            25          -0.15700        0.985\n",
      "            26          -0.15318        0.985\n",
      "            27          -0.14956        0.988\n",
      "            28          -0.14611        0.988\n",
      "            29          -0.14283        0.988\n",
      "            30          -0.13971        0.988\n",
      "            31          -0.13672        0.988\n",
      "            32          -0.13388        0.992\n",
      "            33          -0.13115        0.992\n",
      "            34          -0.12854        0.992\n",
      "            35          -0.12604        0.992\n",
      "            36          -0.12364        0.992\n",
      "            37          -0.12134        0.992\n",
      "            38          -0.11913        0.992\n",
      "            39          -0.11700        0.992\n",
      "            40          -0.11495        0.992\n",
      "            41          -0.11297        0.992\n",
      "            42          -0.11107        0.992\n",
      "            43          -0.10923        0.992\n",
      "            44          -0.10746        0.992\n",
      "            45          -0.10574        0.992\n",
      "            46          -0.10408        0.992\n",
      "            47          -0.10248        0.992\n",
      "            48          -0.10093        0.996\n",
      "            49          -0.09943        0.996\n",
      "            50          -0.09797        0.996\n",
      "            51          -0.09656        0.996\n",
      "            52          -0.09519        0.996\n",
      "            53          -0.09386        0.996\n",
      "            54          -0.09257        0.996\n",
      "            55          -0.09132        0.996\n",
      "            56          -0.09010        0.996\n",
      "            57          -0.08892        0.996\n",
      "            58          -0.08777        0.996\n",
      "            59          -0.08665        0.996\n",
      "            60          -0.08556        0.996\n",
      "            61          -0.08450        0.996\n",
      "            62          -0.08346        0.996\n",
      "            63          -0.08246        0.996\n",
      "            64          -0.08147        0.996\n",
      "            65          -0.08052        0.996\n",
      "            66          -0.07958        0.996\n",
      "            67          -0.07867        0.996\n",
      "            68          -0.07778        0.996\n",
      "            69          -0.07691        0.996\n",
      "            70          -0.07607        0.996\n",
      "            71          -0.07524        0.996\n",
      "            72          -0.07443        0.996\n",
      "            73          -0.07364        0.996\n",
      "            74          -0.07286        0.996\n",
      "            75          -0.07210        0.996\n",
      "            76          -0.07136        0.996\n",
      "            77          -0.07064        0.996\n",
      "            78          -0.06993        0.996\n",
      "            79          -0.06924        0.996\n",
      "            80          -0.06856        0.996\n",
      "            81          -0.06789        0.996\n",
      "            82          -0.06724        0.996\n",
      "            83          -0.06660        0.996\n",
      "            84          -0.06597        0.996\n",
      "            85          -0.06536        0.996\n",
      "            86          -0.06476        0.996\n",
      "            87          -0.06417        0.996\n",
      "            88          -0.06359        0.996\n",
      "            89          -0.06302        0.996\n",
      "            90          -0.06246        0.996\n",
      "            91          -0.06191        0.996\n",
      "            92          -0.06138        0.996\n",
      "            93          -0.06085        0.996\n",
      "            94          -0.06033        0.996\n",
      "            95          -0.05982        0.996\n",
      "            96          -0.05932        0.996\n",
      "            97          -0.05883        0.996\n",
      "            98          -0.05835        0.996\n",
      "            99          -0.05788        0.996\n",
      "         Final          -0.05741        0.996\n",
      "\n",
      "The accuracy is 81.54%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.208\n",
      "             2          -0.36030        0.792\n",
      "             3          -0.32167        0.804\n",
      "             4          -0.29095        0.846\n",
      "             5          -0.26656        0.877\n",
      "             6          -0.24677        0.900\n",
      "             7          -0.23032        0.927\n",
      "             8          -0.21635        0.931\n",
      "             9          -0.20429        0.935\n",
      "            10          -0.19374        0.950\n",
      "            11          -0.18440        0.958\n",
      "            12          -0.17606        0.962\n",
      "            13          -0.16854        0.969\n",
      "            14          -0.16173        0.969\n",
      "            15          -0.15551        0.969\n",
      "            16          -0.14980        0.977\n",
      "            17          -0.14455        0.977\n",
      "            18          -0.13969        0.977\n",
      "            19          -0.13518        0.977\n",
      "            20          -0.13098        0.977\n",
      "            21          -0.12706        0.977\n",
      "            22          -0.12339        0.977\n",
      "            23          -0.11994        0.981\n",
      "            24          -0.11669        0.988\n",
      "            25          -0.11363        0.988\n",
      "            26          -0.11074        0.988\n",
      "            27          -0.10800        0.988\n",
      "            28          -0.10541        0.988\n",
      "            29          -0.10295        0.988\n",
      "            30          -0.10060        0.988\n",
      "            31          -0.09837        0.988\n",
      "            32          -0.09625        0.992\n",
      "            33          -0.09422        0.992\n",
      "            34          -0.09227        0.992\n",
      "            35          -0.09042        0.992\n",
      "            36          -0.08864        0.992\n",
      "            37          -0.08693        0.992\n",
      "            38          -0.08530        0.992\n",
      "            39          -0.08372        0.992\n",
      "            40          -0.08221        0.992\n",
      "            41          -0.08076        0.992\n",
      "            42          -0.07936        0.992\n",
      "            43          -0.07801        0.992\n",
      "            44          -0.07671        0.992\n",
      "            45          -0.07545        0.992\n",
      "            46          -0.07424        0.992\n",
      "            47          -0.07307        0.996\n",
      "            48          -0.07193        0.996\n",
      "            49          -0.07084        0.996\n",
      "            50          -0.06977        0.996\n",
      "            51          -0.06875        0.996\n",
      "            52          -0.06775        0.996\n",
      "            53          -0.06678        0.996\n",
      "            54          -0.06585        0.996\n",
      "            55          -0.06494        0.996\n",
      "            56          -0.06405        0.996\n",
      "            57          -0.06319        0.996\n",
      "            58          -0.06236        0.996\n",
      "            59          -0.06155        0.996\n",
      "            60          -0.06076        0.996\n",
      "            61          -0.05999        0.996\n",
      "            62          -0.05925        0.996\n",
      "            63          -0.05852        0.996\n",
      "            64          -0.05781        0.996\n",
      "            65          -0.05712        0.996\n",
      "            66          -0.05644        0.996\n",
      "            67          -0.05579        0.996\n",
      "            68          -0.05514        0.996\n",
      "            69          -0.05452        0.996\n",
      "            70          -0.05391        0.996\n",
      "            71          -0.05331        0.996\n",
      "            72          -0.05273        0.996\n",
      "            73          -0.05216        0.996\n",
      "            74          -0.05160        0.996\n",
      "            75          -0.05106        0.996\n",
      "            76          -0.05053        0.996\n",
      "            77          -0.05001        0.996\n",
      "            78          -0.04950        0.996\n",
      "            79          -0.04900        0.996\n",
      "            80          -0.04851        0.996\n",
      "            81          -0.04803        0.996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            82          -0.04756        0.996\n",
      "            83          -0.04711        0.996\n",
      "            84          -0.04666        0.996\n",
      "            85          -0.04622        0.996\n",
      "            86          -0.04578        0.996\n",
      "            87          -0.04536        0.996\n",
      "            88          -0.04495        0.996\n",
      "            89          -0.04454        0.996\n",
      "            90          -0.04414        0.996\n",
      "            91          -0.04375        0.996\n",
      "            92          -0.04336        0.996\n",
      "            93          -0.04299        0.996\n",
      "            94          -0.04261        0.996\n",
      "            95          -0.04225        0.996\n",
      "            96          -0.04189        0.996\n",
      "            97          -0.04154        0.996\n",
      "            98          -0.04120        0.996\n",
      "            99          -0.04086        0.996\n",
      "         Final          -0.04052        0.996\n",
      "\n",
      "The accuracy is 63.08%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.169\n",
      "             2          -0.30256        0.831\n",
      "             3          -0.27220        0.831\n",
      "             4          -0.24802        0.877\n",
      "             5          -0.22908        0.885\n",
      "             6          -0.21379        0.904\n",
      "             7          -0.20105        0.923\n",
      "             8          -0.19015        0.927\n",
      "             9          -0.18064        0.931\n",
      "            10          -0.17222        0.935\n",
      "            11          -0.16467        0.946\n",
      "            12          -0.15785        0.942\n",
      "            13          -0.15165        0.950\n",
      "            14          -0.14596        0.950\n",
      "            15          -0.14072        0.962\n",
      "            16          -0.13588        0.965\n",
      "            17          -0.13138        0.969\n",
      "            18          -0.12720        0.973\n",
      "            19          -0.12329        0.973\n",
      "            20          -0.11963        0.977\n",
      "            21          -0.11619        0.985\n",
      "            22          -0.11296        0.988\n",
      "            23          -0.10991        0.988\n",
      "            24          -0.10703        0.992\n",
      "            25          -0.10431        0.992\n",
      "            26          -0.10172        0.992\n",
      "            27          -0.09927        0.992\n",
      "            28          -0.09694        0.996\n",
      "            29          -0.09472        0.996\n",
      "            30          -0.09260        0.996\n",
      "            31          -0.09058        0.996\n",
      "            32          -0.08865        0.996\n",
      "            33          -0.08680        0.996\n",
      "            34          -0.08504        0.996\n",
      "            35          -0.08334        0.996\n",
      "            36          -0.08171        0.996\n",
      "            37          -0.08015        0.996\n",
      "            38          -0.07864        0.996\n",
      "            39          -0.07720        0.996\n",
      "            40          -0.07580        0.996\n",
      "            41          -0.07446        0.996\n",
      "            42          -0.07317        1.000\n",
      "            43          -0.07192        1.000\n",
      "            44          -0.07071        1.000\n",
      "            45          -0.06955        1.000\n",
      "            46          -0.06842        1.000\n",
      "            47          -0.06733        1.000\n",
      "            48          -0.06627        1.000\n",
      "            49          -0.06525        1.000\n",
      "            50          -0.06426        1.000\n",
      "            51          -0.06330        1.000\n",
      "            52          -0.06237        1.000\n",
      "            53          -0.06146        1.000\n",
      "            54          -0.06058        1.000\n",
      "            55          -0.05973        1.000\n",
      "            56          -0.05890        1.000\n",
      "            57          -0.05809        1.000\n",
      "            58          -0.05731        1.000\n",
      "            59          -0.05654        1.000\n",
      "            60          -0.05580        1.000\n",
      "            61          -0.05507        1.000\n",
      "            62          -0.05437        1.000\n",
      "            63          -0.05368        1.000\n",
      "            64          -0.05301        1.000\n",
      "            65          -0.05236        1.000\n",
      "            66          -0.05172        1.000\n",
      "            67          -0.05110        1.000\n",
      "            68          -0.05049        1.000\n",
      "            69          -0.04990        1.000\n",
      "            70          -0.04932        1.000\n",
      "            71          -0.04875        1.000\n",
      "            72          -0.04820        1.000\n",
      "            73          -0.04766        1.000\n",
      "            74          -0.04713        1.000\n",
      "            75          -0.04661        1.000\n",
      "            76          -0.04611        1.000\n",
      "            77          -0.04561        1.000\n",
      "            78          -0.04513        1.000\n",
      "            79          -0.04465        1.000\n",
      "            80          -0.04419        1.000\n",
      "            81          -0.04373        1.000\n",
      "            82          -0.04329        1.000\n",
      "            83          -0.04285        1.000\n",
      "            84          -0.04242        1.000\n",
      "            85          -0.04200        1.000\n",
      "            86          -0.04159        1.000\n",
      "            87          -0.04119        1.000\n",
      "            88          -0.04079        1.000\n",
      "            89          -0.04040        1.000\n",
      "            90          -0.04002        1.000\n",
      "            91          -0.03965        1.000\n",
      "            92          -0.03928        1.000\n",
      "            93          -0.03892        1.000\n",
      "            94          -0.03857        1.000\n",
      "            95          -0.03822        1.000\n",
      "            96          -0.03788        1.000\n",
      "            97          -0.03754        1.000\n",
      "            98          -0.03722        1.000\n",
      "            99          -0.03689        1.000\n",
      "         Final          -0.03657        1.000\n",
      "\n",
      "The accuracy is 52.31%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.258\n",
      "             2          -0.43470        0.742\n",
      "             3          -0.39046        0.777\n",
      "             4          -0.35599        0.819\n",
      "             5          -0.32846        0.846\n",
      "             6          -0.30583        0.877\n",
      "             7          -0.28675        0.888\n",
      "             8          -0.27035        0.919\n",
      "             9          -0.25603        0.931\n",
      "            10          -0.24337        0.935\n",
      "            11          -0.23207        0.935\n",
      "            12          -0.22190        0.942\n",
      "            13          -0.21268        0.950\n",
      "            14          -0.20427        0.962\n",
      "            15          -0.19657        0.965\n",
      "            16          -0.18948        0.977\n",
      "            17          -0.18293        0.981\n",
      "            18          -0.17686        0.985\n",
      "            19          -0.17121        0.985\n",
      "            20          -0.16593        0.985\n",
      "            21          -0.16100        0.985\n",
      "            22          -0.15637        0.985\n",
      "            23          -0.15203        0.985\n",
      "            24          -0.14793        0.985\n",
      "            25          -0.14407        0.985\n",
      "            26          -0.14041        0.985\n",
      "            27          -0.13695        0.988\n",
      "            28          -0.13367        0.988\n",
      "            29          -0.13055        0.988\n",
      "            30          -0.12758        0.988\n",
      "            31          -0.12476        0.988\n",
      "            32          -0.12206        0.988\n",
      "            33          -0.11949        0.988\n",
      "            34          -0.11703        0.988\n",
      "            35          -0.11467        0.988\n",
      "            36          -0.11242        0.988\n",
      "            37          -0.11025        0.992\n",
      "            38          -0.10818        0.992\n",
      "            39          -0.10618        0.992\n",
      "            40          -0.10427        0.992\n",
      "            41          -0.10242        0.992\n",
      "            42          -0.10064        0.992\n",
      "            43          -0.09893        0.992\n",
      "            44          -0.09728        0.992\n",
      "            45          -0.09569        0.992\n",
      "            46          -0.09415        0.992\n",
      "            47          -0.09266        0.992\n",
      "            48          -0.09122        0.992\n",
      "            49          -0.08983        0.992\n",
      "            50          -0.08848        0.992\n",
      "            51          -0.08718        0.992\n",
      "            52          -0.08591        0.996\n",
      "            53          -0.08469        0.996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            54          -0.08350        0.996\n",
      "            55          -0.08235        0.996\n",
      "            56          -0.08123        0.996\n",
      "            57          -0.08014        0.996\n",
      "            58          -0.07908        0.996\n",
      "            59          -0.07805        0.996\n",
      "            60          -0.07705        0.996\n",
      "            61          -0.07608        0.996\n",
      "            62          -0.07513        0.996\n",
      "            63          -0.07421        0.996\n",
      "            64          -0.07331        0.996\n",
      "            65          -0.07243        0.996\n",
      "            66          -0.07158        0.996\n",
      "            67          -0.07074        0.996\n",
      "            68          -0.06993        0.996\n",
      "            69          -0.06914        0.996\n",
      "            70          -0.06837        0.996\n",
      "            71          -0.06761        0.996\n",
      "            72          -0.06687        0.996\n",
      "            73          -0.06615        0.996\n",
      "            74          -0.06545        0.996\n",
      "            75          -0.06476        0.996\n",
      "            76          -0.06408        0.996\n",
      "            77          -0.06342        0.996\n",
      "            78          -0.06278        0.996\n",
      "            79          -0.06215        0.996\n",
      "            80          -0.06153        0.996\n",
      "            81          -0.06093        0.996\n",
      "            82          -0.06034        0.996\n",
      "            83          -0.05976        0.996\n",
      "            84          -0.05919        0.996\n",
      "            85          -0.05863        0.996\n",
      "            86          -0.05809        0.996\n",
      "            87          -0.05755        0.996\n",
      "            88          -0.05703        0.996\n",
      "            89          -0.05651        0.996\n",
      "            90          -0.05601        0.996\n",
      "            91          -0.05551        0.996\n",
      "            92          -0.05502        0.996\n",
      "            93          -0.05455        0.996\n",
      "            94          -0.05408        0.996\n",
      "            95          -0.05362        0.996\n",
      "            96          -0.05317        0.996\n",
      "            97          -0.05272        0.996\n",
      "            98          -0.05229        0.996\n",
      "            99          -0.05186        0.996\n",
      "         Final          -0.05144        0.996\n",
      "\n",
      "The accuracy is 80.00%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.258\n",
      "             2          -0.43491        0.742\n",
      "             3          -0.39017        0.773\n",
      "             4          -0.35505        0.812\n",
      "             5          -0.32689        0.854\n",
      "             6          -0.30372        0.881\n",
      "             7          -0.28420        0.892\n",
      "             8          -0.26744        0.904\n",
      "             9          -0.25284        0.919\n",
      "            10          -0.23995        0.931\n",
      "            11          -0.22846        0.954\n",
      "            12          -0.21812        0.965\n",
      "            13          -0.20877        0.969\n",
      "            14          -0.20025        0.973\n",
      "            15          -0.19245        0.973\n",
      "            16          -0.18527        0.973\n",
      "            17          -0.17864        0.977\n",
      "            18          -0.17250        0.977\n",
      "            19          -0.16678        0.977\n",
      "            20          -0.16145        0.981\n",
      "            21          -0.15647        0.985\n",
      "            22          -0.15179        0.985\n",
      "            23          -0.14740        0.985\n",
      "            24          -0.14326        0.985\n",
      "            25          -0.13936        0.988\n",
      "            26          -0.13567        0.988\n",
      "            27          -0.13217        0.992\n",
      "            28          -0.12886        0.996\n",
      "            29          -0.12571        0.996\n",
      "            30          -0.12271        0.996\n",
      "            31          -0.11986        0.996\n",
      "            32          -0.11714        0.996\n",
      "            33          -0.11454        0.996\n",
      "            34          -0.11206        1.000\n",
      "            35          -0.10968        1.000\n",
      "            36          -0.10741        1.000\n",
      "            37          -0.10523        1.000\n",
      "            38          -0.10313        1.000\n",
      "            39          -0.10112        1.000\n",
      "            40          -0.09919        1.000\n",
      "            41          -0.09733        1.000\n",
      "            42          -0.09554        1.000\n",
      "            43          -0.09381        1.000\n",
      "            44          -0.09215        1.000\n",
      "            45          -0.09054        1.000\n",
      "            46          -0.08899        1.000\n",
      "            47          -0.08749        1.000\n",
      "            48          -0.08605        1.000\n",
      "            49          -0.08465        1.000\n",
      "            50          -0.08329        1.000\n",
      "            51          -0.08198        1.000\n",
      "            52          -0.08071        1.000\n",
      "            53          -0.07947        1.000\n",
      "            54          -0.07828        1.000\n",
      "            55          -0.07712        1.000\n",
      "            56          -0.07599        1.000\n",
      "            57          -0.07490        1.000\n",
      "            58          -0.07383        1.000\n",
      "            59          -0.07280        1.000\n",
      "            60          -0.07180        1.000\n",
      "            61          -0.07082        1.000\n",
      "            62          -0.06987        1.000\n",
      "            63          -0.06894        1.000\n",
      "            64          -0.06804        1.000\n",
      "            65          -0.06716        1.000\n",
      "            66          -0.06631        1.000\n",
      "            67          -0.06547        1.000\n",
      "            68          -0.06466        1.000\n",
      "            69          -0.06387        1.000\n",
      "            70          -0.06309        1.000\n",
      "            71          -0.06234        1.000\n",
      "            72          -0.06160        1.000\n",
      "            73          -0.06088        1.000\n",
      "            74          -0.06017        1.000\n",
      "            75          -0.05948        1.000\n",
      "            76          -0.05881        1.000\n",
      "            77          -0.05815        1.000\n",
      "            78          -0.05751        1.000\n",
      "            79          -0.05688        1.000\n",
      "            80          -0.05627        1.000\n",
      "            81          -0.05566        1.000\n",
      "            82          -0.05507        1.000\n",
      "            83          -0.05450        1.000\n",
      "            84          -0.05393        1.000\n",
      "            85          -0.05338        1.000\n",
      "            86          -0.05283        1.000\n",
      "            87          -0.05230        1.000\n",
      "            88          -0.05178        1.000\n",
      "            89          -0.05127        1.000\n",
      "            90          -0.05077        1.000\n",
      "            91          -0.05028        1.000\n",
      "            92          -0.04980        1.000\n",
      "            93          -0.04932        1.000\n",
      "            94          -0.04886        1.000\n",
      "            95          -0.04840        1.000\n",
      "            96          -0.04796        1.000\n",
      "            97          -0.04752        1.000\n",
      "            98          -0.04709        1.000\n",
      "            99          -0.04666        1.000\n",
      "         Final          -0.04625        1.000\n",
      "\n",
      "The accuracy is 75.38%\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.277\n",
      "             2          -0.45440        0.723\n",
      "             3          -0.40645        0.773\n",
      "             4          -0.37008        0.819\n",
      "             5          -0.34159        0.858\n",
      "             6          -0.31848        0.877\n",
      "             7          -0.29914        0.892\n",
      "             8          -0.28260        0.896\n",
      "             9          -0.26817        0.904\n",
      "            10          -0.25543        0.919\n",
      "            11          -0.24405        0.923\n",
      "            12          -0.23380        0.935\n",
      "            13          -0.22448        0.938\n",
      "            14          -0.21598        0.946\n",
      "            15          -0.20817        0.950\n",
      "            16          -0.20097        0.950\n",
      "            17          -0.19430        0.954\n",
      "            18          -0.18810        0.954\n",
      "            19          -0.18232        0.962\n",
      "            20          -0.17692        0.973\n",
      "            21          -0.17186        0.981\n",
      "            22          -0.16710        0.988\n",
      "            23          -0.16261        0.988\n",
      "            24          -0.15838        0.988\n",
      "            25          -0.15438        0.988\n",
      "            26          -0.15060        0.988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            27          -0.14700        0.992\n",
      "            28          -0.14359        0.992\n",
      "            29          -0.14034        0.992\n",
      "            30          -0.13724        0.992\n",
      "            31          -0.13429        0.992\n",
      "            32          -0.13147        0.992\n",
      "            33          -0.12877        0.992\n",
      "            34          -0.12619        0.992\n",
      "            35          -0.12372        0.992\n",
      "            36          -0.12134        0.992\n",
      "            37          -0.11907        0.992\n",
      "            38          -0.11688        0.992\n",
      "            39          -0.11477        0.992\n",
      "            40          -0.11274        0.992\n",
      "            41          -0.11079        0.992\n",
      "            42          -0.10891        0.992\n",
      "            43          -0.10710        0.992\n",
      "            44          -0.10534        0.992\n",
      "            45          -0.10365        0.992\n",
      "            46          -0.10202        0.992\n",
      "            47          -0.10043        0.996\n",
      "            48          -0.09890        0.996\n",
      "            49          -0.09742        0.996\n",
      "            50          -0.09598        0.996\n",
      "            51          -0.09459        0.996\n",
      "            52          -0.09324        0.996\n",
      "            53          -0.09193        0.996\n",
      "            54          -0.09066        0.996\n",
      "            55          -0.08943        0.996\n",
      "            56          -0.08823        0.996\n",
      "            57          -0.08706        0.996\n",
      "            58          -0.08592        0.996\n",
      "            59          -0.08482        0.996\n",
      "            60          -0.08375        0.996\n",
      "            61          -0.08270        0.996\n",
      "            62          -0.08168        0.996\n",
      "            63          -0.08069        0.996\n",
      "            64          -0.07972        0.996\n",
      "            65          -0.07878        0.996\n",
      "            66          -0.07786        0.996\n",
      "            67          -0.07697        0.996\n",
      "            68          -0.07609        0.996\n",
      "            69          -0.07523        0.996\n",
      "            70          -0.07440        0.996\n",
      "            71          -0.07358        0.996\n",
      "            72          -0.07279        0.996\n",
      "            73          -0.07201        0.996\n",
      "            74          -0.07125        0.996\n",
      "            75          -0.07050        0.996\n",
      "            76          -0.06977        0.996\n",
      "            77          -0.06906        0.996\n",
      "            78          -0.06836        0.996\n",
      "            79          -0.06768        0.996\n",
      "            80          -0.06701        0.996\n",
      "            81          -0.06636        0.996\n",
      "            82          -0.06572        0.996\n",
      "            83          -0.06509        0.996\n",
      "            84          -0.06447        0.996\n",
      "            85          -0.06387        0.996\n",
      "            86          -0.06328        0.996\n",
      "            87          -0.06270        0.996\n",
      "            88          -0.06213        0.996\n",
      "            89          -0.06157        0.996\n",
      "            90          -0.06102        0.996\n",
      "            91          -0.06048        0.996\n",
      "            92          -0.05996        0.996\n",
      "            93          -0.05944        0.996\n",
      "            94          -0.05893        0.996\n",
      "            95          -0.05843        0.996\n",
      "            96          -0.05794        0.996\n",
      "            97          -0.05746        0.996\n",
      "            98          -0.05698        0.996\n",
      "            99          -0.05652        0.996\n",
      "         Final          -0.05606        0.996\n",
      "\n",
      "The accuracy is 81.54%\n"
     ]
    }
   ],
   "source": [
    "rmList = [['Original word', 'Stemmed word', 'lemma'], 'POS', 'POS Tag', 'Dependency', 'isCapital']\n",
    "accHistList = []\n",
    "\n",
    "for col in rmList:\n",
    "    d = data.drop(col, axis=1)\n",
    "    acc = maxEntTrain(d)\n",
    "    accHistList.append(np.sum(acc)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average accuracy after removing ['Original word', 'Stemmed word', 'lemma'] feature is: 70.77%\n",
      "The average accuracy after removing POS feature is: 70.46%\n",
      "The average accuracy after removing POS Tag feature is: 70.46%\n",
      "The average accuracy after removing Dependency feature is: 71.08%\n",
      "The average accuracy after removing isCapital feature is: 70.46%\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(accHistList)):\n",
    "    rmCol = rmList[i]\n",
    "    print('The average accuracy after removing', rmCol, 'feature is: {:.2%}'.format(accHistList[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train by Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras import models\n",
    "from keras import Sequential\n",
    "import numpy as np\n",
    "\n",
    "numFeatures = len(data.columns) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', input_dim=numFeatures))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "            metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "encodedData = data[data.columns[:]].apply(le.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def logstic(df, model):\n",
    "    \n",
    "    acc = []\n",
    "    train1, trainFeature, trainLabel, test1, testLabel = splitData(df)\n",
    "    \n",
    "    for x in range(len(trainFeature)):\n",
    "\n",
    "        mat = np.array(list(trainFeature[x][0].values()))\n",
    "        tsMat = np.array(list(test1[x][0].values()))\n",
    "\n",
    "        for i in range(len(trainFeature[x])-1):\n",
    "            temp = np.array(list(trainFeature[x][i+1].values()))\n",
    "            mat = np.vstack((mat, temp))\n",
    "\n",
    "        for j in range(len(test1[x])-1):\n",
    "            tsTemp = np.array(list(test1[x][j+1].values()))\n",
    "            tsMat = np.vstack((tsMat, tsTemp))\n",
    "\n",
    "        label = np.array(trainLabel[x]).reshape((-1,1))\n",
    "        tsLabel = np.array(testLabel[x]).reshape((-1,1))\n",
    "\n",
    "        oneHotYTrain = keras.utils.to_categorical(label, num_classes=2)\n",
    "        oneHotYtsTrain = keras.utils.to_categorical(tsLabel, num_classes=2)\n",
    "\n",
    "        model.fit(mat, oneHotYTrain, epochs=50, batch_size=32)\n",
    "        score=model.evaluate(tsMat,oneHotYtsTrain,batch_size=32)\n",
    "\n",
    "        acc.append(score[1])\n",
    "        print(\"\\n{}: {:.2%}\".format(model.metrics_names[1], score[1]))\n",
    "        \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "260/260 [==============================] - 0s 746us/step - loss: 2.4891 - categorical_accuracy: 0.7923\n",
      "Epoch 2/50\n",
      "260/260 [==============================] - 0s 59us/step - loss: 2.0693 - categorical_accuracy: 0.7923\n",
      "Epoch 3/50\n",
      "260/260 [==============================] - 0s 68us/step - loss: 1.5036 - categorical_accuracy: 0.7846\n",
      "Epoch 4/50\n",
      "260/260 [==============================] - 0s 100us/step - loss: 1.0450 - categorical_accuracy: 0.7577\n",
      "Epoch 5/50\n",
      "260/260 [==============================] - 0s 79us/step - loss: 0.8503 - categorical_accuracy: 0.7000\n",
      "Epoch 6/50\n",
      "260/260 [==============================] - 0s 72us/step - loss: 0.8060 - categorical_accuracy: 0.6538\n",
      "Epoch 7/50\n",
      "260/260 [==============================] - 0s 84us/step - loss: 0.7629 - categorical_accuracy: 0.6423\n",
      "Epoch 8/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: 0.7231 - categorical_accuracy: 0.7154\n",
      "Epoch 9/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: 0.7002 - categorical_accuracy: 0.7346\n",
      "Epoch 10/50\n",
      "260/260 [==============================] - 0s 79us/step - loss: 0.6809 - categorical_accuracy: 0.7308\n",
      "Epoch 11/50\n",
      "260/260 [==============================] - 0s 79us/step - loss: 0.6604 - categorical_accuracy: 0.7154\n",
      "Epoch 12/50\n",
      "260/260 [==============================] - 0s 92us/step - loss: 0.6465 - categorical_accuracy: 0.7154\n",
      "Epoch 13/50\n",
      "260/260 [==============================] - 0s 93us/step - loss: 0.6312 - categorical_accuracy: 0.7346\n",
      "Epoch 14/50\n",
      "260/260 [==============================] - 0s 92us/step - loss: 0.6185 - categorical_accuracy: 0.7385\n",
      "Epoch 15/50\n",
      "260/260 [==============================] - 0s 83us/step - loss: 0.6138 - categorical_accuracy: 0.7423\n",
      "Epoch 16/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: 0.5919 - categorical_accuracy: 0.7462\n",
      "Epoch 17/50\n",
      "260/260 [==============================] - 0s 89us/step - loss: 0.5883 - categorical_accuracy: 0.7269\n",
      "Epoch 18/50\n",
      "260/260 [==============================] - 0s 80us/step - loss: 0.5773 - categorical_accuracy: 0.7423\n",
      "Epoch 19/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: 0.5716 - categorical_accuracy: 0.7577\n",
      "Epoch 20/50\n",
      "260/260 [==============================] - 0s 100us/step - loss: 0.5678 - categorical_accuracy: 0.7692\n",
      "Epoch 21/50\n",
      "260/260 [==============================] - 0s 82us/step - loss: 0.5658 - categorical_accuracy: 0.7654\n",
      "Epoch 22/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: 0.5593 - categorical_accuracy: 0.7462\n",
      "Epoch 23/50\n",
      "260/260 [==============================] - 0s 84us/step - loss: 0.5489 - categorical_accuracy: 0.7615\n",
      "Epoch 24/50\n",
      "260/260 [==============================] - 0s 79us/step - loss: 0.5404 - categorical_accuracy: 0.7885\n",
      "Epoch 25/50\n",
      "260/260 [==============================] - 0s 99us/step - loss: 0.5374 - categorical_accuracy: 0.7654\n",
      "Epoch 26/50\n",
      "260/260 [==============================] - 0s 121us/step - loss: 0.5271 - categorical_accuracy: 0.7577\n",
      "Epoch 27/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: 0.5225 - categorical_accuracy: 0.7885\n",
      "Epoch 28/50\n",
      "260/260 [==============================] - 0s 82us/step - loss: 0.5198 - categorical_accuracy: 0.7962\n",
      "Epoch 29/50\n",
      "260/260 [==============================] - 0s 82us/step - loss: 0.5137 - categorical_accuracy: 0.7923\n",
      "Epoch 30/50\n",
      "260/260 [==============================] - 0s 84us/step - loss: 0.5088 - categorical_accuracy: 0.7885\n",
      "Epoch 31/50\n",
      "260/260 [==============================] - 0s 77us/step - loss: 0.5082 - categorical_accuracy: 0.7885\n",
      "Epoch 32/50\n",
      "260/260 [==============================] - 0s 80us/step - loss: 0.5054 - categorical_accuracy: 0.7846\n",
      "Epoch 33/50\n",
      "260/260 [==============================] - 0s 83us/step - loss: nan - categorical_accuracy: 0.7731\n",
      "Epoch 34/50\n",
      "260/260 [==============================] - 0s 108us/step - loss: nan - categorical_accuracy: 0.7923\n",
      "Epoch 35/50\n",
      "260/260 [==============================] - 0s 90us/step - loss: nan - categorical_accuracy: 0.7923\n",
      "Epoch 36/50\n",
      "260/260 [==============================] - 0s 104us/step - loss: nan - categorical_accuracy: 0.7923\n",
      "Epoch 37/50\n",
      "260/260 [==============================] - 0s 104us/step - loss: nan - categorical_accuracy: 0.7923\n",
      "Epoch 38/50\n",
      "260/260 [==============================] - 0s 168us/step - loss: nan - categorical_accuracy: 0.7923\n",
      "Epoch 39/50\n",
      "260/260 [==============================] - 0s 104us/step - loss: nan - categorical_accuracy: 0.7923\n",
      "Epoch 40/50\n",
      "260/260 [==============================] - 0s 86us/step - loss: nan - categorical_accuracy: 0.7923\n",
      "Epoch 41/50\n",
      "260/260 [==============================] - 0s 79us/step - loss: nan - categorical_accuracy: 0.7923\n",
      "Epoch 42/50\n",
      "260/260 [==============================] - 0s 92us/step - loss: nan - categorical_accuracy: 0.7923\n",
      "Epoch 43/50\n",
      "260/260 [==============================] - 0s 83us/step - loss: nan - categorical_accuracy: 0.7923\n",
      "Epoch 44/50\n",
      "260/260 [==============================] - 0s 87us/step - loss: nan - categorical_accuracy: 0.7923\n",
      "Epoch 45/50\n",
      "260/260 [==============================] - 0s 91us/step - loss: nan - categorical_accuracy: 0.7923\n",
      "Epoch 46/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: nan - categorical_accuracy: 0.7923\n",
      "Epoch 47/50\n",
      "260/260 [==============================] - 0s 108us/step - loss: nan - categorical_accuracy: 0.7923\n",
      "Epoch 48/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: nan - categorical_accuracy: 0.7923\n",
      "Epoch 49/50\n",
      "260/260 [==============================] - 0s 89us/step - loss: nan - categorical_accuracy: 0.7923\n",
      "Epoch 50/50\n",
      "260/260 [==============================] - 0s 78us/step - loss: nan - categorical_accuracy: 0.7923\n",
      "65/65 [==============================] - 0s 563us/step\n",
      "\n",
      "categorical_accuracy: 66.15%\n",
      "Epoch 1/50\n",
      "260/260 [==============================] - 0s 77us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 2/50\n",
      "260/260 [==============================] - 0s 73us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 3/50\n",
      "260/260 [==============================] - 0s 77us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 4/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 5/50\n",
      "260/260 [==============================] - 0s 100us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 6/50\n",
      "260/260 [==============================] - 0s 91us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 7/50\n",
      "260/260 [==============================] - 0s 83us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 8/50\n",
      "260/260 [==============================] - 0s 98us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 9/50\n",
      "260/260 [==============================] - 0s 82us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 10/50\n",
      "260/260 [==============================] - 0s 112us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 11/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 12/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 13/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 14/50\n",
      "260/260 [==============================] - 0s 78us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 15/50\n",
      "260/260 [==============================] - 0s 84us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 16/50\n",
      "260/260 [==============================] - 0s 98us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 17/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 18/50\n",
      "260/260 [==============================] - 0s 78us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 19/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 20/50\n",
      "260/260 [==============================] - 0s 97us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 21/50\n",
      "260/260 [==============================] - 0s 96us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 22/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 23/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 24/50\n",
      "260/260 [==============================] - 0s 100us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 25/50\n",
      "260/260 [==============================] - 0s 109us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 26/50\n",
      "260/260 [==============================] - 0s 96us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 27/50\n",
      "260/260 [==============================] - 0s 96us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 28/50\n",
      "260/260 [==============================] - 0s 89us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 29/50\n",
      "260/260 [==============================] - 0s 97us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 30/50\n",
      "260/260 [==============================] - 0s 84us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 31/50\n",
      "260/260 [==============================] - 0s 86us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 32/50\n",
      "260/260 [==============================] - 0s 100us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 33/50\n",
      "260/260 [==============================] - 0s 83us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 34/50\n",
      "260/260 [==============================] - 0s 78us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 35/50\n",
      "260/260 [==============================] - 0s 83us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 36/50\n",
      "260/260 [==============================] - 0s 96us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 37/50\n",
      "260/260 [==============================] - 0s 80us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 38/50\n",
      "260/260 [==============================] - 0s 86us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 39/50\n",
      "260/260 [==============================] - 0s 84us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 40/50\n",
      "260/260 [==============================] - 0s 84us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 41/50\n",
      "260/260 [==============================] - 0s 82us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 42/50\n",
      "260/260 [==============================] - 0s 110us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 43/50\n",
      "260/260 [==============================] - 0s 95us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 44/50\n",
      "260/260 [==============================] - 0s 107us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 45/50\n",
      "260/260 [==============================] - 0s 101us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 46/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 47/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 48/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 49/50\n",
      "260/260 [==============================] - 0s 97us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "Epoch 50/50\n",
      "260/260 [==============================] - 0s 91us/step - loss: nan - categorical_accuracy: 0.8308\n",
      "65/65 [==============================] - 0s 77us/step\n",
      "\n",
      "categorical_accuracy: 50.77%\n",
      "Epoch 1/50\n",
      "260/260 [==============================] - 0s 78us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 2/50\n",
      "260/260 [==============================] - 0s 66us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 3/50\n",
      "260/260 [==============================] - 0s 58us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 4/50\n",
      "260/260 [==============================] - 0s 73us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 5/50\n",
      "260/260 [==============================] - 0s 61us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 6/50\n",
      "260/260 [==============================] - 0s 69us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 7/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 8/50\n",
      "260/260 [==============================] - 0s 70us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 9/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 10/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 11/50\n",
      "260/260 [==============================] - 0s 77us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 12/50\n",
      "260/260 [==============================] - 0s 80us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 13/50\n",
      "260/260 [==============================] - 0s 89us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 14/50\n",
      "260/260 [==============================] - 0s 73us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 15/50\n",
      "260/260 [==============================] - 0s 75us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 16/50\n",
      "260/260 [==============================] - 0s 75us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 17/50\n",
      "260/260 [==============================] - 0s 87us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 18/50\n",
      "260/260 [==============================] - 0s 97us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 19/50\n",
      "260/260 [==============================] - 0s 96us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 20/50\n",
      "260/260 [==============================] - 0s 77us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 21/50\n",
      "260/260 [==============================] - 0s 77us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 22/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 23/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 24/50\n",
      "260/260 [==============================] - 0s 89us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 25/50\n",
      "260/260 [==============================] - 0s 79us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 26/50\n",
      "260/260 [==============================] - 0s 74us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 27/50\n",
      "260/260 [==============================] - 0s 77us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 28/50\n",
      "260/260 [==============================] - 0s 75us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 29/50\n",
      "260/260 [==============================] - 0s 77us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 30/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 31/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 32/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 33/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 34/50\n",
      "260/260 [==============================] - 0s 96us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 35/50\n",
      "260/260 [==============================] - 0s 90us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 36/50\n",
      "260/260 [==============================] - 0s 77us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 37/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 38/50\n",
      "260/260 [==============================] - 0s 77us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 39/50\n",
      "260/260 [==============================] - 0s 93us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 40/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 41/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 42/50\n",
      "260/260 [==============================] - 0s 109us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 43/50\n",
      "260/260 [==============================] - 0s 108us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 44/50\n",
      "260/260 [==============================] - 0s 89us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 45/50\n",
      "260/260 [==============================] - 0s 89us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260/260 [==============================] - 0s 88us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 47/50\n",
      "260/260 [==============================] - 0s 73us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 48/50\n",
      "260/260 [==============================] - 0s 78us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 49/50\n",
      "260/260 [==============================] - 0s 77us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 50/50\n",
      "260/260 [==============================] - 0s 69us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "65/65 [==============================] - 0s 77us/step\n",
      "\n",
      "categorical_accuracy: 86.15%\n",
      "Epoch 1/50\n",
      "260/260 [==============================] - 0s 69us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 2/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 3/50\n",
      "260/260 [==============================] - 0s 89us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 4/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 5/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 6/50\n",
      "260/260 [==============================] - 0s 92us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 7/50\n",
      "260/260 [==============================] - 0s 80us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 8/50\n",
      "260/260 [==============================] - 0s 96us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 9/50\n",
      "260/260 [==============================] - 0s 82us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 10/50\n",
      "260/260 [==============================] - 0s 74us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 11/50\n",
      "260/260 [==============================] - 0s 86us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 12/50\n",
      "260/260 [==============================] - 0s 77us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 13/50\n",
      "260/260 [==============================] - 0s 61us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 14/50\n",
      "260/260 [==============================] - 0s 63us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 15/50\n",
      "260/260 [==============================] - 0s 58us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 16/50\n",
      "260/260 [==============================] - 0s 68us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 17/50\n",
      "260/260 [==============================] - 0s 65us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 18/50\n",
      "260/260 [==============================] - 0s 65us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 19/50\n",
      "260/260 [==============================] - 0s 93us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 20/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 21/50\n",
      "260/260 [==============================] - 0s 97us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 22/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 23/50\n",
      "260/260 [==============================] - 0s 84us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 24/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 25/50\n",
      "260/260 [==============================] - 0s 110us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 26/50\n",
      "260/260 [==============================] - 0s 104us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 27/50\n",
      "260/260 [==============================] - 0s 106us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 28/50\n",
      "260/260 [==============================] - 0s 106us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 29/50\n",
      "260/260 [==============================] - 0s 93us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 30/50\n",
      "260/260 [==============================] - 0s 106us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 31/50\n",
      "260/260 [==============================] - 0s 92us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 32/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 33/50\n",
      "260/260 [==============================] - 0s 72us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 34/50\n",
      "260/260 [==============================] - 0s 79us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 35/50\n",
      "260/260 [==============================] - 0s 77us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 36/50\n",
      "260/260 [==============================] - 0s 75us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 37/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 38/50\n",
      "260/260 [==============================] - 0s 83us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 39/50\n",
      "260/260 [==============================] - 0s 89us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 40/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 41/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 42/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 43/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 44/50\n",
      "260/260 [==============================] - 0s 86us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 45/50\n",
      "260/260 [==============================] - 0s 86us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 46/50\n",
      "260/260 [==============================] - 0s 78us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 47/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 48/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 49/50\n",
      "260/260 [==============================] - 0s 79us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "Epoch 50/50\n",
      "260/260 [==============================] - 0s 92us/step - loss: nan - categorical_accuracy: 0.7423\n",
      "65/65 [==============================] - 0s 63us/step\n",
      "\n",
      "categorical_accuracy: 86.15%\n",
      "Epoch 1/50\n",
      "260/260 [==============================] - 0s 58us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 2/50\n",
      "260/260 [==============================] - 0s 71us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 3/50\n",
      "260/260 [==============================] - 0s 90us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 4/50\n",
      "260/260 [==============================] - 0s 80us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 5/50\n",
      "260/260 [==============================] - 0s 72us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 6/50\n",
      "260/260 [==============================] - 0s 58us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 7/50\n",
      "260/260 [==============================] - 0s 61us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 8/50\n",
      "260/260 [==============================] - 0s 62us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 9/50\n",
      "260/260 [==============================] - 0s 59us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 10/50\n",
      "260/260 [==============================] - 0s 80us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 11/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 12/50\n",
      "260/260 [==============================] - 0s 87us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 13/50\n",
      "260/260 [==============================] - 0s 77us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 14/50\n",
      "260/260 [==============================] - 0s 104us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 15/50\n",
      "260/260 [==============================] - 0s 100us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 16/50\n",
      "260/260 [==============================] - 0s 93us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 17/50\n",
      "260/260 [==============================] - 0s 74us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 18/50\n",
      "260/260 [==============================] - 0s 77us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 19/50\n",
      "260/260 [==============================] - 0s 74us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 20/50\n",
      "260/260 [==============================] - 0s 82us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 21/50\n",
      "260/260 [==============================] - 0s 79us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 22/50\n",
      "260/260 [==============================] - 0s 91us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 23/50\n",
      "260/260 [==============================] - 0s 89us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 24/50\n",
      "260/260 [==============================] - 0s 82us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 25/50\n",
      "260/260 [==============================] - 0s 96us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 26/50\n",
      "260/260 [==============================] - 0s 93us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 27/50\n",
      "260/260 [==============================] - 0s 83us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 28/50\n",
      "260/260 [==============================] - 0s 77us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 29/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 30/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 31/50\n",
      "260/260 [==============================] - 0s 73us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 32/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 33/50\n",
      "260/260 [==============================] - 0s 77us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 34/50\n",
      "260/260 [==============================] - 0s 82us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 35/50\n",
      "260/260 [==============================] - 0s 89us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 36/50\n",
      "260/260 [==============================] - 0s 89us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 37/50\n",
      "260/260 [==============================] - 0s 80us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 38/50\n",
      "260/260 [==============================] - 0s 94us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 39/50\n",
      "260/260 [==============================] - 0s 84us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 40/50\n",
      "260/260 [==============================] - 0s 79us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 41/50\n",
      "260/260 [==============================] - 0s 79us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 42/50\n",
      "260/260 [==============================] - 0s 77us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 43/50\n",
      "260/260 [==============================] - 0s 80us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 44/50\n",
      "260/260 [==============================] - 0s 89us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 45/50\n",
      "260/260 [==============================] - 0s 84us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 46/50\n",
      "260/260 [==============================] - 0s 89us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 47/50\n",
      "260/260 [==============================] - 0s 103us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 48/50\n",
      "260/260 [==============================] - 0s 112us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 49/50\n",
      "260/260 [==============================] - 0s 100us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "Epoch 50/50\n",
      "260/260 [==============================] - 0s 104us/step - loss: nan - categorical_accuracy: 0.7231\n",
      "65/65 [==============================] - 0s 123us/step\n",
      "\n",
      "categorical_accuracy: 93.85%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Use relu as activation functions in hidden layers\n",
    "model.add(Dense(10, activation='relu', input_dim=numFeatures))\n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "            metrics=['categorical_accuracy'])\n",
    "\n",
    "acc = logstic(encodedData, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical_accuracy history is: [0.6615384817123413, 0.5076923370361328, 0.8615384697914124, 0.8615384697914124, 0.9384615421295166]\n",
      "The average accuracy is 76.62%\n"
     ]
    }
   ],
   "source": [
    "print('Categorical_accuracy history is:', acc)\n",
    "print('The average accuracy is {:.2%}'.format(np.sum(acc)/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "260/260 [==============================] - 0s 834us/step - loss: 0.7721 - categorical_accuracy: 0.2077\n",
      "Epoch 2/50\n",
      "260/260 [==============================] - 0s 87us/step - loss: 0.7615 - categorical_accuracy: 0.2077\n",
      "Epoch 3/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: 0.7532 - categorical_accuracy: 0.2077\n",
      "Epoch 4/50\n",
      "260/260 [==============================] - 0s 92us/step - loss: 0.7452 - categorical_accuracy: 0.2077\n",
      "Epoch 5/50\n",
      "260/260 [==============================] - 0s 95us/step - loss: 0.7376 - categorical_accuracy: 0.2077\n",
      "Epoch 6/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: 0.7303 - categorical_accuracy: 0.2077\n",
      "Epoch 7/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: 0.7235 - categorical_accuracy: 0.2077\n",
      "Epoch 8/50\n",
      "260/260 [==============================] - 0s 78us/step - loss: 0.7169 - categorical_accuracy: 0.2077\n",
      "Epoch 9/50\n",
      "260/260 [==============================] - 0s 71us/step - loss: 0.7108 - categorical_accuracy: 0.2077\n",
      "Epoch 10/50\n",
      "260/260 [==============================] - 0s 75us/step - loss: 0.7049 - categorical_accuracy: 0.2077\n",
      "Epoch 11/50\n",
      "260/260 [==============================] - 0s 90us/step - loss: 0.6987 - categorical_accuracy: 0.2077\n",
      "Epoch 12/50\n",
      "260/260 [==============================] - 0s 76us/step - loss: 0.6931 - categorical_accuracy: 0.5231\n",
      "Epoch 13/50\n",
      "260/260 [==============================] - 0s 91us/step - loss: 0.6878 - categorical_accuracy: 0.7808\n",
      "Epoch 14/50\n",
      "260/260 [==============================] - 0s 75us/step - loss: 0.6826 - categorical_accuracy: 0.7885\n",
      "Epoch 15/50\n",
      "260/260 [==============================] - 0s 96us/step - loss: 0.6774 - categorical_accuracy: 0.7923\n",
      "Epoch 16/50\n",
      "260/260 [==============================] - 0s 95us/step - loss: 0.6720 - categorical_accuracy: 0.7923\n",
      "Epoch 17/50\n",
      "260/260 [==============================] - 0s 80us/step - loss: 0.6671 - categorical_accuracy: 0.7923\n",
      "Epoch 18/50\n",
      "260/260 [==============================] - 0s 75us/step - loss: 0.6620 - categorical_accuracy: 0.7923\n",
      "Epoch 19/50\n",
      "260/260 [==============================] - 0s 74us/step - loss: 0.6572 - categorical_accuracy: 0.7923\n",
      "Epoch 20/50\n",
      "260/260 [==============================] - 0s 86us/step - loss: 0.6526 - categorical_accuracy: 0.7923\n",
      "Epoch 21/50\n",
      "260/260 [==============================] - 0s 80us/step - loss: 0.6483 - categorical_accuracy: 0.7923\n",
      "Epoch 22/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: 0.6442 - categorical_accuracy: 0.7923\n",
      "Epoch 23/50\n",
      "260/260 [==============================] - 0s 79us/step - loss: 0.6400 - categorical_accuracy: 0.7923\n",
      "Epoch 24/50\n",
      "260/260 [==============================] - 0s 82us/step - loss: 0.6359 - categorical_accuracy: 0.7923\n",
      "Epoch 25/50\n",
      "260/260 [==============================] - 0s 109us/step - loss: 0.6320 - categorical_accuracy: 0.7923\n",
      "Epoch 26/50\n",
      "260/260 [==============================] - 0s 119us/step - loss: 0.6283 - categorical_accuracy: 0.7923\n",
      "Epoch 27/50\n",
      "260/260 [==============================] - 0s 96us/step - loss: 0.6245 - categorical_accuracy: 0.7923\n",
      "Epoch 28/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: 0.6206 - categorical_accuracy: 0.7923\n",
      "Epoch 29/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: 0.6166 - categorical_accuracy: 0.7923\n",
      "Epoch 30/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: 0.6127 - categorical_accuracy: 0.7923\n",
      "Epoch 31/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: 0.6090 - categorical_accuracy: 0.7923\n",
      "Epoch 32/50\n",
      "260/260 [==============================] - 0s 98us/step - loss: 0.6052 - categorical_accuracy: 0.7923\n",
      "Epoch 33/50\n",
      "260/260 [==============================] - 0s 93us/step - loss: 0.6017 - categorical_accuracy: 0.7923\n",
      "Epoch 34/50\n",
      "260/260 [==============================] - 0s 104us/step - loss: 0.5983 - categorical_accuracy: 0.7923\n",
      "Epoch 35/50\n",
      "260/260 [==============================] - 0s 90us/step - loss: 0.5953 - categorical_accuracy: 0.7923\n",
      "Epoch 36/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: 0.5922 - categorical_accuracy: 0.7923\n",
      "Epoch 37/50\n",
      "260/260 [==============================] - 0s 87us/step - loss: 0.5895 - categorical_accuracy: 0.7923\n",
      "Epoch 38/50\n",
      "260/260 [==============================] - 0s 103us/step - loss: 0.5868 - categorical_accuracy: 0.7923\n",
      "Epoch 39/50\n",
      "260/260 [==============================] - 0s 96us/step - loss: 0.5840 - categorical_accuracy: 0.7923\n",
      "Epoch 40/50\n",
      "260/260 [==============================] - 0s 86us/step - loss: 0.5811 - categorical_accuracy: 0.7923\n",
      "Epoch 41/50\n",
      "260/260 [==============================] - 0s 79us/step - loss: 0.5785 - categorical_accuracy: 0.7923\n",
      "Epoch 42/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: 0.5758 - categorical_accuracy: 0.7923\n",
      "Epoch 43/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: 0.5729 - categorical_accuracy: 0.7923\n",
      "Epoch 44/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: 0.5705 - categorical_accuracy: 0.7923\n",
      "Epoch 45/50\n",
      "260/260 [==============================] - 0s 77us/step - loss: 0.5676 - categorical_accuracy: 0.7923\n",
      "Epoch 46/50\n",
      "260/260 [==============================] - 0s 77us/step - loss: 0.5653 - categorical_accuracy: 0.7923\n",
      "Epoch 47/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: 0.5631 - categorical_accuracy: 0.7923\n",
      "Epoch 48/50\n",
      "260/260 [==============================] - 0s 91us/step - loss: 0.5609 - categorical_accuracy: 0.7923\n",
      "Epoch 49/50\n",
      "260/260 [==============================] - 0s 83us/step - loss: 0.5588 - categorical_accuracy: 0.7923\n",
      "Epoch 50/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: 0.5566 - categorical_accuracy: 0.7923\n",
      "65/65 [==============================] - 0s 581us/step\n",
      "\n",
      "categorical_accuracy: 66.15%\n",
      "Epoch 1/50\n",
      "260/260 [==============================] - 0s 67us/step - loss: 0.5292 - categorical_accuracy: 0.8308\n",
      "Epoch 2/50\n",
      "260/260 [==============================] - 0s 69us/step - loss: 0.5262 - categorical_accuracy: 0.8308\n",
      "Epoch 3/50\n",
      "260/260 [==============================] - 0s 71us/step - loss: 0.5226 - categorical_accuracy: 0.8308\n",
      "Epoch 4/50\n",
      "260/260 [==============================] - 0s 75us/step - loss: 0.5195 - categorical_accuracy: 0.8308\n",
      "Epoch 5/50\n",
      "260/260 [==============================] - 0s 69us/step - loss: 0.5163 - categorical_accuracy: 0.8308\n",
      "Epoch 6/50\n",
      "260/260 [==============================] - 0s 75us/step - loss: 0.5134 - categorical_accuracy: 0.8308\n",
      "Epoch 7/50\n",
      "260/260 [==============================] - 0s 77us/step - loss: 0.5106 - categorical_accuracy: 0.8308\n",
      "Epoch 8/50\n",
      "260/260 [==============================] - 0s 80us/step - loss: 0.5084 - categorical_accuracy: 0.8308\n",
      "Epoch 9/50\n",
      "260/260 [==============================] - 0s 92us/step - loss: 0.5057 - categorical_accuracy: 0.8308\n",
      "Epoch 10/50\n",
      "260/260 [==============================] - 0s 114us/step - loss: 0.5032 - categorical_accuracy: 0.8308\n",
      "Epoch 11/50\n",
      "260/260 [==============================] - 0s 100us/step - loss: 0.5006 - categorical_accuracy: 0.8308\n",
      "Epoch 12/50\n",
      "260/260 [==============================] - 0s 92us/step - loss: 0.4986 - categorical_accuracy: 0.8308\n",
      "Epoch 13/50\n",
      "260/260 [==============================] - 0s 78us/step - loss: 0.4967 - categorical_accuracy: 0.8308\n",
      "Epoch 14/50\n",
      "260/260 [==============================] - 0s 84us/step - loss: 0.4949 - categorical_accuracy: 0.8308\n",
      "Epoch 15/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: 0.4928 - categorical_accuracy: 0.8308\n",
      "Epoch 16/50\n",
      "260/260 [==============================] - 0s 84us/step - loss: 0.4909 - categorical_accuracy: 0.8308\n",
      "Epoch 17/50\n",
      "260/260 [==============================] - 0s 96us/step - loss: 0.4891 - categorical_accuracy: 0.8308\n",
      "Epoch 18/50\n",
      "260/260 [==============================] - 0s 89us/step - loss: 0.4870 - categorical_accuracy: 0.8308\n",
      "Epoch 19/50\n",
      "260/260 [==============================] - 0s 86us/step - loss: 0.4854 - categorical_accuracy: 0.8308\n",
      "Epoch 20/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: 0.4835 - categorical_accuracy: 0.8308\n",
      "Epoch 21/50\n",
      "260/260 [==============================] - 0s 99us/step - loss: 0.4817 - categorical_accuracy: 0.8308\n",
      "Epoch 22/50\n",
      "260/260 [==============================] - 0s 108us/step - loss: 0.4801 - categorical_accuracy: 0.8308\n",
      "Epoch 23/50\n",
      "260/260 [==============================] - 0s 100us/step - loss: 0.4785 - categorical_accuracy: 0.8308\n",
      "Epoch 24/50\n",
      "260/260 [==============================] - 0s 104us/step - loss: 0.4771 - categorical_accuracy: 0.8308\n",
      "Epoch 25/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: 0.4760 - categorical_accuracy: 0.8308\n",
      "Epoch 26/50\n",
      "260/260 [==============================] - 0s 83us/step - loss: 0.4747 - categorical_accuracy: 0.8308\n",
      "Epoch 27/50\n",
      "260/260 [==============================] - 0s 91us/step - loss: 0.4738 - categorical_accuracy: 0.8308\n",
      "Epoch 28/50\n",
      "260/260 [==============================] - 0s 83us/step - loss: 0.4728 - categorical_accuracy: 0.8308\n",
      "Epoch 29/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: 0.4718 - categorical_accuracy: 0.8308\n",
      "Epoch 30/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: 0.4709 - categorical_accuracy: 0.8308\n",
      "Epoch 31/50\n",
      "260/260 [==============================] - 0s 80us/step - loss: 0.4700 - categorical_accuracy: 0.8308\n",
      "Epoch 32/50\n",
      "260/260 [==============================] - 0s 80us/step - loss: 0.4691 - categorical_accuracy: 0.8308\n",
      "Epoch 33/50\n",
      "260/260 [==============================] - 0s 89us/step - loss: 0.4683 - categorical_accuracy: 0.8308\n",
      "Epoch 34/50\n",
      "260/260 [==============================] - 0s 80us/step - loss: 0.4673 - categorical_accuracy: 0.8308\n",
      "Epoch 35/50\n",
      "260/260 [==============================] - 0s 91us/step - loss: 0.4665 - categorical_accuracy: 0.8308\n",
      "Epoch 36/50\n",
      "260/260 [==============================] - 0s 86us/step - loss: 0.4655 - categorical_accuracy: 0.8308\n",
      "Epoch 37/50\n",
      "260/260 [==============================] - 0s 91us/step - loss: 0.4648 - categorical_accuracy: 0.8308\n",
      "Epoch 38/50\n",
      "260/260 [==============================] - 0s 103us/step - loss: 0.4641 - categorical_accuracy: 0.8308\n",
      "Epoch 39/50\n",
      "260/260 [==============================] - 0s 93us/step - loss: 0.4636 - categorical_accuracy: 0.8308\n",
      "Epoch 40/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: 0.4631 - categorical_accuracy: 0.8308\n",
      "Epoch 41/50\n",
      "260/260 [==============================] - 0s 86us/step - loss: 0.4626 - categorical_accuracy: 0.8308\n",
      "Epoch 42/50\n",
      "260/260 [==============================] - 0s 83us/step - loss: 0.4623 - categorical_accuracy: 0.8308\n",
      "Epoch 43/50\n",
      "260/260 [==============================] - 0s 86us/step - loss: 0.4619 - categorical_accuracy: 0.8308\n",
      "Epoch 44/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: 0.4615 - categorical_accuracy: 0.8308\n",
      "Epoch 45/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: 0.4612 - categorical_accuracy: 0.8308\n",
      "Epoch 46/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: 0.4609 - categorical_accuracy: 0.8308\n",
      "Epoch 47/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: 0.4605 - categorical_accuracy: 0.8308\n",
      "Epoch 48/50\n",
      "260/260 [==============================] - 0s 87us/step - loss: 0.4600 - categorical_accuracy: 0.8308\n",
      "Epoch 49/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: 0.4597 - categorical_accuracy: 0.8308\n",
      "Epoch 50/50\n",
      "260/260 [==============================] - 0s 84us/step - loss: 0.4592 - categorical_accuracy: 0.8308\n",
      "65/65 [==============================] - 0s 123us/step\n",
      "\n",
      "categorical_accuracy: 50.77%\n",
      "Epoch 1/50\n",
      "260/260 [==============================] - 0s 104us/step - loss: 0.5785 - categorical_accuracy: 0.7423\n",
      "Epoch 2/50\n",
      "260/260 [==============================] - 0s 83us/step - loss: 0.5787 - categorical_accuracy: 0.7423\n",
      "Epoch 3/50\n",
      "260/260 [==============================] - 0s 115us/step - loss: 0.5785 - categorical_accuracy: 0.7423\n",
      "Epoch 4/50\n",
      "260/260 [==============================] - 0s 157us/step - loss: 0.5782 - categorical_accuracy: 0.7423\n",
      "Epoch 5/50\n",
      "260/260 [==============================] - 0s 119us/step - loss: 0.5776 - categorical_accuracy: 0.7423\n",
      "Epoch 6/50\n",
      "260/260 [==============================] - 0s 103us/step - loss: 0.5769 - categorical_accuracy: 0.7423\n",
      "Epoch 7/50\n",
      "260/260 [==============================] - 0s 83us/step - loss: 0.5764 - categorical_accuracy: 0.7423\n",
      "Epoch 8/50\n",
      "260/260 [==============================] - 0s 104us/step - loss: 0.5761 - categorical_accuracy: 0.7423\n",
      "Epoch 9/50\n",
      "260/260 [==============================] - 0s 117us/step - loss: 0.5754 - categorical_accuracy: 0.7423\n",
      "Epoch 10/50\n",
      "260/260 [==============================] - 0s 156us/step - loss: 0.5750 - categorical_accuracy: 0.7423\n",
      "Epoch 11/50\n",
      "260/260 [==============================] - 0s 131us/step - loss: 0.5748 - categorical_accuracy: 0.7423\n",
      "Epoch 12/50\n",
      "260/260 [==============================] - 0s 116us/step - loss: 0.5745 - categorical_accuracy: 0.7423\n",
      "Epoch 13/50\n",
      "260/260 [==============================] - 0s 111us/step - loss: 0.5741 - categorical_accuracy: 0.7423\n",
      "Epoch 14/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: 0.5740 - categorical_accuracy: 0.7423\n",
      "Epoch 15/50\n",
      "260/260 [==============================] - 0s 84us/step - loss: 0.5737 - categorical_accuracy: 0.7423\n",
      "Epoch 16/50\n",
      "260/260 [==============================] - 0s 112us/step - loss: 0.5733 - categorical_accuracy: 0.7423\n",
      "Epoch 17/50\n",
      "260/260 [==============================] - 0s 103us/step - loss: 0.5728 - categorical_accuracy: 0.7423\n",
      "Epoch 18/50\n",
      "260/260 [==============================] - 0s 111us/step - loss: 0.5725 - categorical_accuracy: 0.7423\n",
      "Epoch 19/50\n",
      "260/260 [==============================] - 0s 90us/step - loss: 0.5723 - categorical_accuracy: 0.7423\n",
      "Epoch 20/50\n",
      "260/260 [==============================] - 0s 107us/step - loss: 0.5721 - categorical_accuracy: 0.7423\n",
      "Epoch 21/50\n",
      "260/260 [==============================] - 0s 130us/step - loss: 0.5721 - categorical_accuracy: 0.7423\n",
      "Epoch 22/50\n",
      "260/260 [==============================] - 0s 99us/step - loss: 0.5719 - categorical_accuracy: 0.7423\n",
      "Epoch 23/50\n",
      "260/260 [==============================] - 0s 111us/step - loss: 0.5718 - categorical_accuracy: 0.7423\n",
      "Epoch 24/50\n",
      "260/260 [==============================] - 0s 86us/step - loss: 0.5716 - categorical_accuracy: 0.7423\n",
      "Epoch 25/50\n",
      "260/260 [==============================] - 0s 84us/step - loss: 0.5714 - categorical_accuracy: 0.7423\n",
      "Epoch 26/50\n",
      "260/260 [==============================] - 0s 108us/step - loss: 0.5712 - categorical_accuracy: 0.7423\n",
      "Epoch 27/50\n",
      "260/260 [==============================] - 0s 108us/step - loss: 0.5711 - categorical_accuracy: 0.7423\n",
      "Epoch 28/50\n",
      "260/260 [==============================] - 0s 89us/step - loss: 0.5710 - categorical_accuracy: 0.7423\n",
      "Epoch 29/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: 0.5708 - categorical_accuracy: 0.7423\n",
      "Epoch 30/50\n",
      "260/260 [==============================] - 0s 107us/step - loss: 0.5708 - categorical_accuracy: 0.7423\n",
      "Epoch 31/50\n",
      "260/260 [==============================] - 0s 106us/step - loss: 0.5706 - categorical_accuracy: 0.7423\n",
      "Epoch 32/50\n",
      "260/260 [==============================] - 0s 107us/step - loss: 0.5705 - categorical_accuracy: 0.7423\n",
      "Epoch 33/50\n",
      "260/260 [==============================] - 0s 105us/step - loss: 0.5704 - categorical_accuracy: 0.7423\n",
      "Epoch 34/50\n",
      "260/260 [==============================] - 0s 100us/step - loss: 0.5703 - categorical_accuracy: 0.7423\n",
      "Epoch 35/50\n",
      "260/260 [==============================] - 0s 92us/step - loss: 0.5702 - categorical_accuracy: 0.7423\n",
      "Epoch 36/50\n",
      "260/260 [==============================] - 0s 84us/step - loss: 0.5701 - categorical_accuracy: 0.7423\n",
      "Epoch 37/50\n",
      "260/260 [==============================] - 0s 104us/step - loss: 0.5701 - categorical_accuracy: 0.7423\n",
      "Epoch 38/50\n",
      "260/260 [==============================] - 0s 104us/step - loss: 0.5700 - categorical_accuracy: 0.7423\n",
      "Epoch 39/50\n",
      "260/260 [==============================] - 0s 82us/step - loss: 0.5700 - categorical_accuracy: 0.7423\n",
      "Epoch 40/50\n",
      "260/260 [==============================] - 0s 87us/step - loss: 0.5701 - categorical_accuracy: 0.7423\n",
      "Epoch 41/50\n",
      "260/260 [==============================] - 0s 100us/step - loss: 0.5700 - categorical_accuracy: 0.7423\n",
      "Epoch 42/50\n",
      "260/260 [==============================] - 0s 104us/step - loss: 0.5700 - categorical_accuracy: 0.7423\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260/260 [==============================] - 0s 117us/step - loss: 0.5700 - categorical_accuracy: 0.7423\n",
      "Epoch 44/50\n",
      "260/260 [==============================] - 0s 98us/step - loss: 0.5700 - categorical_accuracy: 0.7423\n",
      "Epoch 45/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "Epoch 46/50\n",
      "260/260 [==============================] - 0s 93us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "Epoch 47/50\n",
      "260/260 [==============================] - 0s 128us/step - loss: 0.5700 - categorical_accuracy: 0.7423\n",
      "Epoch 48/50\n",
      "260/260 [==============================] - 0s 112us/step - loss: 0.5700 - categorical_accuracy: 0.7423\n",
      "Epoch 49/50\n",
      "260/260 [==============================] - 0s 102us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "Epoch 50/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "65/65 [==============================] - 0s 78us/step\n",
      "\n",
      "categorical_accuracy: 86.15%\n",
      "Epoch 1/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: 0.5702 - categorical_accuracy: 0.7423\n",
      "Epoch 2/50\n",
      "260/260 [==============================] - 0s 95us/step - loss: 0.5700 - categorical_accuracy: 0.7423\n",
      "Epoch 3/50\n",
      "260/260 [==============================] - 0s 102us/step - loss: 0.5700 - categorical_accuracy: 0.7423\n",
      "Epoch 4/50\n",
      "260/260 [==============================] - 0s 97us/step - loss: 0.5700 - categorical_accuracy: 0.7423\n",
      "Epoch 5/50\n",
      "260/260 [==============================] - 0s 89us/step - loss: 0.5700 - categorical_accuracy: 0.7423\n",
      "Epoch 6/50\n",
      "260/260 [==============================] - 0s 83us/step - loss: 0.5700 - categorical_accuracy: 0.7423\n",
      "Epoch 7/50\n",
      "260/260 [==============================] - 0s 95us/step - loss: 0.5700 - categorical_accuracy: 0.7423\n",
      "Epoch 8/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: 0.5700 - categorical_accuracy: 0.7423\n",
      "Epoch 9/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "Epoch 10/50\n",
      "260/260 [==============================] - 0s 87us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "Epoch 11/50\n",
      "260/260 [==============================] - 0s 83us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "Epoch 12/50\n",
      "260/260 [==============================] - 0s 111us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "Epoch 13/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "Epoch 14/50\n",
      "260/260 [==============================] - 0s 98us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "Epoch 15/50\n",
      "260/260 [==============================] - 0s 107us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "Epoch 16/50\n",
      "260/260 [==============================] - 0s 100us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "Epoch 17/50\n",
      "260/260 [==============================] - 0s 95us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "Epoch 18/50\n",
      "260/260 [==============================] - 0s 92us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "Epoch 19/50\n",
      "260/260 [==============================] - 0s 103us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "Epoch 20/50\n",
      "260/260 [==============================] - 0s 86us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "Epoch 21/50\n",
      "260/260 [==============================] - 0s 101us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "Epoch 22/50\n",
      "260/260 [==============================] - 0s 121us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "Epoch 23/50\n",
      "260/260 [==============================] - 0s 92us/step - loss: 0.5698 - categorical_accuracy: 0.7423\n",
      "Epoch 24/50\n",
      "260/260 [==============================] - 0s 96us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "Epoch 25/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: 0.5698 - categorical_accuracy: 0.7423\n",
      "Epoch 26/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "Epoch 27/50\n",
      "260/260 [==============================] - 0s 102us/step - loss: 0.5698 - categorical_accuracy: 0.7423\n",
      "Epoch 28/50\n",
      "260/260 [==============================] - 0s 111us/step - loss: 0.5698 - categorical_accuracy: 0.7423\n",
      "Epoch 29/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "Epoch 30/50\n",
      "260/260 [==============================] - 0s 108us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "Epoch 31/50\n",
      "260/260 [==============================] - 0s 134us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "Epoch 32/50\n",
      "260/260 [==============================] - 0s 119us/step - loss: 0.5698 - categorical_accuracy: 0.7423\n",
      "Epoch 33/50\n",
      "260/260 [==============================] - 0s 103us/step - loss: 0.5698 - categorical_accuracy: 0.7423\n",
      "Epoch 34/50\n",
      "260/260 [==============================] - 0s 118us/step - loss: 0.5698 - categorical_accuracy: 0.7423\n",
      "Epoch 35/50\n",
      "260/260 [==============================] - 0s 107us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "Epoch 36/50\n",
      "260/260 [==============================] - 0s 114us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "Epoch 37/50\n",
      "260/260 [==============================] - 0s 105us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "Epoch 38/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: 0.5698 - categorical_accuracy: 0.7423\n",
      "Epoch 39/50\n",
      "260/260 [==============================] - 0s 93us/step - loss: 0.5698 - categorical_accuracy: 0.7423\n",
      "Epoch 40/50\n",
      "260/260 [==============================] - 0s 86us/step - loss: 0.5698 - categorical_accuracy: 0.7423\n",
      "Epoch 41/50\n",
      "260/260 [==============================] - 0s 133us/step - loss: 0.5698 - categorical_accuracy: 0.7423\n",
      "Epoch 42/50\n",
      "260/260 [==============================] - 0s 111us/step - loss: 0.5698 - categorical_accuracy: 0.7423\n",
      "Epoch 43/50\n",
      "260/260 [==============================] - 0s 107us/step - loss: 0.5698 - categorical_accuracy: 0.7423\n",
      "Epoch 44/50\n",
      "260/260 [==============================] - 0s 101us/step - loss: 0.5698 - categorical_accuracy: 0.7423\n",
      "Epoch 45/50\n",
      "260/260 [==============================] - 0s 86us/step - loss: 0.5698 - categorical_accuracy: 0.7423\n",
      "Epoch 46/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: 0.5699 - categorical_accuracy: 0.7423\n",
      "Epoch 47/50\n",
      "260/260 [==============================] - 0s 116us/step - loss: 0.5698 - categorical_accuracy: 0.7423\n",
      "Epoch 48/50\n",
      "260/260 [==============================] - 0s 92us/step - loss: 0.5698 - categorical_accuracy: 0.7423\n",
      "Epoch 49/50\n",
      "260/260 [==============================] - 0s 100us/step - loss: 0.5698 - categorical_accuracy: 0.7423\n",
      "Epoch 50/50\n",
      "260/260 [==============================] - 0s 106us/step - loss: 0.5698 - categorical_accuracy: 0.7423\n",
      "65/65 [==============================] - 0s 103us/step\n",
      "\n",
      "categorical_accuracy: 86.15%\n",
      "Epoch 1/50\n",
      "260/260 [==============================] - 0s 61us/step - loss: 0.5910 - categorical_accuracy: 0.7231\n",
      "Epoch 2/50\n",
      "260/260 [==============================] - 0s 61us/step - loss: 0.5910 - categorical_accuracy: 0.7231\n",
      "Epoch 3/50\n",
      "260/260 [==============================] - 0s 77us/step - loss: 0.5909 - categorical_accuracy: 0.7231\n",
      "Epoch 4/50\n",
      "260/260 [==============================] - 0s 65us/step - loss: 0.5909 - categorical_accuracy: 0.7231\n",
      "Epoch 5/50\n",
      "260/260 [==============================] - 0s 68us/step - loss: 0.5909 - categorical_accuracy: 0.7231\n",
      "Epoch 6/50\n",
      "260/260 [==============================] - 0s 69us/step - loss: 0.5906 - categorical_accuracy: 0.7231\n",
      "Epoch 7/50\n",
      "260/260 [==============================] - 0s 61us/step - loss: 0.5906 - categorical_accuracy: 0.7231\n",
      "Epoch 8/50\n",
      "260/260 [==============================] - 0s 73us/step - loss: 0.5906 - categorical_accuracy: 0.7231\n",
      "Epoch 9/50\n",
      "260/260 [==============================] - 0s 63us/step - loss: 0.5906 - categorical_accuracy: 0.7231\n",
      "Epoch 10/50\n",
      "260/260 [==============================] - 0s 101us/step - loss: 0.5906 - categorical_accuracy: 0.7231\n",
      "Epoch 11/50\n",
      "260/260 [==============================] - 0s 78us/step - loss: 0.5905 - categorical_accuracy: 0.7231\n",
      "Epoch 12/50\n",
      "260/260 [==============================] - 0s 67us/step - loss: 0.5904 - categorical_accuracy: 0.7231\n",
      "Epoch 13/50\n",
      "260/260 [==============================] - 0s 60us/step - loss: 0.5903 - categorical_accuracy: 0.7231\n",
      "Epoch 14/50\n",
      "260/260 [==============================] - 0s 63us/step - loss: 0.5903 - categorical_accuracy: 0.7231\n",
      "Epoch 15/50\n",
      "260/260 [==============================] - 0s 75us/step - loss: 0.5903 - categorical_accuracy: 0.7231\n",
      "Epoch 16/50\n",
      "260/260 [==============================] - 0s 75us/step - loss: 0.5903 - categorical_accuracy: 0.7231\n",
      "Epoch 17/50\n",
      "260/260 [==============================] - 0s 65us/step - loss: 0.5902 - categorical_accuracy: 0.7231\n",
      "Epoch 18/50\n",
      "260/260 [==============================] - 0s 65us/step - loss: 0.5903 - categorical_accuracy: 0.7231\n",
      "Epoch 19/50\n",
      "260/260 [==============================] - 0s 61us/step - loss: 0.5901 - categorical_accuracy: 0.7231\n",
      "Epoch 20/50\n",
      "260/260 [==============================] - 0s 100us/step - loss: 0.5901 - categorical_accuracy: 0.7231\n",
      "Epoch 21/50\n",
      "260/260 [==============================] - 0s 65us/step - loss: 0.5901 - categorical_accuracy: 0.7231\n",
      "Epoch 22/50\n",
      "260/260 [==============================] - 0s 63us/step - loss: 0.5900 - categorical_accuracy: 0.7231\n",
      "Epoch 23/50\n",
      "260/260 [==============================] - 0s 60us/step - loss: 0.5902 - categorical_accuracy: 0.7231\n",
      "Epoch 24/50\n",
      "260/260 [==============================] - 0s 81us/step - loss: 0.5900 - categorical_accuracy: 0.7231\n",
      "Epoch 25/50\n",
      "260/260 [==============================] - 0s 69us/step - loss: 0.5900 - categorical_accuracy: 0.7231\n",
      "Epoch 26/50\n",
      "260/260 [==============================] - 0s 85us/step - loss: 0.5900 - categorical_accuracy: 0.7231\n",
      "Epoch 27/50\n",
      "260/260 [==============================] - 0s 77us/step - loss: 0.5900 - categorical_accuracy: 0.7231\n",
      "Epoch 28/50\n",
      "260/260 [==============================] - 0s 84us/step - loss: 0.5901 - categorical_accuracy: 0.7231\n",
      "Epoch 29/50\n",
      "260/260 [==============================] - 0s 69us/step - loss: 0.5900 - categorical_accuracy: 0.7231\n",
      "Epoch 30/50\n",
      "260/260 [==============================] - 0s 239us/step - loss: 0.5900 - categorical_accuracy: 0.7231\n",
      "Epoch 31/50\n",
      "260/260 [==============================] - 0s 66us/step - loss: 0.5900 - categorical_accuracy: 0.7231\n",
      "Epoch 32/50\n",
      "260/260 [==============================] - 0s 61us/step - loss: 0.5900 - categorical_accuracy: 0.7231\n",
      "Epoch 33/50\n",
      "260/260 [==============================] - 0s 73us/step - loss: 0.5901 - categorical_accuracy: 0.7231\n",
      "Epoch 34/50\n",
      "260/260 [==============================] - 0s 61us/step - loss: 0.5900 - categorical_accuracy: 0.7231\n",
      "Epoch 35/50\n",
      "260/260 [==============================] - 0s 61us/step - loss: 0.5900 - categorical_accuracy: 0.7231\n",
      "Epoch 36/50\n",
      "260/260 [==============================] - 0s 61us/step - loss: 0.5900 - categorical_accuracy: 0.7231\n",
      "Epoch 37/50\n",
      "260/260 [==============================] - 0s 83us/step - loss: 0.5900 - categorical_accuracy: 0.7231\n",
      "Epoch 38/50\n",
      "260/260 [==============================] - 0s 64us/step - loss: 0.5901 - categorical_accuracy: 0.7231\n",
      "Epoch 39/50\n",
      "260/260 [==============================] - 0s 78us/step - loss: 0.5901 - categorical_accuracy: 0.7231\n",
      "Epoch 40/50\n",
      "260/260 [==============================] - 0s 59us/step - loss: 0.5900 - categorical_accuracy: 0.7231\n",
      "Epoch 41/50\n",
      "260/260 [==============================] - 0s 80us/step - loss: 0.5900 - categorical_accuracy: 0.7231\n",
      "Epoch 42/50\n",
      "260/260 [==============================] - 0s 96us/step - loss: 0.5901 - categorical_accuracy: 0.7231\n",
      "Epoch 43/50\n",
      "260/260 [==============================] - 0s 78us/step - loss: 0.5900 - categorical_accuracy: 0.7231\n",
      "Epoch 44/50\n",
      "260/260 [==============================] - 0s 88us/step - loss: 0.5900 - categorical_accuracy: 0.7231\n",
      "Epoch 45/50\n",
      "260/260 [==============================] - 0s 65us/step - loss: 0.5900 - categorical_accuracy: 0.7231\n",
      "Epoch 46/50\n",
      "260/260 [==============================] - 0s 77us/step - loss: 0.5900 - categorical_accuracy: 0.7231\n",
      "Epoch 47/50\n",
      "260/260 [==============================] - 0s 61us/step - loss: 0.5901 - categorical_accuracy: 0.7231\n",
      "Epoch 48/50\n",
      "260/260 [==============================] - 0s 69us/step - loss: 0.5900 - categorical_accuracy: 0.7231\n",
      "Epoch 49/50\n",
      "260/260 [==============================] - 0s 80us/step - loss: 0.5900 - categorical_accuracy: 0.7231\n",
      "Epoch 50/50\n",
      "260/260 [==============================] - 0s 100us/step - loss: 0.5900 - categorical_accuracy: 0.7231\n",
      "65/65 [==============================] - 0s 92us/step\n",
      "\n",
      "categorical_accuracy: 93.85%\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "# Use softmax as activation function\n",
    "model2.add(Dense(10, activation='softmax', input_dim=numFeatures))\n",
    "model2.add(Dense(5, activation='softmax'))\n",
    "model2.add(Dense(2, activation='sigmoid'))\n",
    "model2.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "            metrics=['categorical_accuracy'])\n",
    "\n",
    "acc2 = logstic(encodedData, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical_accuracy history is: [0.6615384817123413, 0.5076923370361328, 0.8615384697914124, 0.8615384697914124, 0.9384615421295166]\n",
      "The average accuracy is 76.62%\n"
     ]
    }
   ],
   "source": [
    "print('Categorical_accuracy history is:', acc2)\n",
    "print('The average accuracy is {:.2%}'.format(np.sum(acc2)/5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Future Direction\n",
    "\n",
    "The result shows the accuracy of predicting of Maximum Entropy Classifier and Logistic Regression similar. Both of them in our test are above 70%. However, we cannot find the features of the greatest weight by comparing the result after removing different features. Due to limited time to labeling documents outdated or not manually, our sample size is small, so the result seems not to be ideal. Creating a sample datasets for training will be a furture task of us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "[1]:  Miles Osborne, Using Maximum Entropy for Sentence Extraction, 2002\n",
    "\n",
    "[2]:  John Mount, The equivalence of logistic regression and maximum entropy models, 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
