{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VTXIH1lbfgbM"
   },
   "source": [
    "# Embedded Topic Model\n",
    "\n",
    "#### Dejan Milacic, Kshirabdhi Patel\n",
    "\n",
    "####  dejan.milacic@ryerson.ca, kshirabdhi.patel@ryerson.ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xr5QdKnTfgbN"
   },
   "source": [
    "# Introduction:\n",
    "\n",
    "#### Problem Description:\n",
    "\n",
    "How to determine the topics present in a set of documents and the mixture of topics discussed in a single document.\n",
    "\n",
    "#### Context of the Problem:\n",
    "\n",
    "Topic modelling can help to organize unlabelled text data and group together documents with similar themes.\n",
    "\n",
    "#### Limitation About other Approaches:\n",
    "\n",
    "Traditional topic models do not take advantage of distributed word representations which can capture semantic similarities between tokens.\n",
    "LDA requires large vocabularies to be pruned severely for a good fit, potentially removing important terms and limiting the scope of the model.\n",
    "\n",
    "#### Solution:\n",
    "\n",
    "The Embedded Topic Model (ETM) uses word embeddings to discover the latent semantic structure of texts and accommodate large vocabularies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oN8LEFeEfgbO"
   },
   "source": [
    "# Background\n",
    "\n",
    "| Reference |Explanation |  Dataset/Input |Weakness\n",
    "| --- | --- | --- | --- |\n",
    "| Moody [1] | lda2vec uses a combination of word and document vectors to model topics | Hacker News comments, 20ng | No quantitative performance measure on Hacker News comments\n",
    "| Dieng et al. [2] | ETM models topics as points in the word embedding space | NYT, 20ng | Future: Could use a parser for more informative tokenized vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sTXWOpdOfgbP"
   },
   "source": [
    "# Methodology\n",
    "\n",
    "<div>\n",
    "<img src=\"https://drive.google.com/uc?id=1Qy0mB14QP0VhUZJ9DZn9OaeT7reRpWrh\" width=\"450\"/>\n",
    "</div>\n",
    "\n",
    "The Embedded Topic Model can use pre-trained word embeddings (e.g. Skip-gram) or learn embeddings during training.\n",
    "\n",
    "The model defines an $L\\times V$ word embedding matrix $\\rho$ where $L$ is the size of the embeddings (= 300) and $V$ is the vocabulary size.\n",
    "\n",
    "The model also learns $K$ (= 50) topic embeddings, defined in an $L\\times K$ embedding matrix $\\alpha$.\n",
    "\n",
    "ETM measures agreement between word embeddings and topic embeddings by taking the inner product of the word embedding matrix and the topic embedding ($\\beta$ above).\n",
    "\n",
    "The marginal likelihood of each document is intractable to compute, so the algorithm uses amortized variational inference.\n",
    "\n",
    "The distributions of the topic proportions of each document depend on the document $d$ and shared variational parameters $\\nu$.\n",
    "Each distribution is a Gaussian whose mean and variance come from an \"inference network,\" a neural network parametrized by $\\nu$.\n",
    "\n",
    "The topic proportions $\\theta$ are then drawn from a logistic-normal distribution with the mean and variance output by the inference network.\n",
    "\n",
    "The evidence lower bound (ELBO) is a function used to train the parameters $\\alpha$, $\\rho$ and $\\nu$:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://drive.google.com/uc?id=1Z1GZBFmBWKf0lvkF6EdIMk0ZfzE_54gc\" width=\"450\"/>\n",
    "</div>\n",
    "\n",
    "As a function of variational parameters $\\nu$, the first term encourages them to place mass on topic proportions $\\delta$ that explain the observed words.\n",
    "The second term encourages them to be close to the prior $p(\\delta_d)$.\n",
    "\n",
    "As a function of model parameters $\\rho$ and $\\alpha$, it maximizes the expected complete log-likelihood.\n",
    "\n",
    "ELBO is optimized using stochastic optimization, taking Monte Carlo approximations of the full gradient through reparameterization.\n",
    "\n",
    "Performance is measured in terms of topic coherence and topic diversity.\n",
    "Topic coherence is the average pointwise mutual information of two words drawn randomly from the same document. \n",
    "Topic diversity is the percentage of unique words in the top 25 words of all topics.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YL2d-HQ-fgbQ"
   },
   "source": [
    "# Implementation\n",
    "\n",
    "In this section, you will provide the code and its explanation. You may have to create more cells after this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3J0UI3I2fgbQ"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch.nn.functional as F \n",
    "import pickle\n",
    "import scipy.io\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import sys\n",
    "import matplotlib.pyplot as plt \n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from etm import ETM\n",
    "import re\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import string\n",
    "import gensim\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7BoeTUPkfgbU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('\\n')\n",
    "np.random.seed(2019)\n",
    "torch.manual_seed(2019)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sKIH1ClwfgbX"
   },
   "outputs": [],
   "source": [
    "path = 'C:/Users/amitp/OneDrive/Ryerson-DS/DS8008/project-nlp/project_code'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the Preprocessed data data\n",
    "def _fetch(path, name):\n",
    "    if name == 'train':\n",
    "        token_file = os.path.join(path, 'bow_tr_tokens')\n",
    "        count_file = os.path.join(path, 'bow_tr_counts')\n",
    "    elif name == 'valid':\n",
    "        token_file = os.path.join(path, 'bow_va_tokens')\n",
    "        count_file = os.path.join(path, 'bow_va_counts')\n",
    "    else:\n",
    "        token_file = os.path.join(path, 'bow_ts_tokens')\n",
    "        count_file = os.path.join(path, 'bow_ts_counts')\n",
    "    tokens = scipy.io.loadmat(token_file)['tokens'].squeeze()\n",
    "    counts = scipy.io.loadmat(count_file)['counts'].squeeze()\n",
    "    if name == 'test':\n",
    "        token_1_file = os.path.join(path, 'bow_ts_h1_tokens')\n",
    "        count_1_file = os.path.join(path, 'bow_ts_h1_counts')\n",
    "        token_2_file = os.path.join(path, 'bow_ts_h2_tokens')\n",
    "        count_2_file = os.path.join(path, 'bow_ts_h2_counts')\n",
    "        tokens_1 = scipy.io.loadmat(token_1_file)['tokens'].squeeze()\n",
    "        counts_1 = scipy.io.loadmat(count_1_file)['counts'].squeeze()\n",
    "        tokens_2 = scipy.io.loadmat(token_2_file)['tokens'].squeeze()\n",
    "        counts_2 = scipy.io.loadmat(count_2_file)['counts'].squeeze()\n",
    "        return {'tokens': tokens, 'counts': counts, \n",
    "                    'tokens_1': tokens_1, 'counts_1': counts_1, \n",
    "                        'tokens_2': tokens_2, 'counts_2': counts_2}\n",
    "    return {'tokens': tokens, 'counts': counts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = _fetch(path, 'train')\n",
    "test = _fetch(path, 'test')\n",
    "valid = _fetch(path, 'valid')\n",
    "# 1. training data\n",
    "train_tokens = train['tokens']\n",
    "train_counts = train['counts']\n",
    "num_docs_train = len(train_tokens)\n",
    "\n",
    "# 2. dev set\n",
    "valid_tokens = valid['tokens']\n",
    "valid_counts = valid['counts']\n",
    "num_docs_valid = len(valid_tokens)\n",
    "\n",
    "# 3. test data\n",
    "test_tokens = test['tokens']\n",
    "test_counts = test['counts']\n",
    "num_docs_test = len(test_tokens)\n",
    "\n",
    "test_1_tokens = test['tokens_1']\n",
    "test_1_counts = test['counts_1']\n",
    "num_docs_test_1 = len(test_1_tokens)\n",
    "\n",
    "test_2_tokens = test['tokens_2']\n",
    "test_2_counts = test['counts_2']\n",
    "num_docs_test_2 = len(test_2_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['embedded', 'erzurum', 'hypocritical', 'acc', 'vgalogo', 'smithsonian', 'friendly', 'yk', 'hollow', 'twisto']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18627"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the data(vocabulary v)\n",
    "with open(os.path.join(path, 'vocab.pkl'), 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "print(vocab[0:10])\n",
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 2 choice \n",
    "    1. To use pre trained embedding for words \n",
    "    2. create the embedding for words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we choose to use the pre trained word2vec skip-gram model for word embeddings the the below block of code will be executed . every word will be a vector of 300 dimensions. \n",
    "- embedding.txt is the embedding file generated by the word_embadding.ipynb file. we will be using this file to fit vector embedding for vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_path = path+\"/embeddings.txt\"\n",
    "emb_size = 300\n",
    "\n",
    "# decide embedding\n",
    "train_embeddings = True\n",
    "mode = 'Train'  # mode = eval if train_embeddings = False else mode = 'Train'\n",
    "# if using the word2vect pretrained embadding then:\n",
    "# if using the pre trained embadding \n",
    "embeddings = None\n",
    "if not train_embeddings:\n",
    "    \n",
    "    vectors = {}\n",
    "    with open(emb_path, encoding=\"ISO-8859-1\") as f:\n",
    "        for l in f:\n",
    "            line = l.split()\n",
    "            word = line[0]\n",
    "            if word in vocab:\n",
    "                vect = np.array(line[1:]).astype(np.float)\n",
    "                vectors[word] = vect\n",
    "    embeddings = np.zeros((vocab_size, emb_size))\n",
    "    words_found = 0\n",
    "    for i, word in enumerate(vocab):\n",
    "        try: \n",
    "            embeddings[i] = vectors[word]\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            embeddings[i] = np.random.normal(scale=0.6, size=(emb_size, ))\n",
    "    embeddings = torch.from_numpy(embeddings).to(device)\n",
    "    embeddings_dim = embeddings.size()\n",
    "\n",
    "    print('finished...................................\\n')\n",
    "    print(embeddings.size())\n",
    "    print(embeddings[0].size()) # 300 features for every word in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the activation function which will be used for training the neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose activation function for the model\n",
    "def get_activation( act):\n",
    "    if act == 'tanh':\n",
    "        act = nn.Tanh()\n",
    "    elif act == 'relu':\n",
    "        act = nn.ReLU()\n",
    "    elif act == 'softplus':\n",
    "        act = nn.Softplus()\n",
    "    elif act == 'rrelu':\n",
    "        act = nn.RReLU()\n",
    "    elif act == 'leakyrelu':\n",
    "        act = nn.LeakyReLU()\n",
    "    elif act == 'elu':\n",
    "        act = nn.ELU()\n",
    "    elif act == 'selu':\n",
    "        act = nn.SELU()\n",
    "    elif act == 'glu':\n",
    "        act = nn.GLU()\n",
    "    else:\n",
    "        print('Defaulting to tanh activations...')\n",
    "        act = nn.Tanh()\n",
    "    return act "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All the hyper parameters used by ETM model. \n",
    "- For detail visit ETM.py file.\n",
    "- this include functions for computing:\n",
    "    - Topic embedding\n",
    "    - Word embadding\n",
    "    - The variational distribution network\n",
    "    - function to optimize network parameters like theta_d,mean,standard dev\n",
    "    - generation of Gaussian distribution for topic whose mean and variance come from an “inference network,”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETM Implimentation\n",
    "# Decide all the hyperparameters\n",
    "num_topics = 50\n",
    "t_hidden_size = 1200 # neuron in hidden layer\n",
    "rho_size = 300 # wordembedding dimension size. a matrix whose columns contain the embedding representations of the vocabulary\n",
    "theta_act = get_activation( \"tanh\")\n",
    "enc_drop = 0.5  # dropout rate on encoder\n",
    "model = ETM(num_topics, vocab_size, t_hidden_size, rho_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose optimizer to use for all the  neural networks\n",
    "wdecay = 1.2e-6\n",
    "lr = 0.005 # learning rate\n",
    "\n",
    "def opt(optimizer):\n",
    "    if optimizer == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "    elif optimizer == 'adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "    elif optimizer == 'adadelta':\n",
    "        optimizer = optim.Adadelta(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "    elif optimizer == 'asgd':\n",
    "        optimizer = optim.ASGD(model.parameters(), lr=lr, t0=0, lambd=0., weight_decay=wdecay)\n",
    "    else:\n",
    "        print('Defaulting to vanilla SGD')\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    return optimizer\n",
    "optimizer = opt('adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to split the formated data to bath of some hundreds tensor\n",
    "def get_batch(tokens, counts, ind, vocab_size, device, emsize=300):\n",
    "    \"\"\"fetch input data by batch.\"\"\"\n",
    "    batch_size = len(ind)\n",
    "    data_batch = np.zeros((batch_size, vocab_size))\n",
    "    \n",
    "    for i, doc_id in enumerate(ind):\n",
    "        doc = tokens[doc_id]\n",
    "        count = counts[doc_id]\n",
    "        L = count.shape[1]\n",
    "        if len(doc) == 1: \n",
    "            doc = [doc.squeeze()]\n",
    "            count = [count.squeeze()]\n",
    "        else:\n",
    "            doc = doc.squeeze()\n",
    "            count = count.squeeze()\n",
    "        if doc_id != -1:\n",
    "            for j, word in enumerate(doc):\n",
    "                data_batch[i, word] = count[j]\n",
    "    data_batch = torch.from_numpy(data_batch).float().to(device)\n",
    "    return data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training hyperparameters\n",
    "\n",
    "batch_size = 1000\n",
    "bow_norm = True  # Decide whether to use normalized representation of bag of words or as it is\n",
    "clip = 0.0       # gradient clipping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if we are not using the pretrained embedding the the model weights will be used as embeddings vector.\n",
    "- this function learns the weights of embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train() \n",
    "    acc_loss = 0\n",
    "    acc_kl_theta_loss = 0\n",
    "    cnt = 0\n",
    "    indices = torch.randperm(num_docs_train)  # random number generator\n",
    "    indices = torch.split(indices, batch_size) # split into chunk of size batch_size\n",
    "    for idx, ind in enumerate(indices):\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        data_batch = get_batch(train_tokens, train_counts, ind, vocab_size, device) # batch of hundreds\n",
    "        sums = data_batch.sum(1).unsqueeze(1) # sum for normalisation\n",
    "        if bow_norm:\n",
    "            normalized_data_batch = data_batch / sums\n",
    "        else:\n",
    "            normalized_data_batch = data_batch\n",
    "        recon_loss, kld_theta = model(data_batch, normalized_data_batch)\n",
    "        total_loss = recon_loss + kld_theta\n",
    "        total_loss.backward()\n",
    "        \n",
    "\n",
    "        if clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        acc_loss += torch.sum(recon_loss).item()\n",
    "        acc_kl_theta_loss += torch.sum(kld_theta).item()\n",
    "        cnt += 1\n",
    "    \n",
    "    cur_loss = round(acc_loss / cnt, 2) \n",
    "    cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "    cur_real_loss = round(cur_loss + cur_kl_theta, 2)\n",
    "    print('*'*100)\n",
    "    print('Epoch----->{} .. LR: {} .. KL_theta: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "            epoch, optimizer.param_groups[0]['lr'], cur_kl_theta, cur_loss, cur_real_loss))\n",
    "    print('*'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This function extract the nearby words surrounded by the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word visualisation in embedding space\n",
    "\n",
    "def nearest_neighbors(word, embeddings, vocab):\n",
    "    vectors = embeddings.data.cpu().numpy() \n",
    "    index = vocab.index(word)\n",
    "    #print('vectors: ', vectors.shape)\n",
    "    query = vectors[index]\n",
    "    #print('query: ', query.shape)\n",
    "    ranks = vectors.dot(query).squeeze()\n",
    "    denom = query.T.dot(query).squeeze()\n",
    "    denom = denom * np.sum(vectors**2, 1)\n",
    "    denom = np.sqrt(denom)\n",
    "    ranks = ranks / denom\n",
    "    mostSimilar = []\n",
    "    [mostSimilar.append(idx) for idx in ranks.argsort()[::-1]]\n",
    "    nearest_neighbors = mostSimilar[:20]\n",
    "    nearest_neighbors = [vocab[comp] for comp in nearest_neighbors]\n",
    "    return nearest_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 20   #number of words for topic viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(m, show_emb=True):\n",
    "    m.eval()\n",
    "\n",
    "    queries = ['andrew', 'computer', 'sports', 'religion', 'man', 'love', \n",
    "                'intelligence', 'money', 'politics', 'health', 'people', 'family']\n",
    "\n",
    "    ## visualize topics using monte carlo\n",
    "    with torch.no_grad():\n",
    "        print('**.'*30)\n",
    "        print('Visualize topics...\\n')\n",
    "        topics_words = []\n",
    "        gammas = m.get_beta()\n",
    "        for k in range(num_topics):\n",
    "            gamma = gammas[k]\n",
    "            top_words = list(gamma.cpu().numpy().argsort()[-num_words+1:][::-1])\n",
    "            topic_words = [vocab[a] for a in top_words]\n",
    "            topics_words.append(' '.join(topic_words))\n",
    "            print('Topic {}: {}'.format(k, topic_words))\n",
    "\n",
    "        if show_emb:\n",
    "            ## visualize word embeddings by using V to get nearest neighbors\n",
    "            print('\\n','**.'*30)\n",
    "            print('\\nVisualize word embeddings by using output embedding matrix\\n')\n",
    "            try:\n",
    "                embeddings = m.rho.weight  # Vocab_size x E\n",
    "            except:\n",
    "                embeddings = m.rho         # Vocab_size x E\n",
    "            neighbors = []\n",
    "            for word in queries:\n",
    "                print('word: {} .. neighbors:\\n {}\\n'.format(\n",
    "                    word, nearest_neighbors(word, embeddings, vocab)))\n",
    "            print('**.'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here 3 function are written:\n",
    "    1. get_document_frequency - count of number of documents in which particular word exist.\n",
    "    2. get_topic_coherence - average pointwise mutual information of two words drawn randomly from the same document\n",
    "    3. get_topic_diversity - topic diversity is the percentage of unique words in the top 25 words of all topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_frequency(data, wi, wj=None):\n",
    "    if wj is None:\n",
    "        D_wi = 0\n",
    "        for l in range(len(data)):\n",
    "            doc = data[l].squeeze(0)\n",
    "            if len(doc) == 1: \n",
    "                continue\n",
    "            else:\n",
    "                doc = doc.squeeze()\n",
    "            if wi in doc:\n",
    "                D_wi += 1\n",
    "        return D_wi\n",
    "    D_wj = 0\n",
    "    D_wi_wj = 0\n",
    "    for l in range(len(data)):\n",
    "        doc = data[l].squeeze(0)\n",
    "        if len(doc) == 1: \n",
    "            doc = [doc.squeeze()]\n",
    "        else:\n",
    "            doc = doc.squeeze()\n",
    "        if wj in doc:\n",
    "            D_wj += 1\n",
    "            if wi in doc:\n",
    "                D_wi_wj += 1\n",
    "    return D_wj, D_wi_wj \n",
    "\n",
    "def get_topic_coherence(beta, data, vocab):\n",
    "    D = len(data) ## number of docs...data is list of documents\n",
    "    TC = []\n",
    "    num_topics = len(beta)\n",
    "    for k in range(num_topics):\n",
    "        top_10 = list(beta[k].argsort()[-11:][::-1])\n",
    "        TC_k = 0\n",
    "        counter = 0\n",
    "        for i, word in enumerate(top_10):\n",
    "            # get D(w_i)\n",
    "            D_wi = get_document_frequency(data, word)\n",
    "            j = i + 1\n",
    "            tmp = 0\n",
    "            while j < len(top_10) and j > i:\n",
    "                # get D(w_j) and D(w_i, w_j)\n",
    "                D_wj, D_wi_wj = get_document_frequency(data, word, top_10[j])\n",
    "                # get f(w_i, w_j)\n",
    "                if D_wi_wj == 0:\n",
    "                    f_wi_wj = -1\n",
    "                else:\n",
    "                    f_wi_wj = -1 + ( np.log(D_wi) + np.log(D_wj)  - 2.0 * np.log(D) ) / ( np.log(D_wi_wj) - np.log(D) )\n",
    "                # update tmp: \n",
    "                tmp += f_wi_wj\n",
    "                j += 1\n",
    "                counter += 1\n",
    "            # update TC_k\n",
    "            TC_k += tmp \n",
    "        TC.append(TC_k)\n",
    "    TC = np.mean(TC) / counter\n",
    "    print('Topic coherence is: {}'.format(TC))\n",
    "\n",
    "def get_topic_diversity(beta, topk):\n",
    "    num_topics = beta.shape[0]\n",
    "    list_w = np.zeros((num_topics, topk))\n",
    "    for k in range(num_topics):\n",
    "        idx = beta[k,:].argsort()[-topk:][::-1]\n",
    "        list_w[k,:] = idx\n",
    "    n_unique = len(np.unique(list_w))\n",
    "    TD = n_unique / (topk * num_topics)\n",
    "    print('Topic diveristy is: {}'.format(TD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch_size = 1000 #input batch size for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Evaluate the model perfomance:\n",
    "\n",
    "- Topic cohorence \n",
    "- Topic diversity\n",
    "- perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(m, source, tc=False, td=False):\n",
    "    \"\"\"Compute perplexity on document completion.\n",
    "    \"\"\"\n",
    "    m.eval()\n",
    "    with torch.no_grad():\n",
    "        if source == 'val':\n",
    "            indices = torch.split(torch.tensor(range(num_docs_valid)), eval_batch_size)\n",
    "            tokens = valid_tokens\n",
    "            counts = valid_counts\n",
    "        else: \n",
    "            indices = torch.split(torch.tensor(range(num_docs_test)), eval_batch_size)\n",
    "            tokens = test_tokens\n",
    "            counts = test_counts\n",
    "\n",
    "        ## get \\beta here\n",
    "        beta = m.get_beta()\n",
    "\n",
    "        ### do dc and tc here\n",
    "        acc_loss = 0\n",
    "        cnt = 0\n",
    "        indices_1 = torch.split(torch.tensor(range(num_docs_test_1)), eval_batch_size)\n",
    "        for idx, ind in enumerate(indices_1):\n",
    "            ## get theta from first half of docs\n",
    "            data_batch_1 = get_batch(test_1_tokens, test_1_counts, ind, vocab_size, device)\n",
    "            sums_1 = data_batch_1.sum(1).unsqueeze(1)\n",
    "            if bow_norm:\n",
    "                normalized_data_batch_1 = data_batch_1 / sums_1\n",
    "            else:\n",
    "                normalized_data_batch_1 = data_batch_1\n",
    "            theta, _ = m.get_theta(normalized_data_batch_1)\n",
    "\n",
    "            ## get prediction loss using second half\n",
    "            data_batch_2 = get_batch(test_2_tokens, test_2_counts, ind, vocab_size, device)\n",
    "            sums_2 = data_batch_2.sum(1).unsqueeze(1)\n",
    "            res = torch.mm(theta, beta)\n",
    "            preds = torch.log(res)\n",
    "            recon_loss = -(preds * data_batch_2).sum(1)\n",
    "            \n",
    "            loss = recon_loss / sums_2.squeeze()\n",
    "            loss = loss.mean().item()\n",
    "            acc_loss += loss\n",
    "            cnt += 1\n",
    "        cur_loss = acc_loss / cnt\n",
    "        ppl_dc = round(math.exp(cur_loss), 1)\n",
    "        print('*'*100)\n",
    "        print('{} Doc Completion PPL: {}'.format(source.upper(), ppl_dc))\n",
    "        print('*'*100)\n",
    "        if tc or td:\n",
    "            beta = beta.data.cpu().numpy()\n",
    "            if tc:\n",
    "                print('Computing topic coherence...')\n",
    "                get_topic_coherence(beta, train_tokens, vocab)\n",
    "            if td:\n",
    "                print('Computing topic diversity...')\n",
    "                get_topic_diversity(beta, 25)\n",
    "        return ppl_dc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trainning model and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs :\n",
    "epochs =21\n",
    "nonmono = 10 #number of bad hits allowed\n",
    "anneal_lr =0 # whether to anneal the learning rate or not\n",
    "visualize_every =10 #when to visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Visualizing model quality before training...\n",
      "**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.\n",
      "Visualize topics...\n",
      "\n",
      "Topic 0: ['backups', 'october', 'interpreter', 'devoid', 'occupied', 'wait', 'sticker', 'escrow', 'fingers', 'eaten', 'convincing', 'tau', 'bona', 'peice', 'hears', 'petroleum', 'exodus', 'sir', 'shores']\n",
      "Topic 1: ['bream', 'adelaide', 'consultant', 'administrators', 'dichotomy', 'ul', 'individual', 'murray', 'pittsburgh', 'legends', 'pasadena', 'plugging', 'outlined', 'wsnc', 'sinned', 'assurances', 'dragged', 'mob', 'utility']\n",
      "Topic 2: ['distinguish', 'fled', 'gnuplot', 'ku', 'restaurant', 'waits', 'lakers', 'galileo', 'problematic', 'placement', 'unreadable', 'wireframe', 'vus', 'laird', 'constraints', 'alaska', 'someones', 'stewart', 'negotiated']\n",
      "Topic 3: ['breeding', 'pif', 'sherri', 'disobedience', 'edwards', 'evasive', 'corvette', 'willis', 'cdac', 'residence', 'turkish', 'behold', 'dug', 'broadly', 'penetrate', 'retreat', 'pitchers', 'religions', 'quicktime']\n",
      "Topic 4: ['restaurant', 'dl', 'picked', 'concepts', 'branches', 'computational', 'ottawa', 'dreaming', 'sri', 'holloway', 'garry', 'jd', 'cosmetic', 'accepted', 'garpenlov', 'butler', 'marrying', 'tate', 'champs']\n",
      "Topic 5: ['cheaply', 'iris', 'proponent', 'hammerl', 'manufacturing', 'idea', 'friction', 'capacitors', 'defeated', 'ancestor', 'fbi', 'der', 'crispin', 'fame', 'anwar', 'prefers', 'embedded', 'drifted', 'kariya']\n",
      "Topic 6: ['convert', 'borrowed', 'billy', 'safer', 'intend', 'involved', 'fpu', 'alchemy', 'qucdn', 'insightful', 'immaturity', 'favourite', 'maintainer', 'deion', 'tcora', 'video', 'divides', 'casares', 'dw']\n",
      "Topic 7: ['explode', 'cowboy', 'kirlian', 'chastity', 'timing', 'nurses', 'visualisation', 'withdrawal', 'processors', 'increase', 'routine', 'microcontroller', 'dogs', 'di', 'claremont', 'refuted', 'mik', 'trustworthy', 'competence']\n",
      "Topic 8: ['bitnet', 'glendenning', 'lovely', 'flashes', 'philosopher', 'buddhists', 'knock', 'speaker', 'marxhausen', 'techniques', 'sheltered', 'likud', 'nurses', 'venus', 'ont', 'propriety', 'unchanged', 'longest', 'infrared']\n",
      "Topic 9: ['comm', 'jimf', 'andy', 'eder', 'ethan', 'essay', 'cap', 'mater', 'officers', 'chico', 'france', 'warren', 'sussex', 'commited', 'tearing', 'denny', 'paying', 'colossians', 'bashed']\n",
      "Topic 10: ['beta', 'jays', 'general', 'committees', 'overpass', 'sanity', 'horrible', 'sterling', 'liner', 'earnest', 'behaviour', 'reread', 'systems', 'listings', 'citation', 'rodan', 'demonstrates', 'professed', 'ji']\n",
      "Topic 11: ['inria', 'facts', 'ulm', 'ussr', 'inspection', 'priority', 'uceng', 'verify', 'osteopathic', 'lq', 'listserv', 'kpc', 'legislature', 'fac', 'bar', 'simplest', 'latonia', 'surprises', 'laboratories']\n",
      "Topic 12: ['endorses', 'mehl', 'utsa', 'shades', 'boys', 'ctl', 'inferior', 'carney', 'petch', 'nicoll', 'landsat', 'cheat', 'maynard', 'ashton', 'genuinely', 'mace', 'stuttgart', 'base', 'mature']\n",
      "Topic 13: ['counts', 'thruster', 'gxxor', 'achieve', 'ching', 'azw', 'tick', 'firebird', 'rapid', 'condemns', 'plotting', 'virtues', 'ankle', 'bungled', 'feeling', 'bosco', 'aio', 'bigdog', 'ranging']\n",
      "Topic 14: ['subsidies', 'steals', 'cygnus', 'examinations', 'hearts', 'champaign', 'valid', 'redistribute', 'olaf', 'senses', 'migraine', 'zoo', 'supported', 'commanders', 'pangea', 'lcd', 'cars', 'hurt', 'neptune']\n",
      "Topic 15: ['davidians', 'torah', 'bkw', 'visualization', 'allison', 'lid', 'hug', 'jefferson', 'ballgames', 'calories', 'annoying', 'nr', 'designation', 'dave', 'foard', 'ideals', 'rods', 'observatory', 'freshman']\n",
      "Topic 16: ['rock', 'patrick', 'impulses', 'accomplish', 'newbie', 'students', 'julian', 'denotes', 'geoffrey', 'increasingly', 'operate', 'predicts', 'sprites', 'franchise', 'wishes', 'cycle', 'incarnation', 'wu', 'wax']\n",
      "Topic 17: ['mcsun', 'analysis', 'packaged', 'rats', 'gspira', 'cod', 'redirected', 'radically', 'instructor', 'scene', 'squibb', 'rtsg', 'psychiatric', 'retain', 'grant', 'wen', 'todays', 'readiness', 'instability']\n",
      "Topic 18: ['michigan', 'nicholson', 'cunningham', 'flyers', 'initiate', 'town', 'nh', 'egypt', 'ir', 'unixg', 'floppies', 'ih', 'mei', 'canadian', 'classes', 'administer', 'timed', 'electricity', 'propriety']\n",
      "Topic 19: ['gupta', 'ql', 'cast', 'gk', 'catalyst', 'requiring', 'journey', 'advisor', 'scicom', 'wrap', 'revenue', 'juts', 'bureaucracy', 'permitting', 'characters', 'commited', 'dedication', 'passer', 'mhz']\n",
      "Topic 20: ['wheelie', 'exceeding', 'kirk', 'tablets', 'dates', 'worlds', 'tsn', 'xy', 'allocation', 'andersson', 'ruu', 'lustig', 'impossible', 'alan', 'patent', 'conferencing', 'descendants', 'reversed', 'khz']\n",
      "Topic 21: ['pavel', 'vicinity', 'oakhill', 'hatred', 'pip', 'versus', 'adam', 'zenith', 'urc', 'australian', 'ebay', 'breathe', 'figuring', 'bedroom', 'adapters', 'protection', 'accomodate', 'monogamous', 'observed']\n",
      "Topic 22: ['cjackson', 'followups', 'grayscale', 'nanaimo', 'distinct', 'synopsis', 'attmail', 'comm', 'dart', 'premeditated', 'uncecs', 'nonprofit', 'oxygen', 'fearful', 'birth', 'mappings', 'rtfm', 'lid', 'dental']\n",
      "Topic 23: ['stereo', 'reviewed', 'pilchuck', 'foolish', 'stored', 'flew', 'disassemble', 'ethnical', 'dn', 'plague', 'airport', 'vary', 'indianapolis', 'fides', 'dsinc', 'limits', 'completly', 'pogo', 'mod']\n",
      "Topic 24: ['major', 'quiz', 'compatibility', 'affects', 'rand', 'desired', 'worried', 'localhost', 'fudge', 'satisfy', 'balkan', 'sentiment', 'grant', 'powerless', 'hemul', 'achievement', 'deion', 'visually', 'usaf']\n",
      "Topic 25: ['exchange', 'flashlight', 'behold', 'jokerit', 'owned', 'jeq', 'fierkelab', 'qwk', 'howell', 'tp', 'dd', 'tentative', 'poll', 'potentially', 'unr', 'orleans', 'discusses', 'aj', 'similar']\n",
      "Topic 26: ['bomb', 'strength', 'csh', 'engaging', 'glenn', 'seagate', 'kuleuven', 'attorney', 'scaling', 'shift', 'interaction', 'micronics', 'developing', 'dug', 'sickening', 'exhibition', 'mcimail', 'dial', 'sir']\n",
      "Topic 27: ['zionists', 'pub', 'repository', 'surya', 'evening', 'transferable', 'reactor', 'drafted', 'swallowing', 'bruchner', 'binah', 'pm', 'estate', 'stylewriter', 'mineral', 'dev', 'david', 'wisconsin', 'unstable']\n",
      "Topic 28: ['stricken', 'shafer', 'lazarus', 'lucifer', 'rtfm', 'uncecs', 'plumbing', 'rack', 'merchant', 'absorption', 'weaklings', 'postfach', 'pilchuck', 'penguins', 'csa', 'doubles', 'carnegie', 'pager', 'terminated']\n",
      "Topic 29: ['alleg', 'susie', 'beemer', 'originate', 'richard', 'fallacy', 'newsweek', 'kstar', 'veal', 'detecting', 'ghost', 'proportion', 'symbol', 'crc', 'grabbed', 'scratched', 'mining', 'metropolitan', 'ringing']\n",
      "Topic 30: ['poisonous', 'soaked', 'misuse', 'censors', 'olympic', 'im', 'org', 'selfishness', 'nuts', 'mea', 'signed', 'nova', 'jumbo', 'unambiguous', 'voltmeter', 'amendments', 'advice', 'leftist', 'jcpl']\n",
      "Topic 31: ['bribe', 'bms', 'diesels', 'greece', 'produce', 'bigotry', 'uprising', 'veterans', 'establishes', 'jeremiah', 'configuration', 'marketplace', 'allergies', 'mentions', 'reproduce', 'clockwise', 'stripe', 'cosmetic', 'omniscience']\n",
      "Topic 32: ['made', 'startup', 'mariner', 'married', 'trivial', 'responds', 'blah', 'moron', 'igc', 'ballgame', 'anxious', 'bump', 'napoleon', 'playback', 'tim', 'consulting', 'bruins', 'destined', 'griffith']\n",
      "Topic 33: ['mathematical', 'foil', 'contribution', 'super', 'important', 'brook', 'baptized', 'tears', 'legit', 'caleb', 'scorers', 'hayward', 'real', 'bayonet', 'fetus', 'decompress', 'mehl', 'tiff', 'religous']\n",
      "Topic 34: ['realize', 'neosoft', 'rubble', 'cnsvax', 'farther', 'interception', 'hw', 'utils', 'quicker', 'infrared', 'sat', 'palace', 'sanctions', 'defenses', 'cameras', 'cps', 'unexpected', 'boyd', 'shelter']\n",
      "Topic 35: ['suffer', 'accurate', 'museums', 'arrow', 'guided', 'az', 'esc', 'fiery', 'informatik', 'marks', 'bizarre', 'naturally', 'bully', 'intelligence', 'daredevil', 'irish', 'reject', 'advtech', 'chyang']\n",
      "Topic 36: ['mieux', 'tte', 'width', 'daimi', 'bursters', 'awake', 'barred', 'museum', 'cluster', 'glens', 'nasty', 'comprehend', 'affected', 'temple', 'domi', 'xdm', 'authentication', 'ld', 'ferrari']\n",
      "Topic 37: ['quick', 'ecst', 'scientists', 'duo', 'relying', 'dominion', 'wooden', 'ksc', 'validate', 'emulates', 'cu', 'interactive', 'summaries', 'lin', 'deletions', 'dime', 'thigh', 'syrian', 'didnt']\n",
      "Topic 38: ['proportionately', 'timmbake', 'demands', 'difficulties', 'parallel', 'forgive', 'staffs', 'resultant', 'mei', 'photos', 'ks', 'bream', 'attributable', 'enacted', 'findings', 'conversation', 'faculty', 'flashlight', 'lc']\n",
      "Topic 39: ['cameras', 'erupting', 'cheering', 'dat', 'nt', 'advising', 'das', 'iz', 'poorly', 'logo', 'factual', 'standpoint', 'unsw', 'spends', 'gld', 'sign', 'povray', 'spock', 'emphasizing']\n",
      "Topic 40: ['esteem', 'lower', 'quickdraw', 'qi', 'civilized', 'exports', 'practicing', 'justify', 'eliott', 'walker', 'espn', 'parts', 'geological', 'send', 'educators', 'introduction', 'fsl', 'boat', 'tube']\n",
      "Topic 41: ['priesthood', 'butcher', 'terrible', 'trailer', 'olivetti', 'tasmania', 'header', 'tpm', 'adress', 'exterminate', 'medical', 'rocky', 'liquid', 'brook', 'edward', 'upside', 'propoganda', 'pennies', 'rg']\n",
      "Topic 42: ['med', 'disappears', 'mcdonnell', 'graca', 'enduring', 'utdallas', 'jmh', 'nt', 'bone', 'leland', 'charts', 'dykstra', 'cisco', 'entail', 'absence', 'economy', 'weber', 'seed', 'die']\n",
      "Topic 43: ['loan', 'resurrection', 'advise', 'husband', 'plants', 'trouble', 'innocent', 'shoei', 'reichel', 'gentiles', 'accessing', 'individually', 'holly', 'organism', 'chairman', 'physiology', 'outlined', 'includes', 'distinctions']\n",
      "Topic 44: ['kuhub', 'button', 'loyal', 'leap', 'extracted', 'jennifer', 'sulfur', 'birmingham', 'taylor', 'sen', 'expecting', 'striped', 'retard', 'gibson', 'posing', 'creatures', 'err', 'lucas', 'believers']\n",
      "Topic 45: ['va', 'synthetic', 'micronics', 'taxed', 'contradiction', 'cmuvm', 'finals', 'validation', 'zv', 'glowing', 'sinner', 'macweek', 'loyal', 'carb', 'packaged', 'ranger', 'interview', 'exclusive', 'mystery']\n",
      "Topic 46: ['repairs', 'magnitude', 'rite', 'mainframe', 'assign', 'implementing', 'bursts', 'compress', 'generally', 'antiquity', 'sm', 'fictional', 'fruit', 'manchester', 'maria', 'computes', 'ron', 'till', 'congressman']\n",
      "Topic 47: ['mare', 'jenk', 'compat', 'informed', 'workstations', 'governmental', 'mailer', 'ccastco', 'aristotle', 'tourism', 'goggles', 'gerald', 'zionism', 'faa', 'viewed', 'fetch', 'hurts', 'nyr', 'mortal']\n",
      "Topic 48: ['allocating', 'perfection', 'professed', 'kirby', 'experience', 'foligno', 'grayscale', 'cheap', 'geek', 'explorer', 'canterbury', 'technologies', 'destroys', 'circulating', 'stack', 'willful', 'waited', 'halt', 'minerva']\n",
      "Topic 49: ['carried', 'nextwork', 'overnight', 'day', 'drawer', 'reposting', 'brady', 'shielded', 'cadillac', 'acknowledgement', 'accomplishments', 'prosecution', 'hou', 'lev', 'negotiations', 'dillon', 'siegel', 'vf', 'matt']\n",
      "\n",
      " **.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.\n",
      "\n",
      "Visualize word embeddings by using output embedding matrix\n",
      "\n",
      "word: andrew .. neighbors:\n",
      " ['andrew', 'headlines', 'strategic', 'brahms', 'accept', 'fallible', 'stems', 'infallible', 'gloves', 'fare', 'venerable', 'rtp', 'gassed', 'mindlink', 'annexed', 'proper', 'denis', 'describes', 'possessing', 'wright']\n",
      "\n",
      "word: computer .. neighbors:\n",
      " ['computer', 'tyrants', 'constitutionally', 'alternator', 'gspira', 'risks', 'arrangements', 'junior', 'retirement', 'means', 'atc', 'roughing', 'russian', 'faculty', 'doublespace', 'ernie', 'sewer', 'touches', 'gearing', 'veracity']\n",
      "\n",
      "word: sports .. neighbors:\n",
      " ['sports', 'armenian', 'supported', 'ubvms', 'surround', 'lynn', 'ceccarelli', 'uwovax', 'vermont', 'cbnewsk', 'navy', 'buy', 'resizing', 'extensive', 'hopper', 'relaxed', 'adapted', 'shows', 'sniper', 'stays']\n",
      "\n",
      "word: religion .. neighbors:\n",
      " ['religion', 'korean', 'commie', 'systems', 'historian', 'khoros', 'encrypt', 'private', 'uclink', 'integrate', 'lucy', 'oldest', 'raleigh', 'uts', 'salvage', 'agriculture', 'ef', 'blah', 'draper', 'acknowledging']\n",
      "\n",
      "word: man .. neighbors:\n",
      " ['man', 'otto', 'wuecl', 'wrongdoing', 'noticeably', 'enzo', 'resembling', 'pirate', 'taib', 'ave', 'blasting', 'stake', 'uranus', 'triple', 'fanaticism', 'pagemaker', 'tops', 'jesus', 'patricia', 'astronauts']\n",
      "\n",
      "word: love .. neighbors:\n",
      " ['love', 'philosophies', 'lucid', 'bypass', 'stockpile', 'wear', 'doctoral', 'ancestry', 'behaves', 'stortek', 'numbers', 'fro', 'schmidt', 'byuvm', 'radiosity', 'bedroom', 'mouths', 'appoint', 'gizwt', 'sand']\n",
      "\n",
      "word: intelligence .. neighbors:\n",
      " ['intelligence', 'mercury', 'sunysb', 'counting', 'including', 'ccastco', 'treats', 'risen', 'bidirectional', 'hypothesis', 'ej', 'influential', 'dumping', 'friday', 'beirut', 'spoke', 'affected', 'ur', 'specialized', 'whalers']\n",
      "\n",
      "word: money .. neighbors:\n",
      " ['money', 'jwa', 'err', 'lobbying', 'pass', 'oxford', 'cpsr', 'legit', 'variance', 'occidental', 'luriem', 'hoskyns', 'transarc', 'pristine', 'margin', 'municipal', 'interprets', 'lausanne', 'discount', 'mood']\n",
      "\n",
      "word: politics .. neighbors:\n",
      " ['politics', 'torah', 'tyne', 'bull', 'terziogl', 'investigation', 'homeless', 'workaround', 'span', 'disadvantage', 'tens', 'alfa', 'desperation', 'webo', 'gripes', 'ranges', 'dtp', 'flb', 'bubblejet', 'doright']\n",
      "\n",
      "word: health .. neighbors:\n",
      " ['health', 'fulfilled', 'louray', 'unknowns', 'kovalev', 'registry', 'reichel', 'escrow', 'deem', 'fried', 'abpsoft', 'relativism', 'egyptian', 'cary', 'western', 'flee', 'months', 'pride', 'steinn', 'xserver']\n",
      "\n",
      "word: people .. neighbors:\n",
      " ['people', 'comparative', 'exists', 'sorts', 'dmm', 'arrests', 'rates', 'invisible', 'megatek', 'allied', 'ea', 'keys', 'jmeritt', 'blocks', 'barrasso', 'bruins', 'revenue', 'vanheyningen', 'wordperfect', 'severity']\n",
      "\n",
      "word: family .. neighbors:\n",
      " ['family', 'graphics', 'sponsoring', 'flyers', 'fraction', 'dive', 'object', 'wondering', 'nbc', 'dartmouth', 'rabbits', 'buick', 'marriage', 'pluses', 'occured', 'resist', 'addition', 'unrelated', 'psi', 'misunderstand']\n",
      "\n",
      "**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1153.89 .. NELBO: 1153.96\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 9713.7\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1063.14 .. NELBO: 1063.17\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 7485.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1041.59 .. NELBO: 1041.65\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 7092.9\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.34 .. Rec_loss: 1048.08 .. NELBO: 1048.42\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 6950.6\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.27 .. Rec_loss: 1039.08 .. NELBO: 1039.35\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 6892.9\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.3 .. Rec_loss: 1040.76 .. NELBO: 1041.06\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 6853.4\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.37 .. Rec_loss: 1033.96 .. NELBO: 1034.33\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 6846.5\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 1042.84 .. NELBO: 1043.32\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 6843.2\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 1030.87 .. NELBO: 1031.36\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 6864.8\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.74 .. Rec_loss: 1021.92 .. NELBO: 1022.66\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 6818.6\n",
      "****************************************************************************************************\n",
      "**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.\n",
      "Visualize topics...\n",
      "\n",
      "Topic 0: ['writes', 'article', 'university', 'people', 'posting', 'host', 'nntp', 'time', 'max', 'good', 'ca', 'cs', 'distribution', 'make', 'world', 'system', 'god', 'reply', 'work']\n",
      "Topic 1: ['writes', 'article', 'people', 'university', 'posting', 'host', 'nntp', 'time', 'good', 'max', 'ca', 'cs', 'make', 'system', 'distribution', 'world', 'god', 'reply', 'computer']\n",
      "Topic 2: ['writes', 'article', 'people', 'posting', 'university', 'host', 'nntp', 'time', 'ca', 'good', 'max', 'cs', 'system', 'make', 'world', 'distribution', 'god', 'reply', 'state']\n",
      "Topic 3: ['writes', 'article', 'people', 'posting', 'university', 'host', 'nntp', 'time', 'max', 'good', 'ca', 'cs', 'system', 'make', 'world', 'distribution', 'god', 'reply', 'work']\n",
      "Topic 4: ['writes', 'article', 'university', 'people', 'posting', 'nntp', 'host', 'time', 'max', 'ca', 'good', 'system', 'cs', 'make', 'distribution', 'world', 'god', 'reply', 'state']\n",
      "Topic 5: ['writes', 'article', 'university', 'people', 'posting', 'host', 'nntp', 'time', 'max', 'good', 'ca', 'cs', 'system', 'make', 'distribution', 'world', 'god', 'reply', 'state']\n",
      "Topic 6: ['writes', 'article', 'people', 'posting', 'university', 'host', 'nntp', 'time', 'max', 'good', 'ca', 'make', 'cs', 'distribution', 'system', 'world', 'god', 'reply', 'state']\n",
      "Topic 7: ['writes', 'article', 'people', 'university', 'posting', 'host', 'nntp', 'max', 'time', 'ca', 'good', 'cs', 'make', 'system', 'distribution', 'world', 'god', 'reply', 'state']\n",
      "Topic 8: ['writes', 'article', 'people', 'university', 'posting', 'host', 'nntp', 'time', 'good', 'max', 'ca', 'cs', 'make', 'distribution', 'system', 'world', 'god', 'reply', 'state']\n",
      "Topic 9: ['writes', 'article', 'people', 'university', 'posting', 'host', 'nntp', 'time', 'max', 'good', 'ca', 'cs', 'system', 'distribution', 'make', 'god', 'world', 'reply', 'state']\n",
      "Topic 10: ['writes', 'article', 'people', 'posting', 'university', 'host', 'nntp', 'max', 'time', 'good', 'ca', 'cs', 'system', 'make', 'distribution', 'god', 'world', 'reply', 'work']\n",
      "Topic 11: ['writes', 'article', 'university', 'people', 'posting', 'host', 'nntp', 'time', 'good', 'ca', 'max', 'system', 'make', 'cs', 'distribution', 'world', 'god', 'state', 'work']\n",
      "Topic 12: ['writes', 'article', 'people', 'university', 'posting', 'host', 'nntp', 'time', 'max', 'ca', 'good', 'cs', 'system', 'distribution', 'make', 'world', 'god', 'work', 'state']\n",
      "Topic 13: ['writes', 'article', 'university', 'people', 'posting', 'host', 'nntp', 'time', 'max', 'good', 'ca', 'cs', 'system', 'make', 'distribution', 'world', 'god', 'reply', 'work']\n",
      "Topic 14: ['writes', 'article', 'max', 'people', 'university', 'posting', 'host', 'nntp', 'time', 'good', 'ca', 'cs', 'system', 'make', 'distribution', 'world', 'god', 'reply', 'computer']\n",
      "Topic 15: ['writes', 'article', 'posting', 'people', 'university', 'host', 'nntp', 'time', 'good', 'ca', 'max', 'cs', 'system', 'make', 'distribution', 'world', 'state', 'god', 'reply']\n",
      "Topic 16: ['writes', 'article', 'university', 'people', 'posting', 'host', 'nntp', 'time', 'max', 'ca', 'good', 'cs', 'system', 'distribution', 'make', 'world', 'god', 'reply', 'work']\n",
      "Topic 17: ['writes', 'article', 'people', 'posting', 'university', 'host', 'nntp', 'time', 'max', 'good', 'ca', 'cs', 'system', 'distribution', 'make', 'god', 'world', 'reply', 'work']\n",
      "Topic 18: ['writes', 'article', 'university', 'people', 'posting', 'host', 'nntp', 'time', 'max', 'good', 'ca', 'cs', 'system', 'make', 'distribution', 'world', 'god', 'reply', 'computer']\n",
      "Topic 19: ['writes', 'article', 'university', 'people', 'posting', 'host', 'nntp', 'time', 'max', 'good', 'ca', 'cs', 'system', 'make', 'distribution', 'world', 'god', 'reply', 'work']\n",
      "Topic 20: ['writes', 'article', 'university', 'posting', 'people', 'host', 'nntp', 'time', 'good', 'max', 'ca', 'make', 'cs', 'distribution', 'system', 'world', 'state', 'god', 'work']\n",
      "Topic 21: ['writes', 'article', 'posting', 'university', 'people', 'host', 'nntp', 'time', 'max', 'good', 'ca', 'cs', 'system', 'make', 'distribution', 'world', 'god', 'reply', 'work']\n",
      "Topic 22: ['writes', 'article', 'university', 'people', 'posting', 'nntp', 'host', 'time', 'good', 'max', 'ca', 'cs', 'distribution', 'make', 'system', 'god', 'world', 'reply', 'work']\n",
      "Topic 23: ['writes', 'article', 'university', 'people', 'posting', 'nntp', 'host', 'time', 'good', 'max', 'ca', 'cs', 'distribution', 'system', 'make', 'world', 'god', 'state', 'reply']\n",
      "Topic 24: ['writes', 'article', 'people', 'posting', 'university', 'host', 'nntp', 'time', 'good', 'max', 'ca', 'cs', 'make', 'system', 'distribution', 'world', 'god', 'reply', 'work']\n",
      "Topic 25: ['max', 'article', 'writes', 'university', 'posting', 'host', 'people', 'nntp', 'time', 'ca', 'good', 'pl', 'cs', 'distribution', 'system', 'god', 'make', 'world', 'reply']\n",
      "Topic 26: ['writes', 'article', 'people', 'university', 'posting', 'nntp', 'host', 'time', 'good', 'max', 'ca', 'cs', 'distribution', 'system', 'make', 'world', 'god', 'work', 'state']\n",
      "Topic 27: ['writes', 'article', 'university', 'people', 'posting', 'host', 'nntp', 'time', 'max', 'ca', 'good', 'cs', 'system', 'distribution', 'make', 'world', 'god', 'reply', 'work']\n",
      "Topic 28: ['writes', 'article', 'university', 'people', 'posting', 'host', 'nntp', 'time', 'good', 'ca', 'max', 'make', 'cs', 'system', 'distribution', 'world', 'god', 'state', 'reply']\n",
      "Topic 29: ['writes', 'article', 'people', 'university', 'posting', 'host', 'nntp', 'max', 'time', 'good', 'ca', 'make', 'cs', 'distribution', 'world', 'system', 'god', 'reply', 'work']\n",
      "Topic 30: ['writes', 'article', 'posting', 'people', 'university', 'nntp', 'host', 'time', 'max', 'good', 'ca', 'cs', 'system', 'make', 'distribution', 'god', 'world', 'reply', 'work']\n",
      "Topic 31: ['writes', 'article', 'university', 'people', 'posting', 'host', 'nntp', 'max', 'time', 'good', 'ca', 'cs', 'system', 'make', 'distribution', 'world', 'god', 'reply', 'state']\n",
      "Topic 32: ['writes', 'article', 'posting', 'people', 'university', 'nntp', 'host', 'time', 'max', 'good', 'ca', 'cs', 'make', 'distribution', 'system', 'world', 'god', 'work', 'reply']\n",
      "Topic 33: ['writes', 'article', 'people', 'university', 'posting', 'host', 'nntp', 'time', 'good', 'ca', 'max', 'cs', 'system', 'make', 'distribution', 'world', 'god', 'reply', 'state']\n",
      "Topic 34: ['writes', 'article', 'posting', 'people', 'university', 'host', 'nntp', 'time', 'good', 'ca', 'max', 'cs', 'make', 'world', 'system', 'god', 'distribution', 'state', 'reply']\n",
      "Topic 35: ['writes', 'article', 'posting', 'people', 'university', 'host', 'nntp', 'time', 'good', 'ca', 'max', 'cs', 'distribution', 'world', 'make', 'god', 'system', 'reply', 'state']\n",
      "Topic 36: ['writes', 'article', 'max', 'university', 'people', 'posting', 'host', 'nntp', 'time', 'ca', 'good', 'cs', 'system', 'make', 'distribution', 'world', 'god', 'reply', 'computer']\n",
      "Topic 37: ['writes', 'article', 'university', 'people', 'posting', 'host', 'nntp', 'time', 'max', 'good', 'ca', 'cs', 'system', 'make', 'distribution', 'god', 'world', 'reply', 'work']\n",
      "Topic 38: ['writes', 'article', 'people', 'university', 'posting', 'host', 'nntp', 'time', 'max', 'good', 'ca', 'cs', 'distribution', 'system', 'make', 'world', 'god', 'reply', 'work']\n",
      "Topic 39: ['writes', 'article', 'people', 'university', 'posting', 'host', 'nntp', 'time', 'max', 'good', 'ca', 'cs', 'make', 'system', 'distribution', 'world', 'god', 'reply', 'work']\n",
      "Topic 40: ['period', 'cx', 'van', 'pp', 'det', 'ah', 'nj', 'pit', 'stl', 'tor', 'lk', 'chi', 'ma', 'mq', 'mu', 'hz', 'cal', 'uw', 'sj']\n",
      "Topic 41: ['writes', 'article', 'people', 'university', 'posting', 'host', 'nntp', 'time', 'max', 'good', 'ca', 'cs', 'system', 'make', 'distribution', 'world', 'god', 'reply', 'work']\n",
      "Topic 42: ['writes', 'article', 'university', 'people', 'posting', 'host', 'nntp', 'time', 'max', 'good', 'ca', 'cs', 'make', 'system', 'distribution', 'world', 'god', 'work', 'reply']\n",
      "Topic 43: ['writes', 'article', 'people', 'posting', 'university', 'host', 'nntp', 'time', 'max', 'good', 'ca', 'cs', 'system', 'distribution', 'make', 'god', 'world', 'reply', 'work']\n",
      "Topic 44: ['writes', 'article', 'people', 'university', 'posting', 'host', 'nntp', 'max', 'time', 'ca', 'good', 'cs', 'system', 'make', 'distribution', 'world', 'god', 'reply', 'computer']\n",
      "Topic 45: ['writes', 'article', 'people', 'university', 'posting', 'host', 'nntp', 'time', 'max', 'good', 'ca', 'cs', 'distribution', 'make', 'system', 'god', 'world', 'reply', 'state']\n",
      "Topic 46: ['writes', 'article', 'people', 'posting', 'university', 'host', 'nntp', 'time', 'max', 'good', 'ca', 'cs', 'distribution', 'system', 'make', 'world', 'god', 'reply', 'state']\n",
      "Topic 47: ['writes', 'article', 'people', 'university', 'posting', 'host', 'nntp', 'time', 'max', 'good', 'ca', 'cs', 'distribution', 'make', 'system', 'world', 'god', 'reply', 'work']\n",
      "Topic 48: ['writes', 'article', 'people', 'university', 'posting', 'host', 'nntp', 'time', 'max', 'good', 'ca', 'system', 'cs', 'distribution', 'make', 'world', 'god', 'reply', 'computer']\n",
      "Topic 49: ['writes', 'article', 'people', 'university', 'posting', 'host', 'nntp', 'max', 'time', 'ca', 'good', 'cs', 'system', 'distribution', 'make', 'world', 'god', 'reply', 'work']\n",
      "\n",
      " **.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.\n",
      "\n",
      "Visualize word embeddings by using output embedding matrix\n",
      "\n",
      "word: andrew .. neighbors:\n",
      " ['andrew', 'article', 'good', 'writes', 'host', 'time', 'nntp', 'world', 'make', 'cs', 'space', 'posting', 'university', 'work', 'de', 'distribution', 'problem', 'point', 'years', 'information']\n",
      "\n",
      "word: computer .. neighbors:\n",
      " ['computer', 'article', 'posting', 'nntp', 'writes', 'people', 'host', 'good', 'university', 'system', 'make', 'time', 'god', 'problem', 'drive', 'work', 'key', 'reply', 'world', 'back']\n",
      "\n",
      "word: sports .. neighbors:\n",
      " ['sports', 'buy', 'post', 'change', 'bitnet', 'installed', 'long', 'children', 'local', 'top', 'personal', 'room', 'clinton', 'price', 'window', 'car', 'cwru', 'simple', 'source', 'previous']\n",
      "\n",
      "word: religion .. neighbors:\n",
      " ['religion', 'posting', 'good', 'host', 'point', 'systems', 'information', 'back', 'article', 'university', 'work', 'state', 'time', 'software', 'put', 'reply', 'called', 'nntp', 'people', 'writes']\n",
      "\n",
      "word: man .. neighbors:\n",
      " ['man', 'posting', 'writes', 'time', 'article', 'host', 'uk', 'university', 'state', 'usa', 'year', 'make', 'nntp', 'good', 'jesus', 'world', 'news', 'software', 'years', 'cs']\n",
      "\n",
      "word: love .. neighbors:\n",
      " ['love', 'university', 'make', 'article', 'time', 'software', 'problem', 'years', 'writes', 'read', 'posting', 'world', 'people', 'mail', 'computer', 'good', 'information', 'cs', 'part', 'state']\n",
      "\n",
      "word: intelligence .. neighbors:\n",
      " ['intelligence', 'including', 'children', 'anti', 'truth', 'long', 'past', 'days', 'religion', 'live', 'care', 'good', 'mind', 'fax', 'money', 'agree', 'current', 'article', 'great', 'michael']\n",
      "\n",
      "word: money .. neighbors:\n",
      " ['money', 'time', 'people', 'posting', 'university', 'article', 'host', 'writes', 'problem', 'software', 'case', 'distribution', 'state', 'world', 'system', 'government', 'space', 'usa', 'nntp', 'ca']\n",
      "\n",
      "word: politics .. neighbors:\n",
      " ['politics', 'keys', 'christians', 'purpose', 'agree', 'bitnet', 'virginia', 'code', 'net', 'phone', 'dept', 'people', 'means', 'writes', 'insurance', 'nasa', 'law', 'department', 'bad', 'created']\n",
      "\n",
      "word: health .. neighbors:\n",
      " ['health', 'make', 'state', 'good', 'world', 'years', 'writes', 'lot', 'number', 'university', 'posting', 'mark', 'space', 'cs', 'host', 'nntp', 'year', 'team', 'left', 'article']\n",
      "\n",
      "word: people .. neighbors:\n",
      " ['people', 'article', 'host', 'nntp', 'writes', 'system', 'university', 'time', 'god', 'posting', 'good', 'make', 'reply', 'world', 'problem', 'distribution', 'computer', 'ca', 'cs', 'mail']\n",
      "\n",
      "word: family .. neighbors:\n",
      " ['family', 'graphics', 'good', 'writes', 'word', 'number', 'message', 'point', 'world', 'posting', 'team', 'uk', 'time', 'david', 'work', 'back', 'line', 'reason', 'people', 'find']\n",
      "\n",
      "**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.9 .. Rec_loss: 1063.39 .. NELBO: 1064.29\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 6804.3\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 1.24 .. Rec_loss: 1034.9 .. NELBO: 1036.14\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 6789.2\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 1.57 .. Rec_loss: 1022.73 .. NELBO: 1024.3\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 6721.7\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 1.79 .. Rec_loss: 1032.11 .. NELBO: 1033.9\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 6628.3\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 2.59 .. Rec_loss: 1034.69 .. NELBO: 1037.28\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 6515.1\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 3.36 .. Rec_loss: 1009.71 .. NELBO: 1013.07\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 6323.3\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 4.7 .. Rec_loss: 1013.7 .. NELBO: 1018.4\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 6249.2\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 5.14 .. Rec_loss: 1007.1 .. NELBO: 1012.24\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 6145.8\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 5.78 .. Rec_loss: 1014.45 .. NELBO: 1020.23\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 5995.4\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 6.18 .. Rec_loss: 996.56 .. NELBO: 1002.74\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 5937.8\n",
      "****************************************************************************************************\n",
      "**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.\n",
      "Visualize topics...\n",
      "\n",
      "Topic 0: ['writes', 'article', 'people', 'university', 'time', 'good', 'posting', 'space', 'make', 'world', 'state', 'nntp', 'work', 'question', 'host', 'things', 'years', 'thing', 'government']\n",
      "Topic 1: ['writes', 'article', 'people', 'time', 'university', 'good', 'make', 'space', 'posting', 'world', 'work', 'state', 'host', 'nntp', 'question', 'system', 'problem', 'back', 'things']\n",
      "Topic 2: ['writes', 'article', 'posting', 'nntp', 'university', 'host', 'people', 'space', 'good', 'system', 'time', 'distribution', 'make', 'world', 'car', 'state', 'work', 'reply', 'question']\n",
      "Topic 3: ['writes', 'article', 'people', 'university', 'posting', 'good', 'nntp', 'time', 'make', 'world', 'space', 'host', 'state', 'work', 'question', 'car', 'thing', 'science', 'system']\n",
      "Topic 4: ['writes', 'article', 'posting', 'nntp', 'university', 'host', 'distribution', 'ca', 'good', 'reply', 'cs', 'computer', 'time', 'system', 'usa', 'people', 'space', 'world', 'make']\n",
      "Topic 5: ['writes', 'article', 'posting', 'university', 'nntp', 'host', 'good', 'people', 'distribution', 'time', 'space', 'world', 'make', 'reply', 'system', 'state', 'cs', 'ca', 'work']\n",
      "Topic 6: ['people', 'writes', 'article', 'time', 'good', 'make', 'world', 'years', 'state', 'university', 'government', 'things', 'question', 'work', 'point', 'space', 'back', 'thing', 'made']\n",
      "Topic 7: ['writes', 'article', 'posting', 'university', 'host', 'nntp', 'ca', 'distribution', 'good', 'cs', 'reply', 'time', 'computer', 'usa', 'system', 'people', 'world', 'make', 'mail']\n",
      "Topic 8: ['writes', 'article', 'people', 'university', 'posting', 'good', 'nntp', 'time', 'host', 'make', 'world', 'space', 'state', 'work', 'distribution', 'system', 'question', 'reply', 'problem']\n",
      "Topic 9: ['writes', 'article', 'university', 'posting', 'people', 'nntp', 'host', 'good', 'time', 'world', 'state', 'make', 'space', 'distribution', 'reply', 'work', 'system', 'question', 'ca']\n",
      "Topic 10: ['writes', 'article', 'posting', 'university', 'nntp', 'host', 'good', 'people', 'time', 'distribution', 'system', 'make', 'space', 'world', 'reply', 'cs', 'ca', 'work', 'state']\n",
      "Topic 11: ['people', 'time', 'writes', 'article', 'make', 'good', 'world', 'government', 'state', 'things', 'years', 'point', 'question', 'work', 'god', 'fact', 'case', 'made', 'find']\n",
      "Topic 12: ['writes', 'article', 'posting', 'host', 'nntp', 'university', 'ca', 'distribution', 'cs', 'good', 'reply', 'computer', 'time', 'system', 'usa', 'mail', 'world', 'make', 'space']\n",
      "Topic 13: ['writes', 'posting', 'article', 'host', 'nntp', 'university', 'distribution', 'ca', 'computer', 'cs', 'reply', 'system', 'mail', 'good', 'usa', 'time', 'uk', 'gov', 'drive']\n",
      "Topic 14: ['writes', 'posting', 'article', 'host', 'university', 'nntp', 'ca', 'cs', 'distribution', 'good', 'reply', 'computer', 'time', 'usa', 'mail', 'system', 'cc', 'world', 'make']\n",
      "Topic 15: ['people', 'writes', 'article', 'time', 'good', 'make', 'university', 'world', 'state', 'space', 'years', 'question', 'things', 'work', 'government', 'posting', 'thing', 'back', 'long']\n",
      "Topic 16: ['writes', 'article', 'posting', 'university', 'nntp', 'host', 'distribution', 'ca', 'good', 'reply', 'cs', 'time', 'system', 'computer', 'people', 'usa', 'world', 'make', 'space']\n",
      "Topic 17: ['file', 'system', 'windows', 'key', 'files', 'bit', 'scsi', 'data', 'version', 'image', 'software', 'information', 'ftp', 'drive', 'dos', 'server', 'program', 'mit', 'window']\n",
      "Topic 18: ['writes', 'article', 'posting', 'host', 'nntp', 'university', 'ca', 'distribution', 'good', 'cs', 'computer', 'reply', 'system', 'time', 'usa', 'mail', 'make', 'world', 'people']\n",
      "Topic 19: ['writes', 'article', 'university', 'posting', 'nntp', 'host', 'people', 'good', 'time', 'make', 'world', 'distribution', 'reply', 'space', 'state', 'work', 'system', 'cs', 'ca']\n",
      "Topic 20: ['writes', 'article', 'people', 'university', 'good', 'time', 'space', 'make', 'state', 'world', 'posting', 'question', 'work', 'nntp', 'years', 'things', 'thing', 'government', 'long']\n",
      "Topic 21: ['writes', 'article', 'posting', 'host', 'nntp', 'university', 'distribution', 'ca', 'good', 'reply', 'cs', 'system', 'computer', 'time', 'space', 'people', 'mail', 'make', 'world']\n",
      "Topic 22: ['writes', 'article', 'people', 'time', 'university', 'good', 'make', 'world', 'state', 'question', 'space', 'work', 'posting', 'years', 'government', 'things', 'thing', 'point', 'long']\n",
      "Topic 23: ['writes', 'article', 'posting', 'university', 'nntp', 'host', 'people', 'good', 'time', 'distribution', 'world', 'make', 'space', 'state', 'reply', 'system', 'ca', 'work', 'cs']\n",
      "Topic 24: ['writes', 'article', 'posting', 'university', 'nntp', 'host', 'people', 'good', 'time', 'distribution', 'system', 'space', 'make', 'reply', 'world', 'work', 'computer', 'state', 'ca']\n",
      "Topic 25: ['max', 'pl', 'ah', 'giz', 'wm', 'bhj', 'sl', 'mv', 'ma', 'cx', 'tm', 'mr', 'mq', 'ql', 'bj', 'mt', 'mp', 'lk', 'mc']\n",
      "Topic 26: ['writes', 'article', 'people', 'university', 'posting', 'space', 'good', 'nntp', 'time', 'make', 'world', 'state', 'host', 'work', 'question', 'thing', 'car', 'science', 'system']\n",
      "Topic 27: ['posting', 'writes', 'article', 'host', 'nntp', 'university', 'computer', 'distribution', 'ca', 'system', 'mail', 'cs', 'reply', 'drive', 'gov', 'access', 'software', 'uk', 'usa']\n",
      "Topic 28: ['people', 'writes', 'article', 'god', 'time', 'world', 'make', 'good', 'state', 'question', 'government', 'years', 'things', 'gun', 'point', 'life', 'fact', 'law', 'made']\n",
      "Topic 29: ['writes', 'article', 'people', 'time', 'good', 'make', 'university', 'space', 'world', 'state', 'work', 'question', 'posting', 'government', 'things', 'years', 'case', 'thing', 'point']\n",
      "Topic 30: ['writes', 'article', 'posting', 'nntp', 'university', 'host', 'distribution', 'good', 'ca', 'system', 'time', 'reply', 'computer', 'cs', 'people', 'space', 'make', 'world', 'work']\n",
      "Topic 31: ['posting', 'host', 'nntp', 'writes', 'article', 'university', 'computer', 'distribution', 'ca', 'mail', 'cs', 'system', 'drive', 'reply', 'software', 'uk', 'gov', 'access', 'usa']\n",
      "Topic 32: ['writes', 'article', 'posting', 'university', 'nntp', 'host', 'good', 'people', 'distribution', 'time', 'make', 'world', 'space', 'reply', 'state', 'ca', 'system', 'work', 'cs']\n",
      "Topic 33: ['writes', 'article', 'people', 'university', 'time', 'good', 'make', 'space', 'world', 'state', 'posting', 'question', 'work', 'nntp', 'thing', 'years', 'things', 'host', 'long']\n",
      "Topic 34: ['people', 'writes', 'time', 'article', 'make', 'good', 'world', 'government', 'years', 'things', 'state', 'point', 'space', 'work', 'question', 'back', 'university', 'fact', 'case']\n",
      "Topic 35: ['people', 'god', 'jesus', 'law', 'jews', 'time', 'christian', 'christians', 'israel', 'turkish', 'human', 'armenian', 'life', 'world', 'war', 'bible', 'faith', 'gun', 'fact']\n",
      "Topic 36: ['writes', 'article', 'posting', 'host', 'nntp', 'university', 'ca', 'cs', 'distribution', 'good', 'reply', 'computer', 'time', 'usa', 'mail', 'system', 'world', 'make', 'cc']\n",
      "Topic 37: ['writes', 'article', 'posting', 'university', 'host', 'nntp', 'ca', 'distribution', 'good', 'cs', 'reply', 'time', 'system', 'computer', 'mail', 'make', 'people', 'world', 'usa']\n",
      "Topic 38: ['host', 'posting', 'article', 'computer', 'nntp', 'writes', 'system', 'software', 'distribution', 'mail', 'drive', 'university', 'access', 'gov', 'ca', 'cs', 'uk', 'version', 'space']\n",
      "Topic 39: ['writes', 'article', 'posting', 'university', 'nntp', 'host', 'distribution', 'good', 'time', 'people', 'system', 'make', 'cs', 'reply', 'ca', 'space', 'world', 'computer', 'work']\n",
      "Topic 40: ['team', 'year', 'game', 'games', 'ca', 'play', 'db', 'season', 'hockey', 'win', 'good', 'players', 'cs', 'league', 'period', 'nhl', 'teams', 'baseball', 'pittsburgh']\n",
      "Topic 41: ['writes', 'article', 'posting', 'host', 'nntp', 'university', 'ca', 'distribution', 'cs', 'good', 'reply', 'computer', 'time', 'system', 'mail', 'usa', 'make', 'world', 'work']\n",
      "Topic 42: ['writes', 'article', 'people', 'university', 'posting', 'nntp', 'good', 'space', 'host', 'time', 'make', 'world', 'state', 'work', 'car', 'question', 'distribution', 'thing', 'science']\n",
      "Topic 43: ['writes', 'article', 'posting', 'host', 'nntp', 'university', 'ca', 'distribution', 'cs', 'good', 'reply', 'computer', 'time', 'system', 'usa', 'mail', 'world', 'make', 'people']\n",
      "Topic 44: ['writes', 'posting', 'article', 'host', 'nntp', 'university', 'ca', 'distribution', 'cs', 'good', 'reply', 'computer', 'mail', 'time', 'usa', 'system', 'cc', 'make', 'uk']\n",
      "Topic 45: ['writes', 'article', 'posting', 'university', 'nntp', 'host', 'good', 'people', 'time', 'distribution', 'reply', 'make', 'ca', 'space', 'world', 'system', 'state', 'cs', 'work']\n",
      "Topic 46: ['writes', 'article', 'posting', 'university', 'host', 'nntp', 'good', 'distribution', 'ca', 'time', 'cs', 'reply', 'people', 'world', 'system', 'computer', 'make', 'usa', 'state']\n",
      "Topic 47: ['writes', 'article', 'posting', 'host', 'nntp', 'university', 'distribution', 'system', 'people', 'computer', 'good', 'space', 'reply', 'time', 'work', 'make', 'problem', 'ca', 'cs']\n",
      "Topic 48: ['writes', 'article', 'posting', 'host', 'nntp', 'university', 'distribution', 'ca', 'good', 'cs', 'reply', 'computer', 'system', 'time', 'usa', 'mail', 'world', 'people', 'make']\n",
      "Topic 49: ['writes', 'article', 'posting', 'host', 'nntp', 'university', 'ca', 'distribution', 'cs', 'reply', 'good', 'computer', 'mail', 'usa', 'time', 'system', 'world', 'uk', 'cc']\n",
      "\n",
      " **.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.\n",
      "\n",
      "Visualize word embeddings by using output embedding matrix\n",
      "\n",
      "word: andrew .. neighbors:\n",
      " ['andrew', 'cs', 'host', 'nntp', 'ca', 'posting', 'mike', 'university', 'steve', 'mark', 'cc', 'net', 'washington', 'cmu', 'keywords', 'dept', 'nice', 'pretty', 'usa', 'stanford']\n",
      "\n",
      "word: computer .. neighbors:\n",
      " ['computer', 'gov', 'ac', 'access', 'drive', 'mail', 'email', 'uk', 'phone', 'internet', 'speed', 'software', 'price', 'distribution', 'board', 'posting', 'host', 'technology', 'info', 'systems']\n",
      "\n",
      "word: sports .. neighbors:\n",
      " ['sports', 'baseball', 'games', 'roger', 'fan', 'team', 'hockey', 'game', 'sport', 'league', 'players', 'cunixb', 'player', 'atlanta', 'play', 'bay', 'season', 'louis', 'jose', 'ball']\n",
      "\n",
      "word: religion .. neighbors:\n",
      " ['religion', 'christian', 'truth', 'jewish', 'jesus', 'weapons', 'christians', 'evil', 'god', 'human', 'religious', 'bible', 'evidence', 'death', 'existence', 'faith', 'land', 'children', 'anti', 'arms']\n",
      "\n",
      "word: man .. neighbors:\n",
      " ['man', 'left', 'home', 'made', 'years', 'great', 'american', 'year', 'east', 'place', 'started', 'give', 'april', 'power', 'city', 'john', 'back', 'times', 'rest', 'time']\n",
      "\n",
      "word: love .. neighbors:\n",
      " ['love', 'gun', 'live', 'agree', 'life', 'mind', 'people', 'wrong', 'thought', 'thing', 'kind', 'matter', 'question', 'hear', 'happen', 'talk', 'remember', 'police', 'understand', 'heard']\n",
      "\n",
      "word: intelligence .. neighbors:\n",
      " ['intelligence', 'cramer', 'tax', 'auto', 'care', 'clinton', 'rutgers', 'spend', 'billion', 'house', 'deleted', 'experience', 'driving', 'reasonable', 'sort', 'speak', 'insurance', 'court', 'miles', 'earth']\n",
      "\n",
      "word: money .. neighbors:\n",
      " ['money', 'writes', 'article', 'thing', 'car', 'opinions', 'bill', 'kind', 'guess', 'ago', 'pay', 'turn', 'colorado', 'california', 'science', 'james', 'post', 'wanted', 'university', 'state']\n",
      "\n",
      "word: politics .. neighbors:\n",
      " ['politics', 'moral', 'rutgers', 'batf', 'kids', 'morality', 'cramer', 'laws', 'guns', 'concerned', 'sandvik', 'budget', 'atheists', 'clayton', 'atheism', 'optilink', 'food', 'drugs', 'waco', 'amendment']\n",
      "\n",
      "word: health .. neighbors:\n",
      " ['health', 'gun', 'life', 'people', 'house', 'person', 'fact', 'president', 'agree', 'live', 'country', 'matter', 'days', 'love', 'society', 'sense', 'things', 'states', 'today', 'book']\n",
      "\n",
      "word: people .. neighbors:\n",
      " ['people', 'things', 'question', 'state', 'make', 'fact', 'world', 'government', 'wrong', 'true', 'day', 'case', 'long', 'reason', 'book', 'hand', 'find', 'thing', 'group', 'president']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "word: family .. neighbors:\n",
      " ['family', 'ways', 'present', 'specific', 'cases', 'policy', 'rights', 'groups', 'exists', 'involved', 'law', 'purpose', 'common', 'word', 'building', 'page', 'simply', 'events', 'act', 'areas']\n",
      "\n",
      "**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.**.\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 5937.8\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "TEST Doc Completion PPL: 5937.8\n",
      "****************************************************************************************************\n",
      "Computing topic coherence...\n",
      "Topic coherence is: 0.11004299629945907\n",
      "Computing topic diversity...\n",
      "Topic diveristy is: 0.1096\n",
      "\n",
      "The 10 most used topics are [35 17 40 25 28 11 34  6 15 20]\n",
      "\n",
      "\n",
      "Topic 0: ['writes', 'article', 'people', 'university', 'time', 'good', 'posting', 'space', 'make', 'world', 'state', 'nntp', 'work', 'question', 'host', 'things', 'years', 'thing', 'government']\n",
      "Topic 1: ['writes', 'article', 'people', 'time', 'university', 'good', 'make', 'space', 'posting', 'world', 'work', 'state', 'host', 'nntp', 'question', 'system', 'problem', 'back', 'things']\n",
      "Topic 2: ['writes', 'article', 'posting', 'nntp', 'university', 'host', 'people', 'space', 'good', 'system', 'time', 'distribution', 'make', 'world', 'car', 'state', 'work', 'reply', 'question']\n",
      "Topic 3: ['writes', 'article', 'people', 'university', 'posting', 'good', 'nntp', 'time', 'make', 'world', 'space', 'host', 'state', 'work', 'question', 'car', 'thing', 'science', 'system']\n",
      "Topic 4: ['writes', 'article', 'posting', 'nntp', 'university', 'host', 'distribution', 'ca', 'good', 'reply', 'cs', 'computer', 'time', 'system', 'usa', 'people', 'space', 'world', 'make']\n",
      "Topic 5: ['writes', 'article', 'posting', 'university', 'nntp', 'host', 'good', 'people', 'distribution', 'time', 'space', 'world', 'make', 'reply', 'system', 'state', 'cs', 'ca', 'work']\n",
      "Topic 6: ['people', 'writes', 'article', 'time', 'good', 'make', 'world', 'years', 'state', 'university', 'government', 'things', 'question', 'work', 'point', 'space', 'back', 'thing', 'made']\n",
      "Topic 7: ['writes', 'article', 'posting', 'university', 'host', 'nntp', 'ca', 'distribution', 'good', 'cs', 'reply', 'time', 'computer', 'usa', 'system', 'people', 'world', 'make', 'mail']\n",
      "Topic 8: ['writes', 'article', 'people', 'university', 'posting', 'good', 'nntp', 'time', 'host', 'make', 'world', 'space', 'state', 'work', 'distribution', 'system', 'question', 'reply', 'problem']\n",
      "Topic 9: ['writes', 'article', 'university', 'posting', 'people', 'nntp', 'host', 'good', 'time', 'world', 'state', 'make', 'space', 'distribution', 'reply', 'work', 'system', 'question', 'ca']\n",
      "Topic 10: ['writes', 'article', 'posting', 'university', 'nntp', 'host', 'good', 'people', 'time', 'distribution', 'system', 'make', 'space', 'world', 'reply', 'cs', 'ca', 'work', 'state']\n",
      "Topic 11: ['people', 'time', 'writes', 'article', 'make', 'good', 'world', 'government', 'state', 'things', 'years', 'point', 'question', 'work', 'god', 'fact', 'case', 'made', 'find']\n",
      "Topic 12: ['writes', 'article', 'posting', 'host', 'nntp', 'university', 'ca', 'distribution', 'cs', 'good', 'reply', 'computer', 'time', 'system', 'usa', 'mail', 'world', 'make', 'space']\n",
      "Topic 13: ['writes', 'posting', 'article', 'host', 'nntp', 'university', 'distribution', 'ca', 'computer', 'cs', 'reply', 'system', 'mail', 'good', 'usa', 'time', 'uk', 'gov', 'drive']\n",
      "Topic 14: ['writes', 'posting', 'article', 'host', 'university', 'nntp', 'ca', 'cs', 'distribution', 'good', 'reply', 'computer', 'time', 'usa', 'mail', 'system', 'cc', 'world', 'make']\n",
      "Topic 15: ['people', 'writes', 'article', 'time', 'good', 'make', 'university', 'world', 'state', 'space', 'years', 'question', 'things', 'work', 'government', 'posting', 'thing', 'back', 'long']\n",
      "Topic 16: ['writes', 'article', 'posting', 'university', 'nntp', 'host', 'distribution', 'ca', 'good', 'reply', 'cs', 'time', 'system', 'computer', 'people', 'usa', 'world', 'make', 'space']\n",
      "Topic 17: ['file', 'system', 'windows', 'key', 'files', 'bit', 'scsi', 'data', 'version', 'image', 'software', 'information', 'ftp', 'drive', 'dos', 'server', 'program', 'mit', 'window']\n",
      "Topic 18: ['writes', 'article', 'posting', 'host', 'nntp', 'university', 'ca', 'distribution', 'good', 'cs', 'computer', 'reply', 'system', 'time', 'usa', 'mail', 'make', 'world', 'people']\n",
      "Topic 19: ['writes', 'article', 'university', 'posting', 'nntp', 'host', 'people', 'good', 'time', 'make', 'world', 'distribution', 'reply', 'space', 'state', 'work', 'system', 'cs', 'ca']\n",
      "Topic 20: ['writes', 'article', 'people', 'university', 'good', 'time', 'space', 'make', 'state', 'world', 'posting', 'question', 'work', 'nntp', 'years', 'things', 'thing', 'government', 'long']\n",
      "Topic 21: ['writes', 'article', 'posting', 'host', 'nntp', 'university', 'distribution', 'ca', 'good', 'reply', 'cs', 'system', 'computer', 'time', 'space', 'people', 'mail', 'make', 'world']\n",
      "Topic 22: ['writes', 'article', 'people', 'time', 'university', 'good', 'make', 'world', 'state', 'question', 'space', 'work', 'posting', 'years', 'government', 'things', 'thing', 'point', 'long']\n",
      "Topic 23: ['writes', 'article', 'posting', 'university', 'nntp', 'host', 'people', 'good', 'time', 'distribution', 'world', 'make', 'space', 'state', 'reply', 'system', 'ca', 'work', 'cs']\n",
      "Topic 24: ['writes', 'article', 'posting', 'university', 'nntp', 'host', 'people', 'good', 'time', 'distribution', 'system', 'space', 'make', 'reply', 'world', 'work', 'computer', 'state', 'ca']\n",
      "Topic 25: ['max', 'pl', 'ah', 'giz', 'wm', 'bhj', 'sl', 'mv', 'ma', 'cx', 'tm', 'mr', 'mq', 'ql', 'bj', 'mt', 'mp', 'lk', 'mc']\n",
      "Topic 26: ['writes', 'article', 'people', 'university', 'posting', 'space', 'good', 'nntp', 'time', 'make', 'world', 'state', 'host', 'work', 'question', 'thing', 'car', 'science', 'system']\n",
      "Topic 27: ['posting', 'writes', 'article', 'host', 'nntp', 'university', 'computer', 'distribution', 'ca', 'system', 'mail', 'cs', 'reply', 'drive', 'gov', 'access', 'software', 'uk', 'usa']\n",
      "Topic 28: ['people', 'writes', 'article', 'god', 'time', 'world', 'make', 'good', 'state', 'question', 'government', 'years', 'things', 'gun', 'point', 'life', 'fact', 'law', 'made']\n",
      "Topic 29: ['writes', 'article', 'people', 'time', 'good', 'make', 'university', 'space', 'world', 'state', 'work', 'question', 'posting', 'government', 'things', 'years', 'case', 'thing', 'point']\n",
      "Topic 30: ['writes', 'article', 'posting', 'nntp', 'university', 'host', 'distribution', 'good', 'ca', 'system', 'time', 'reply', 'computer', 'cs', 'people', 'space', 'make', 'world', 'work']\n",
      "Topic 31: ['posting', 'host', 'nntp', 'writes', 'article', 'university', 'computer', 'distribution', 'ca', 'mail', 'cs', 'system', 'drive', 'reply', 'software', 'uk', 'gov', 'access', 'usa']\n",
      "Topic 32: ['writes', 'article', 'posting', 'university', 'nntp', 'host', 'good', 'people', 'distribution', 'time', 'make', 'world', 'space', 'reply', 'state', 'ca', 'system', 'work', 'cs']\n",
      "Topic 33: ['writes', 'article', 'people', 'university', 'time', 'good', 'make', 'space', 'world', 'state', 'posting', 'question', 'work', 'nntp', 'thing', 'years', 'things', 'host', 'long']\n",
      "Topic 34: ['people', 'writes', 'time', 'article', 'make', 'good', 'world', 'government', 'years', 'things', 'state', 'point', 'space', 'work', 'question', 'back', 'university', 'fact', 'case']\n",
      "Topic 35: ['people', 'god', 'jesus', 'law', 'jews', 'time', 'christian', 'christians', 'israel', 'turkish', 'human', 'armenian', 'life', 'world', 'war', 'bible', 'faith', 'gun', 'fact']\n",
      "Topic 36: ['writes', 'article', 'posting', 'host', 'nntp', 'university', 'ca', 'cs', 'distribution', 'good', 'reply', 'computer', 'time', 'usa', 'mail', 'system', 'world', 'make', 'cc']\n",
      "Topic 37: ['writes', 'article', 'posting', 'university', 'host', 'nntp', 'ca', 'distribution', 'good', 'cs', 'reply', 'time', 'system', 'computer', 'mail', 'make', 'people', 'world', 'usa']\n",
      "Topic 38: ['host', 'posting', 'article', 'computer', 'nntp', 'writes', 'system', 'software', 'distribution', 'mail', 'drive', 'university', 'access', 'gov', 'ca', 'cs', 'uk', 'version', 'space']\n",
      "Topic 39: ['writes', 'article', 'posting', 'university', 'nntp', 'host', 'distribution', 'good', 'time', 'people', 'system', 'make', 'cs', 'reply', 'ca', 'space', 'world', 'computer', 'work']\n",
      "Topic 40: ['team', 'year', 'game', 'games', 'ca', 'play', 'db', 'season', 'hockey', 'win', 'good', 'players', 'cs', 'league', 'period', 'nhl', 'teams', 'baseball', 'pittsburgh']\n",
      "Topic 41: ['writes', 'article', 'posting', 'host', 'nntp', 'university', 'ca', 'distribution', 'cs', 'good', 'reply', 'computer', 'time', 'system', 'mail', 'usa', 'make', 'world', 'work']\n",
      "Topic 42: ['writes', 'article', 'people', 'university', 'posting', 'nntp', 'good', 'space', 'host', 'time', 'make', 'world', 'state', 'work', 'car', 'question', 'distribution', 'thing', 'science']\n",
      "Topic 43: ['writes', 'article', 'posting', 'host', 'nntp', 'university', 'ca', 'distribution', 'cs', 'good', 'reply', 'computer', 'time', 'system', 'usa', 'mail', 'world', 'make', 'people']\n",
      "Topic 44: ['writes', 'posting', 'article', 'host', 'nntp', 'university', 'ca', 'distribution', 'cs', 'good', 'reply', 'computer', 'mail', 'time', 'usa', 'system', 'cc', 'make', 'uk']\n",
      "Topic 45: ['writes', 'article', 'posting', 'university', 'nntp', 'host', 'good', 'people', 'time', 'distribution', 'reply', 'make', 'ca', 'space', 'world', 'system', 'state', 'cs', 'work']\n",
      "Topic 46: ['writes', 'article', 'posting', 'university', 'host', 'nntp', 'good', 'distribution', 'ca', 'time', 'cs', 'reply', 'people', 'world', 'system', 'computer', 'make', 'usa', 'state']\n",
      "Topic 47: ['writes', 'article', 'posting', 'host', 'nntp', 'university', 'distribution', 'system', 'people', 'computer', 'good', 'space', 'reply', 'time', 'work', 'make', 'problem', 'ca', 'cs']\n",
      "Topic 48: ['writes', 'article', 'posting', 'host', 'nntp', 'university', 'distribution', 'ca', 'good', 'cs', 'reply', 'computer', 'system', 'time', 'usa', 'mail', 'world', 'people', 'make']\n",
      "Topic 49: ['writes', 'article', 'posting', 'host', 'nntp', 'university', 'ca', 'distribution', 'cs', 'reply', 'good', 'computer', 'mail', 'usa', 'time', 'system', 'world', 'uk', 'cc']\n",
      "\n",
      "\n",
      "ETM embeddings...\n",
      "word: andrew .. etm neighbors: ['andrew', 'cs', 'host', 'nntp', 'ca', 'posting', 'mike', 'university', 'steve', 'mark', 'cc', 'net', 'washington', 'cmu', 'keywords', 'dept', 'nice', 'pretty', 'usa', 'stanford']\n",
      "word: woman .. etm neighbors: ['woman', 'armenian', 'armenians', 'israel', 'armenia', 'turkish', 'mountain', 'population', 'israeli', 'son', 'born', 'lived', 'soldiers', 'greek', 'killing', 'father', 'soviet', 'holy', 'genocide', 'died']\n",
      "word: computer .. etm neighbors: ['computer', 'gov', 'ac', 'access', 'drive', 'mail', 'email', 'uk', 'phone', 'internet', 'speed', 'software', 'price', 'distribution', 'board', 'posting', 'host', 'technology', 'info', 'systems']\n",
      "word: sports .. etm neighbors: ['sports', 'baseball', 'games', 'roger', 'fan', 'team', 'hockey', 'game', 'sport', 'league', 'players', 'cunixb', 'player', 'atlanta', 'play', 'bay', 'season', 'louis', 'jose', 'ball']\n",
      "word: religion .. etm neighbors: ['religion', 'christian', 'truth', 'jewish', 'jesus', 'weapons', 'christians', 'evil', 'god', 'human', 'religious', 'bible', 'evidence', 'death', 'existence', 'faith', 'land', 'children', 'anti', 'arms']\n",
      "word: man .. etm neighbors: ['man', 'left', 'home', 'made', 'years', 'great', 'american', 'year', 'east', 'place', 'started', 'give', 'april', 'power', 'city', 'john', 'back', 'times', 'rest', 'time']\n",
      "word: love .. etm neighbors: ['love', 'gun', 'live', 'agree', 'life', 'mind', 'people', 'wrong', 'thought', 'thing', 'kind', 'matter', 'question', 'hear', 'happen', 'talk', 'remember', 'police', 'understand', 'heard']\n",
      "word: intelligence .. etm neighbors: ['intelligence', 'cramer', 'tax', 'auto', 'care', 'clinton', 'rutgers', 'spend', 'billion', 'house', 'deleted', 'experience', 'driving', 'reasonable', 'sort', 'speak', 'insurance', 'court', 'miles', 'earth']\n",
      "word: money .. etm neighbors: ['money', 'writes', 'article', 'thing', 'car', 'opinions', 'bill', 'kind', 'guess', 'ago', 'pay', 'turn', 'colorado', 'california', 'science', 'james', 'post', 'wanted', 'university', 'state']\n",
      "word: politics .. etm neighbors: ['politics', 'moral', 'rutgers', 'batf', 'kids', 'morality', 'cramer', 'laws', 'guns', 'concerned', 'sandvik', 'budget', 'atheists', 'clayton', 'atheism', 'optilink', 'food', 'drugs', 'waco', 'amendment']\n",
      "word: health .. etm neighbors: ['health', 'gun', 'life', 'people', 'house', 'person', 'fact', 'president', 'agree', 'live', 'country', 'matter', 'days', 'love', 'society', 'sense', 'things', 'states', 'today', 'book']\n",
      "word: people .. etm neighbors: ['people', 'things', 'question', 'state', 'make', 'fact', 'world', 'government', 'wrong', 'true', 'day', 'case', 'long', 'reason', 'book', 'hand', 'find', 'thing', 'group', 'president']\n",
      "word: family .. etm neighbors: ['family', 'ways', 'present', 'specific', 'cases', 'policy', 'rights', 'groups', 'exists', 'involved', 'law', 'purpose', 'common', 'word', 'building', 'page', 'simply', 'events', 'act', 'areas']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## train model on data \n",
    "if mode == 'Train':\n",
    "    best_epoch = 0\n",
    "    best_val_ppl = 1e9\n",
    "    all_val_ppls = []\n",
    "    print('\\n')\n",
    "    print('Visualizing model quality before training...')\n",
    "    visualize(model)\n",
    "    print('\\n')\n",
    "    for epoch in range(1, epochs):\n",
    "        train(epoch)\n",
    "        val_ppl = evaluate(model, 'val')\n",
    "        if val_ppl < best_val_ppl:\n",
    "            best_epoch = epoch\n",
    "            best_val_ppl = val_ppl\n",
    "        else:\n",
    "            ## check whether to anneal lr\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            if anneal_lr and (len(all_val_ppls) > nonmono and val_ppl > min(all_val_ppls[:-nonmono]) and lr > 1e-5):\n",
    "                optimizer.param_groups[0]['lr'] /= lr_factor\n",
    "        if epoch % visualize_every == 0:\n",
    "            visualize(model)\n",
    "        all_val_ppls.append(val_ppl)\n",
    "\n",
    "    model = model.to(device)\n",
    "    val_ppl = evaluate(model, 'val')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ## get document completion perplexities\n",
    "        test_ppl = evaluate(model, 'test', tc=True, td=True)\n",
    "\n",
    "        ## get most used topics\n",
    "        indices = torch.tensor(range(num_docs_train))\n",
    "        indices = torch.split(indices, batch_size)\n",
    "        thetaAvg = torch.zeros(1, num_topics).to(device)\n",
    "        thetaWeightedAvg = torch.zeros(1, num_topics).to(device)\n",
    "        cnt = 0\n",
    "        for idx, ind in enumerate(indices):\n",
    "            data_batch = get_batch(train_tokens, train_counts, ind, vocab_size, device)\n",
    "            sums = data_batch.sum(1).unsqueeze(1)\n",
    "            cnt += sums.sum(0).squeeze().cpu().numpy()\n",
    "            if bow_norm:\n",
    "                normalized_data_batch = data_batch / sums\n",
    "            else:\n",
    "                normalized_data_batch = data_batch\n",
    "            theta, _ = model.get_theta(normalized_data_batch)\n",
    "            thetaAvg += theta.sum(0).unsqueeze(0) / num_docs_train\n",
    "            weighed_theta = sums * theta\n",
    "            thetaWeightedAvg += weighed_theta.sum(0).unsqueeze(0)\n",
    "            if idx % 100 == 0 and idx > 0:\n",
    "                print('batch: {}/{}'.format(idx, len(indices)))\n",
    "        thetaWeightedAvg = thetaWeightedAvg.squeeze().cpu().numpy() / cnt\n",
    "        print('\\nThe 10 most used topics are {}'.format(thetaWeightedAvg.argsort()[::-1][:10]))\n",
    "\n",
    "        ## show topics\n",
    "        beta = model.get_beta()\n",
    "        topic_indices = list(np.random.choice(num_topics, 10)) # 10 random topics\n",
    "        print('\\n')\n",
    "        for k in range(num_topics):#topic_indices:\n",
    "            gamma = beta[k]\n",
    "            top_words = list(gamma.cpu().numpy().argsort()[-num_words+1:][::-1])\n",
    "            topic_words = [vocab[a] for a in top_words]\n",
    "            print('Topic {}: {}'.format(k, topic_words))\n",
    "\n",
    "        if train_embeddings:\n",
    "            ## show etm embeddings \n",
    "            try:\n",
    "                rho_etm = model.rho.weight.cpu()\n",
    "            except:\n",
    "                rho_etm = model.rho.cpu()\n",
    "            queries = ['andrew', 'woman', 'computer', 'sports', 'religion', 'man', 'love', \n",
    "                            'intelligence', 'money', 'politics', 'health', 'people', 'family']\n",
    "            print('\\n')\n",
    "            print('ETM embeddings...')\n",
    "            for word in queries:\n",
    "                print('word: {} .. etm neighbors: {}'.format(word, nearest_neighbors(word, rho_etm, vocab)))\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 'eval':\n",
    "    best_epoch = 0\n",
    "    best_val_ppl = 1e9\n",
    "    all_val_ppls = []\n",
    "    print('\\n')\n",
    "    print('Visualizing model quality before training...')\n",
    "    visualize(model)\n",
    "    print('\\n')\n",
    "    for epoch in range(1, epochs):\n",
    "        train(epoch)\n",
    "        val_ppl = evaluate(model, 'val')\n",
    "        if val_ppl < best_val_ppl:\n",
    "            best_epoch = epoch\n",
    "            best_val_ppl = val_ppl\n",
    "        else:\n",
    "            ## check whether to anneal lr\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            if anneal_lr and (len(all_val_ppls) > nonmono and val_ppl > min(all_val_ppls[:-nonmono]) and lr > 1e-5):\n",
    "                optimizer.param_groups[0]['lr'] /= lr_factor\n",
    "        if epoch % visualize_every == 0:\n",
    "            visualize(model)\n",
    "        all_val_ppls.append(val_ppl)\n",
    "\n",
    "    model = model.to(device)\n",
    "    val_ppl = evaluate(model, 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(subset='train')\n",
    "data = data.data\n",
    "\n",
    "\n",
    "with open('C:/Users/amitp/OneDrive/Ryerson-DS/DS8008/project-nlp/ETM-master/ETM-master/scripts/stops.txt', 'r') as f:\n",
    "    stops = f.read().split('\\n')\n",
    "stops[0:5]\n",
    "\n",
    "\n",
    "data_clean = [re.findall(r'''[\\w']+|[.,!?;-~{}`´_<=>:/@*()&'$%#\"]''',data[doc]) for doc in range(len(data))]\n",
    "\n",
    "def contains_punctuation(w):\n",
    "    return any(char in string.punctuation for char in w)\n",
    "\n",
    "def contains_numeric(w):\n",
    "    return any(char.isdigit() for char in w)\n",
    "\n",
    "data_clean1 = [[w.lower() for w in data_clean[doc] if not contains_punctuation(w)] for doc in range(len(data_clean))]\n",
    "data_clean1 = [[w for w in data_clean1[doc] if not contains_numeric(w)] for doc in range(len(data_clean1))]\n",
    "data_clean = [\" \".join(data_clean1[doc]) for doc in range(len(data_clean1))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(data_clean1)\n",
    "corpus = [dictionary.doc2bow(text) for text in data_clean1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit to the model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=50, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=1000,\n",
    "                                           passes=5,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -10.55420551940949\n",
      "\n",
      "Coherence Score:  0.4678729093056614\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_clean1, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10, perplexity: 1866.3155\n"
     ]
    }
   ],
   "source": [
    "# LDA from sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "cvectorizer = CountVectorizer(min_df=0.3, max_df=1.0, stop_words=None)\n",
    "cvz = cvectorizer.fit_transform(data_clean)\n",
    "\n",
    "k = 50\n",
    "lda = LatentDirichletAllocation(n_components=k,\n",
    "                                learning_method='online',\n",
    "                                learning_decay=0.85,\n",
    "                                learning_offset=10.,\n",
    "                                evaluate_every=10,\n",
    "                                verbose=1,\n",
    "                                random_state=5).fit(cvz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E3LVEKeefgbb"
   },
   "source": [
    "# Conclusion and Future Direction\n",
    "\n",
    "-  Here we implemented Topic Modelling in Different environmental settings:\n",
    "    1. Implemented Topic modelling with word2Vec pre trained embeddings  (perplexity :6064.30)\n",
    "    2. Implemented Topic modelling with training of word embedding and Topic embeddings (perplexity :5937.8)\n",
    "    3. The traditional Topic Model with LDA (3170.5589)\n",
    "- In 20news group data, The choice of topic word were more convincing with pretrained embedded topic models then LDA and ETM embeddings of words and documents.\n",
    "   1. All of the three models are highly sensitive to random words.\n",
    "   2. We should very  carefully while selecting token because these models tend to form a topic of random words itself.\n",
    "**Future work:**\n",
    "- We should come up with a model which can select words or predict the misspelled word correctly or may be try some POS tagging Techniques to eliminate irrelevant words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fynzI35Ffgbb"
   },
   "source": [
    "# References:\n",
    "\n",
    "\n",
    "\n",
    "[2]:  Adji B. Dieng, Francisco J. R. Ruiz and David M. Blei, Topic Modeling in Embedded Spaces, ArXiv, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Final_project_draft.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
