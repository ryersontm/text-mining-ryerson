{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from scipy import sparse\n",
    "import itertools\n",
    "from scipy.io import savemat, loadmat\n",
    "import re\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum / minimum document frequency\n",
    "max_df = 0.7\n",
    "min_df = 10  # choose desired value for min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'able', 'about', 'above', 'according']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('stops.txt', 'r') as f:\n",
    "    stops = f.read().split('\\n')\n",
    "stops[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = fetch_20newsgroups(subset='train')\n",
    "test_data = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  \n",
    " - [\\w']+ = word including aphostopie\n",
    " -         or\n",
    " - [.,!?;-~{}`´_<=>:/@*()&'$%#\"]  - all special characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_docs_tr = [re.findall(r'''[\\w']+|[.,!?;-~{}`´_<=>:/@*()&'$%#\"]''', train_data.data[doc]) for doc in range(len(train_data.data))]\n",
    "init_docs_ts = [re.findall(r'''[\\w']+|[.,!?;-~{}`´_<=>:/@*()&'$%#\"]''', test_data.data[doc]) for doc in range(len(test_data.data))]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_punctuation(w):\n",
    "    return any(char in string.punctuation for char in w)\n",
    "\n",
    "def contains_numeric(w):\n",
    "    return any(char.isdigit() for char in w)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of documents\n",
    "len(init_docs_tr)+len(init_docs_ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_docs = init_docs_tr + init_docs_ts\n",
    "init_docs = [[w.lower() for w in init_docs[doc] if not contains_punctuation(w)] for doc in range(len(init_docs))]\n",
    "init_docs = [[w for w in init_docs[doc] if not contains_numeric(w)] for doc in range(len(init_docs))]\n",
    "init_docs = [[w for w in init_docs[doc] if len(w)>1] for doc in range(len(init_docs))]\n",
    "init_docs = [\" \".join(init_docs[doc]) for doc in range(len(init_docs))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting document frequency of words...\n"
     ]
    }
   ],
   "source": [
    "# Create count vectorizer\n",
    "print('counting document frequency of words...')\n",
    "cvectorizer = CountVectorizer(min_df=min_df, max_df=max_df, stop_words=None)\n",
    "cvz = cvectorizer.fit_transform(init_docs).sign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18846, 19148)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document - term matrix (document*vocabulary)\n",
    "cvz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building the vocabulary...\n",
      "\n",
      "vocabulary size :  19148\n"
     ]
    }
   ],
   "source": [
    "# Get vocabulary\n",
    "print('building the vocabulary...')\n",
    "sum_counts = cvz.sum(axis=0)\n",
    "v_size = sum_counts.shape[1]\n",
    "print(\"\\nvocabulary size : \",v_size)\n",
    "sum_counts_np = np.zeros(v_size, dtype=int)\n",
    "\n",
    "for v in range(v_size):\n",
    "    sum_counts_np[v] = sum_counts[0,v]\n",
    "    \n",
    "word2id = dict([(w, cvectorizer.vocabulary_.get(w)) for w in cvectorizer.vocabulary_])\n",
    "id2word = dict([(cvectorizer.vocabulary_.get(w), w) for w in cvectorizer.vocabulary_])\n",
    "#del cvectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 19148)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19148,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_counts_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort elements in vocabulary\n",
    "idx_sort = np.argsort(sum_counts_np)\n",
    "vocab_aux = [id2word[idx_sort[cc]] for cc in range(v_size)]   #words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  vocabulary size after removing stopwords from list: 18677\n"
     ]
    }
   ],
   "source": [
    "# Filter out stopwords (if any)\n",
    "vocab_aux = [w for w in vocab_aux if w not in stops] # eliminate stop words\n",
    "print('  vocabulary size after removing stopwords from list: {}'.format(len(vocab_aux)))\n",
    "\n",
    "# Create dictionary and inverse dictionary\n",
    "vocab = vocab_aux\n",
    "#del vocab_aux\n",
    "word2id = dict([(w, j) for j, w in enumerate(vocab)])\n",
    "id2word = dict([(j, w) for j, w in enumerate(vocab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'regan': 0, 'safest': 1, 'cosmetic': 2, 'salomon': 3, 'coronal': 4}\n",
      "{0: 'regan', 1: 'safest', 2: 'cosmetic', 3: 'salomon', 4: 'coronal'}\n"
     ]
    }
   ],
   "source": [
    "# word_id\n",
    "print(dict(itertools.islice(word2id.items(), 5)) )\n",
    "# id to word\n",
    "print(dict(itertools.islice(id2word.items(), 5)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing documents and splitting into train/test/valid...\n"
     ]
    }
   ],
   "source": [
    "# Split in train/test/valid\n",
    "print('tokenizing documents and splitting into train/test/valid...')\n",
    "num_docs_tr = len(init_docs_tr)\n",
    "trSize = num_docs_tr-100\n",
    "tsSize = len(init_docs_ts)\n",
    "vaSize = 100\n",
    "idx_permute = np.random.permutation(num_docs_tr).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  vocabulary after removing words not in train: 18626\n"
     ]
    }
   ],
   "source": [
    "# Remove words not in train_data\n",
    "vocab = list(set([w for idx_d in range(trSize) for w in init_docs[idx_permute[idx_d]].split() if w in word2id]))\n",
    "word2id = dict([(w, j) for j, w in enumerate(vocab)])\n",
    "id2word = dict([(j, w) for j, w in enumerate(vocab)])\n",
    "print('  vocabulary after removing words not in train: {}'.format(len(vocab)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in train/test/valid\n",
    "docs_tr = [[word2id[w] for w in init_docs[idx_permute[idx_d]].split() if w in word2id] for idx_d in range(trSize)]\n",
    "docs_va = [[word2id[w] for w in init_docs[idx_permute[idx_d+trSize]].split() if w in word2id] for idx_d in range(vaSize)]\n",
    "docs_ts = [[word2id[w] for w in init_docs[idx_d+num_docs_tr].split() if w in word2id] for idx_d in range(tsSize)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  number of documents (train): 11214 [this should be equal to 11214]\n",
      "  number of documents (test): 7532 [this should be equal to 7532]\n",
      "  number of documents (valid): 100 [this should be equal to 100]\n"
     ]
    }
   ],
   "source": [
    "print('  number of documents (train): {} [this should be equal to {}]'.format(len(docs_tr), trSize))\n",
    "print('  number of documents (test): {} [this should be equal to {}]'.format(len(docs_ts), tsSize))\n",
    "print('  number of documents (valid): {} [this should be equal to {}]'.format(len(docs_va), vaSize))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing empty documents...\n"
     ]
    }
   ],
   "source": [
    "# Remove empty documents\n",
    "print('removing empty documents...')\n",
    "\n",
    "def remove_empty(in_docs):\n",
    "    return [doc for doc in in_docs if doc!=[]]\n",
    "\n",
    "docs_tr = remove_empty(docs_tr)\n",
    "docs_ts = remove_empty(docs_ts)\n",
    "docs_va = remove_empty(docs_va)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove test documents with length=1\n",
    "docs_ts = [doc for doc in docs_ts if len(doc)>1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting test documents in 2 halves...\n"
     ]
    }
   ],
   "source": [
    "# Split test set in 2 halves\n",
    "print('splitting test documents in 2 halves...')\n",
    "docs_ts_h1 = [[w for i,w in enumerate(doc) if i<=len(doc)/2.0-1] for doc in docs_ts]\n",
    "docs_ts_h2 = [[w for i,w in enumerate(doc) if i>len(doc)/2.0-1] for doc in docs_ts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating lists of words...\n",
      "  len(words_tr):  1339505\n",
      "  len(words_ts):  860813\n",
      "  len(words_ts_h1):  428558\n",
      "  len(words_ts_h2):  432255\n",
      "  len(words_va):  9601\n",
      "getting doc indices...\n"
     ]
    }
   ],
   "source": [
    "# Getting lists of words and doc_indices\n",
    "print('creating lists of words...')\n",
    "\n",
    "def create_list_words(in_docs):\n",
    "    return [x for y in in_docs for x in y]\n",
    "\n",
    "words_tr = create_list_words(docs_tr)\n",
    "words_ts = create_list_words(docs_ts)\n",
    "words_ts_h1 = create_list_words(docs_ts_h1)\n",
    "words_ts_h2 = create_list_words(docs_ts_h2)\n",
    "words_va = create_list_words(docs_va)\n",
    "\n",
    "print('  len(words_tr): ', len(words_tr))\n",
    "print('  len(words_ts): ', len(words_ts))\n",
    "print('  len(words_ts_h1): ', len(words_ts_h1))\n",
    "print('  len(words_ts_h2): ', len(words_ts_h2))\n",
    "print('  len(words_va): ', len(words_va))\n",
    "\n",
    "# Get doc indices\n",
    "print('getting doc indices...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1447, 2459, 18163, 8679, 16829]\n",
      "[8624, 6156, 17654, 193, 5193]\n"
     ]
    }
   ],
   "source": [
    "print(words_tr[0:5])\n",
    "print(words_ts_h1[0:5])# id of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  len(np.unique(doc_indices_tr)): 11214 [this should be 11214]\n",
      "  len(np.unique(doc_indices_ts)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_ts_h1)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_ts_h2)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_va)): 100 [this should be 100]\n"
     ]
    }
   ],
   "source": [
    "def create_doc_indices(in_docs):\n",
    "    aux = [[j for i in range(len(doc))] for j, doc in enumerate(in_docs)]\n",
    "    return [int(x) for y in aux for x in y]\n",
    "\n",
    "doc_indices_tr = create_doc_indices(docs_tr)\n",
    "doc_indices_ts = create_doc_indices(docs_ts)\n",
    "doc_indices_ts_h1 = create_doc_indices(docs_ts_h1)\n",
    "doc_indices_ts_h2 = create_doc_indices(docs_ts_h2)\n",
    "doc_indices_va = create_doc_indices(docs_va)\n",
    "\n",
    "\n",
    "print('  len(np.unique(doc_indices_tr)): {} [this should be {}]'.format(len(np.unique(doc_indices_tr)), len(docs_tr)))\n",
    "print('  len(np.unique(doc_indices_ts)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts)), len(docs_ts)))\n",
    "print('  len(np.unique(doc_indices_ts_h1)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts_h1)), len(docs_ts_h1)))\n",
    "print('  len(np.unique(doc_indices_ts_h2)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts_h2)), len(docs_ts_h2)))\n",
    "print('  len(np.unique(doc_indices_va)): {} [this should be {}]'.format(len(np.unique(doc_indices_va)), len(docs_va)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_docs_tr : 11214  n_docs_ts : 7532\n"
     ]
    }
   ],
   "source": [
    "# Number of documents in each set\n",
    "n_docs_tr = len(docs_tr)\n",
    "n_docs_ts = len(docs_ts)\n",
    "n_docs_ts_h1 = len(docs_ts_h1)\n",
    "n_docs_ts_h2 = len(docs_ts_h2)\n",
    "n_docs_va = len(docs_va)\n",
    "\n",
    "print(\"n_docs_tr :\",n_docs_tr,' n_docs_ts :',n_docs_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unused variables\n",
    "del docs_tr\n",
    "del docs_ts\n",
    "del docs_ts_h1\n",
    "del docs_ts_h2\n",
    "del docs_va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bow(doc_indices, words, n_docs, vocab_size):\n",
    "    return sparse.coo_matrix(([1]*len(doc_indices),(doc_indices, words)), shape=(n_docs, vocab_size)).tocsr()\n",
    "\n",
    "bow_tr = create_bow(doc_indices_tr, words_tr, n_docs_tr, len(vocab))\n",
    "bow_ts = create_bow(doc_indices_ts, words_ts, n_docs_ts, len(vocab))\n",
    "bow_ts_h1 = create_bow(doc_indices_ts_h1, words_ts_h1, n_docs_ts_h1, len(vocab))\n",
    "bow_ts_h2 = create_bow(doc_indices_ts_h2, words_ts_h2, n_docs_ts_h2, len(vocab))\n",
    "bow_va = create_bow(doc_indices_va, words_va, n_docs_va, len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del words_tr\n",
    "del words_ts\n",
    "del words_ts_h1\n",
    "del words_ts_h2\n",
    "del words_va\n",
    "del doc_indices_tr\n",
    "del doc_indices_ts\n",
    "del doc_indices_ts_h1\n",
    "del doc_indices_ts_h2\n",
    "del doc_indices_va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the vocabulary to a file\n",
    "path_save = 'C:/Users/amitp/OneDrive/Ryerson-DS/DS8008/project-nlp/ETM-master/ETM-master/scripts/'\n",
    "if not os.path.isdir(path_save):\n",
    "    os.system('mkdir -p ' + path_save)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_save + 'vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "#del vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['embedded', 'erzurum', 'hypocritical', 'acc', 'vgalogo', 'smithsonian', 'friendly', 'yk', 'hollow', 'twisto']\n"
     ]
    }
   ],
   "source": [
    "print(vocab[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting bow intro token/value pairs and saving to disk...\n"
     ]
    }
   ],
   "source": [
    "# Split bow intro token/value pairs\n",
    "print('splitting bow intro token/value pairs and saving to disk...')\n",
    "\n",
    "def split_bow(bow_in, n_docs):\n",
    "    indices = [[w for w in bow_in[doc,:].indices] for doc in range(n_docs)]\n",
    "    counts = [[c for c in bow_in[doc,:].data] for doc in range(n_docs)]\n",
    "    return indices, counts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_tr_tokens, bow_tr_counts = split_bow(bow_tr, n_docs_tr)\n",
    "savemat(path_save + 'bow_tr_tokens', {'tokens': bow_tr_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_tr_counts', {'counts': bow_tr_counts}, do_compression=True)\n",
    "\n",
    "bow_ts_tokens, bow_ts_counts = split_bow(bow_ts, n_docs_ts)\n",
    "savemat(path_save + 'bow_ts_tokens', {'tokens': bow_ts_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_ts_counts', {'counts': bow_ts_counts}, do_compression=True)\n",
    "\n",
    "bow_ts_h1_tokens, bow_ts_h1_counts = split_bow(bow_ts_h1, n_docs_ts_h1)\n",
    "savemat(path_save + 'bow_ts_h1_tokens', {'tokens': bow_ts_h1_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_ts_h1_counts', {'counts': bow_ts_h1_counts}, do_compression=True)\n",
    "\n",
    "bow_ts_h2_tokens, bow_ts_h2_counts = split_bow(bow_ts_h2, n_docs_ts_h2)\n",
    "savemat(path_save + 'bow_ts_h2_tokens', {'tokens': bow_ts_h2_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_ts_h2_counts', {'counts': bow_ts_h2_counts}, do_compression=True)\n",
    "\n",
    "bow_va_tokens, bow_va_counts = split_bow(bow_va, n_docs_va)\n",
    "savemat(path_save + 'bow_va_tokens', {'tokens': bow_va_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_va_counts', {'counts': bow_va_counts}, do_compression=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bow_tr\n",
    "del bow_tr_tokens\n",
    "del bow_tr_counts\n",
    "\n",
    "del bow_ts\n",
    "del bow_ts_tokens\n",
    "del bow_ts_counts\n",
    "\n",
    "del bow_ts_h1\n",
    "del bow_ts_h1_tokens\n",
    "del bow_ts_h1_counts\n",
    "\n",
    "del bow_ts_h2\n",
    "del bow_ts_h2_tokens\n",
    "del bow_ts_h2_counts\n",
    "\n",
    "del bow_va\n",
    "del bow_va_tokens\n",
    "del bow_va_counts\n",
    "\n",
    "print('Data ready !!')\n",
    "print('*************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
