{"cells":[{"cell_type":"markdown","id":"a36975d6-5880-40e0-ab7e-b1ca115b88c0","metadata":{"id":"a36975d6-5880-40e0-ab7e-b1ca115b88c0"},"source":["# HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face\n","\n","#### Members: Kunal Gurnani, Murad Taher\n","#### Emails: kunal.gurnani@torontomu.ca, mtaher@torontomu.ca"]},{"cell_type":"markdown","id":"bf2aab33-fbf8-4ea2-8db7-ea070cc3e456","metadata":{"id":"bf2aab33-fbf8-4ea2-8db7-ea070cc3e456"},"source":["# Introduction:\n","\n","### Problem Description:\n","\n","Although current LLMs exhibit superior capabilities in language understanding, generation, and reasoning, they are still imperfect and confront some challenges on the way to building an advanced AI system. Current LLMs lack the ability to process complex information such as vision and speech, the ability to coordinate multiple models to solve multiple sub-tasks that compose a complex one, and are weaker than some experts (e.g., fine-tuned models).\n","\n","### Context of the Problem:\n","\n","The problem today centers around the fact that while many AI models (like LLMs) have become very advanced when it comes to generating and understanding different forms and the semantics behind language, they still fail to perform tasks that require a diverse range of understandings and the use of multiple advanced drivers. The main challenge and context of this problem is not to develop a more advanced and capable model, but rather to uncover a way to integrate multiple models that are able to come together and solve real-world problems that are usually not just relegated to one specific task or depth.\n","\n","### Limitation About other Approaches:\n","\n","Many alternative approaches struggle with complex combinations of information such as pictures and sounds, or to bring together specialized solutions for a more general and broad problem. Other approaches fall short because they are unable to effectively and seamlessly bring together multiple AI tools to perform one unified and coherent task. There is no clear solution that currently accomodates for complex problems that have numerous specific needs whilst also ensuring that all tools employed flow together coherently to provide a logical solution.\n","\n","### Solution:\n","\n","By developing a control center that can direct different AI models from Hugging Face by using an LLM that utilizes API calls, the authors believe they can develop a way that can coherently leverage multiple models to solve more complex and multi-faceted tasks than the traditional approach."]},{"cell_type":"markdown","source":["# Background\n","\n","| Reference |Explanation |  Dataset/Input |Weakness |\n","| --- | --- | --- | --- |\n","| Alayrac, Jean-Baptiste, et al. [1] | They propose a Vision Language Model named Flamingo that can perform open-ended vision and language tasks| M3W, ALIGN, LTIP, and VTP datasets | Performance lags behind on classification tasks, trade-offs of few-shot learning methods, hallucination |\n","| Huang, Shaohan, et al. [2] | They propose Kosmos-1, a multimodal large language model that can perceive general modalities, learn in context and follow instructions | The Pile, Common Crawl, English LAION-2B, LAION-400M, and COYO-700M datasets | Worse performance in zero-shot and one-shot language tasks compared to a baseline LLM |\n","| Shen, Yongliang, et al. [3] | They propose an LLM-powered agent that disassembles tasks based on requests and assigns suitable models to the tasks | Requests submitted by annotators | Requires multiple interactions with LLMs thus increasing time costs and monetary costs |"],"metadata":{"id":"LirVYfDOOnJ4"},"id":"LirVYfDOOnJ4"},{"cell_type":"markdown","source":["# Methodology\n","\n","### Overview\n","\n","The proposed LLM-powered agent in this paper tackles a wide range of complex AI tasks by connecting LLMs (i.e., ChatGPT) and the ML community (i.e., Hugging Face) and can process inputs from different modalities.  More specifically, the LLM acts as a brain: on one hand, it disassembles tasks based on user requests, and on the other hand, assigns suitable models to the tasks according to the model description. By executing models and integrating results in the planned tasks, HuggingGPT can autonomously fulfill complex user requests. The whole process of HuggingGPT can be divided into four stages:\n","\n","1. **Task Planning:** Using ChatGPT to analyze the requests of users to understand their intention, and disassemble them into possible solvable tasks.\n","2. **Model Selection:** To solve the planned tasks, ChatGPT selects expert models that are hosted on Hugging Face based on model descriptions.\n","3. **Task Execution:** Invoke and execute each selected model, and return the results to ChatGPT.\n","4. **Response Generation:** Finally, ChatGPT is utilized to integrate the predictions from all models and generate responses for users.\n","\n","![HuggingGPT](https://drive.google.com/uc?id=1rYzruuiywruPcmZEzcB1JGhKtBdjCEw3)\n","\n","### Task Planning\n","\n","The AI assistant performs task parsing on user input, generating a list of tasks with the following format: `[{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]`. The \"dep\" field denotes the id of the previous task which generates a new resource upon which the current task relies. The tag \"<resource>-task_id\" represents the generated text, image, audio, or video from the dependency task with the corresponding task_id.\n","\n","#### Template for Task Planning\n","\n","| Name | Definitions |\n","| ---- | ------ |\n","| task | It represents the type of the parsed task. It covers different tasks in language, visual, video, audio, etc. |\n","| id | The unique identifier for task planning, which is used for references to dependent tasks and their generated resources. |\n","| dep | It defines the pre-requisite tasks required for execution. The task will be launched only when all the pre-requisite dependent tasks are finished. |\n","| args | It contains the list of required arguments for task execution. It contains three subfields populated with text, image, and audio resources according to the task type. They are resolved from either the user's request or the generated resources of the dependent tasks. |\n","\n","#### Example of Tasks and Models\n","\n","| Task | Candidate Models |\n","| ---- | ---------------- |\n","| Text-CLS | [cardiffnlp/twitter-robertabase-sentiment, ...] |\n","| Summarization | [bart-large-cnn, ...] |\n","| Translation | [t5-base, ...] |\n","| Image-to-Text | [nlpconnect/vit-gpt2-imagecaptioning, ...] |\n","| Segmentation | [facebook/detr-resnet-50-panoptic, ...] |\n","| Object-Detection | [facebook/detr-resnet-50, ...] |\n","| Text-to-Speech | [espnet/kanbayashi_ljspeech_vits, ...] |\n","| Text-to-Video | [damo-vilab/text-to-videoms-1.7b, ...] |\n","\n","#### Demonstration-based Parsing\n","\n","To better understand the intention and criteria for task planning, HuggingGPT incorporates multiple demonstrations in the prompt. Each demonstration consists of a user request and its corresponding output, which represents the expected sequence of parsed tasks.\n","\n","#### Example Top Level Prompt\n","\n","`#1 Task Planning Stage: The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\": task_id, \"dep\": dependency_task_id, \"args\": {\"text\": text or <GENERATED>-dep_id, \"image\": image_url or <GENERATED>-dep_id, \"audio\": audio_url or <GENERATED>-dep_id}}]. The special tag \"<GENERATED>-dep_id\" refer to the one generated text/image/audio in the dependency task (Please consider whether the dependency task generates resources of this type.) and \"dep_id\" must be in \"dep\" list. The \"dep\" field denotes the ids of the previous prerequisite tasks which generate a new resource that the current task relies on. The \"args\" field must in [\"text\", \"image\", \"audio\"], nothing else. The task MUST be selected from the following options: \"token-classification\", \"text2text-generation\", \"summarization\", \"translation\", \"question-answering\", \"conversational\", \"text-generation\", \"sentence-similarity\", \"tabular-classification\", \"object-detection\", \"image-classification\", \"image-to-image\", \"image-to-text\", \"text-to-image\", \"text-to-video\", \"visual-question-answering\", \"document-question-answering\", \"image-segmentation\", \"depth-estimation\", \"text-to-speech\", \"automatic-speech-recognition\", \"audio-to-audio\", \"audio-classification\", \"canny-control\", \"hed-control\", \"mlsd-control\", \"normal-control\", \"openpose-control\", \"canny-text-to-image\", \"depth-text-to-image\", \"hed-text-to-image\", \"mlsd-text-to-image\", \"normal-text-to-image\", \"openpose-text-to-image\", \"seg-text-to-image\". There may be multiple tasks of the same type. Think step by step about all the tasks needed to resolve the user's request. Parse out as few tasks as possible while ensuring that the user request can be resolved. Pay attention to the dependencies and order among tasks. If the user input can't be parsed, you need to reply empty JSON [].`\n","\n","#### Example Follow Up Prompt\n","\n","`The chat log [ {{context}} ] may contain the resources I mentioned. Now I input { {{input}} }. Pay attention to the input and output types of tasks and the dependencies between tasks.`\n","\n","### Model Selection\n","\n","Given the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: `{\"id\": \"id\", \"reason\": \"your detail reason for the choice\"}`.\n","\n","#### Example Top Level Prompt\n","\n","`#2 Model Selection Stage: Given the user request and the parsed tasks, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The assistant should focus more on the description of the model and find the model that has the most potential to solve requests and tasks. Also, prefer models with local inference endpoints for speed and stability.`\n","\n","#### Example Follow Up Prompt\n","\n","`Please choose the most suitable model from {{metas}} for the task {{task}}. The output must be in a strict JSON format: {\"id\": \"id\", \"reason\": \"your detail reasons for the choice\"}.`\n","\n","### Task Execution\n","\n","In this stage, HuggingGPT will automatically feed these task arguments into the\n","models, execute these models to obtain the inference results, and then send them back to the LLM.\n","\n","#### Resource Dependency\n","\n","HuggingGPT identifies the resources generated by the prerequisite task as <resource>-task_id, where task_id is the id of the prerequisite task. During the task planning stage, if some tasks are dependent on the outputs of previously executed tasks (e.g., task_id), HuggingGPT sets this symbol (i.e., <resource>-task_id) to the corresponding resource subfield in the arguments. Then in the task execution stage, HuggingGPT dynamically replaces this symbol with the resource generated by the prerequisite task.\n","\n","### Response Generation\n","\n","With the input and the inference results, the AI assistant needs to\n","describe the process and results. The previous stages can be formed as - User Input: `{{ User Input}}`, Task Planning: `{{ Tasks }}`, Model Selection: `{{ Model Assignment }}`, Task Execution: `{{ Predictions }}`.\n","\n","#### Example Top Level Prompt\n","\n","`#4 Response Generation Stage: With the task execution logs, the AI assistant needs to describe the process and inference results.`\n","\n","#### Example Follow Up Prompt\n","\n","`Yes. Please first think carefully and directly answer my request based on the inference results. Some of the inferences may not always turn out to be correct and require you to make careful consideration in making decisions. Then please detail your workflow including the used models and inference results for my request in your friendly tone. Please filter out information that is not relevant to my request. Tell me the complete path or urls of files in inference results. If there is nothing in the results, please tell me you can\\'t make it.`\n","\n","### Human Evaluation\n","\n","To properly evaluate the outputs of the different stages where we interact with the LLM agent, the authors of the original paper have used these three key metrics on a collection of 130 diverse user requests:\n","\n","* Passing Rate: to determine whether the planned task graph or selected model can be successfully\n","executed.\n","* Rationality: to assess whether the generated task sequence or selected tools align with user requests\n","in a rational manner.\n","* Success Rate: to verify if the final results satisfy the user's request.\n","\n","Due to a lack of access to these requests, and the time it would take us to develop our own test set, we wont be replicating them in this notebook. What we will do is show the results table in the paper:\n","\n","| LLM | Task Planning Passing Rate | Task Planning Rationality | Model Selection Passing Rate | Model Selection Rationality | Response Success Rate |\n","| --- | --- | --- | --- | --- | --- |\n","| Alpaca-13b | 51.04 | 32.17 | - | - | 6.92 |\n","| Vicuna-13b | 79.41 | 58.41 | - | - | 15.64 |\n","| GPT-3.5 | 91.22 | 78.47 | 93.89 | 84.29 | 63.08 |\n"],"metadata":{"id":"hnaO7ujdgrQL"},"id":"hnaO7ujdgrQL"},{"cell_type":"markdown","source":["# Implementation"],"metadata":{"id":"VwxFr1gnLRSG"},"id":"VwxFr1gnLRSG"},{"cell_type":"markdown","source":["### Dependencies Installation\n","\n","We install some important dependencies that don't come with Colab."],"metadata":{"id":"vvrpCw9hqdYR"},"id":"vvrpCw9hqdYR"},{"cell_type":"code","source":["!pip install tiktoken diffusers pydub"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A06B47SDnq71","executionInfo":{"status":"ok","timestamp":1713572877633,"user_tz":300,"elapsed":25578,"user":{"displayName":"Kunal Gurnani","userId":"08187749967867283185"}},"outputId":"4ee5cc1d-80cc-4555-90d2-9b975ac51cfb"},"id":"A06B47SDnq71","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tiktoken\n","  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting diffusers\n","  Downloading diffusers-0.27.2-py3-none-any.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydub\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (7.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.13.4)\n","Requirement already satisfied: huggingface-hub>=0.20.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.20.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.25.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.4.3)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (9.4.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers) (2023.6.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers) (4.66.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers) (4.11.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers) (24.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.18.1)\n","Installing collected packages: pydub, tiktoken, diffusers\n","Successfully installed diffusers-0.27.2 pydub-0.25.1 tiktoken-0.6.0\n"]}]},{"cell_type":"markdown","source":["### Mounting Google Drive to access files\n","\n","We save important files in Drive that are going to be used either as input or are going to be used to prompt the LLM agent. For example, `demo_parse_task.json` contains several examples for task parsing in json format.\n","\n","All the files to upload are:\n","\n","\n","*   p0_models.jsonl\n","*   demo_parse_task.json\n","*   demo_choose_model.json\n","*   demo_response_results.json\n","*   food.jpeg\n","\n","We have saved everything in this folder on our Drive: \"/MyDrive/Colab Notebooks/HuggingGPT\"\n","\n"],"metadata":{"id":"old1NG6ptpTg"},"id":"old1NG6ptpTg"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ThBWAn7QCfmZ","executionInfo":{"status":"ok","timestamp":1713572895443,"user_tz":300,"elapsed":17813,"user":{"displayName":"Kunal Gurnani","userId":"08187749967867283185"}},"outputId":"5fd5d078-140a-4396-db22-3f2d85ea0d78"},"id":"ThBWAn7QCfmZ","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["### Imports"],"metadata":{"id":"eneYdG8ktwKi"},"id":"eneYdG8ktwKi"},{"cell_type":"code","source":["import tiktoken\n","import requests\n","from diffusers.utils import load_image\n","import re\n","import json\n","import copy\n","import time\n","import threading\n","from queue import Queue\n","from huggingface_hub.inference_api import InferenceApi\n","from huggingface_hub.inference_api import ALL_TASKS\n","from io import BytesIO\n","import io\n","import uuid\n","import random\n","from PIL import Image, ImageDraw\n","import base64\n","from pydub import AudioSegment"],"metadata":{"id":"NXOkXWEtqZvq"},"id":"NXOkXWEtqZvq","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### OpenAI Key and HuggingFace Token\n","\n","Here we set our OpenAI Key and HuggingFace Token. These are needed to use their api."],"metadata":{"id":"OzL1m-Cqu9W_"},"id":"OzL1m-Cqu9W_"},{"cell_type":"code","source":["OPENAI_KEY = input(\"Your OpenAI Key: \")\n","HUGGINGFACE_TOKEN = input(\"Your HuggingFace Token: \")"],"metadata":{"id":"BdXSCDu4u82r"},"id":"BdXSCDu4u82r","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Setting up Constants\n","\n","Here we set some values that wont be changing, like our inference_mode is going to be only huggingface for this implementation, we set encodings and max context length for gpt-4, we read a file with information about existing models available in hugging face, and set hugging face request headers."],"metadata":{"id":"RHFucgZ4t05Z"},"id":"RHFucgZ4t05Z"},{"cell_type":"code","source":["inference_mode = \"huggingface\"\n","\n","encodings = {\n","    \"gpt-4\": tiktoken.get_encoding(\"cl100k_base\")\n","}\n","\n","max_length = {\n","    \"gpt-4\": 8192\n","}\n","\n","MODELS = [json.loads(line) for line in open(\"/content/drive/MyDrive/Colab Notebooks/HuggingGPT/p0_models.jsonl\", \"r\").readlines()]\n","MODELS_MAP = {}\n","for model in MODELS:\n","    tag = model[\"task\"]\n","    if tag not in MODELS_MAP:\n","        MODELS_MAP[tag] = []\n","    MODELS_MAP[tag].append(model)\n","\n","LLM = \"gpt-4\"\n","LLM_encoding = LLM\n","\n","HUGGINGFACE_HEADERS = {\n","    \"Authorization\": f\"Bearer {HUGGINGFACE_TOKEN}\",\n","}\n","\n","openai_endpoint = \"https://api.openai.com/v1/chat/completions\""],"metadata":{"id":"eXaIpU3dt1KW"},"id":"eXaIpU3dt1KW","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Prompts\n","\n","These template prompts are going to be used for properly \"chatting\" with the LLM agent and getting data in the structure we want, except for input_prompt, which is our example input prompt to show the capabilities of this method in this implementation. We will se them in action later."],"metadata":{"id":"ZQcpeyQ0wAaN"},"id":"ZQcpeyQ0wAaN"},{"cell_type":"code","source":["input_prompt = \"There is a picture of some pizza located in '/content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg' what topping does it have?\"\n","\n","parse_task_prompt = \"\"\"\n","The chat log [ {{context}} ] may contain the resources I mentioned. Now I input { {{input}} }. Pay attention to the input and output types of tasks and the dependencies between tasks.\n","\"\"\"\n","parse_task_tprompt = \"\"\"\n","#1 Task Planning Stage: The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\": task_id, \"dep\": dependency_task_id, \"args\": {\"text\": text or <GENERATED>-dep_id, \"image\": image_url or <GENERATED>-dep_id, \"audio\": audio_url or <GENERATED>-dep_id}}]. The special tag \"<GENERATED>-dep_id\" refer to the one generated text/image/audio in the dependency task (Please consider whether the dependency task generates resources of this type.) and \"dep_id\" must be in \"dep\" list. The \"dep\" field denotes the ids of the previous prerequisite tasks which generate a new resource that the current task relies on. The \"args\" field must in [\"text\", \"image\", \"audio\"], nothing else. The task MUST be selected from the following options: \"token-classification\", \"text2text-generation\", \"summarization\", \"translation\", \"question-answering\", \"conversational\", \"text-generation\", \"sentence-similarity\", \"tabular-classification\", \"object-detection\", \"image-classification\", \"image-to-image\", \"image-to-text\", \"text-to-image\", \"text-to-video\", \"visual-question-answering\", \"document-question-answering\", \"image-segmentation\", \"depth-estimation\", \"text-to-speech\", \"automatic-speech-recognition\", \"audio-to-audio\", \"audio-classification\", \"canny-control\", \"hed-control\", \"mlsd-control\", \"normal-control\", \"openpose-control\", \"canny-text-to-image\", \"depth-text-to-image\", \"hed-text-to-image\", \"mlsd-text-to-image\", \"normal-text-to-image\", \"openpose-text-to-image\", \"seg-text-to-image\". There may be multiple tasks of the same type. Think step by step about all the tasks needed to resolve the user's request. Parse out as few tasks as possible while ensuring that the user request can be resolved. Pay attention to the dependencies and order among tasks. If the user input can't be parsed, you need to reply empty JSON [].\n","\"\"\"\n","\n","choose_model_prompt = 'Please choose the most suitable model from {{metas}} for the task {{task}}. The output must be in a strict JSON format: {\"id\": \"id\", \"reason\": \"your detail reasons for the choice\"}.'\n","choose_model_tprompt = \"#2 Model Selection Stage: Given the user request and the parsed tasks, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The assistant should focus more on the description of the model and find the model that has the most potential to solve requests and tasks. Also, prefer models with local inference endpoints for speed and stability.\"\n","\n","response_results_prompt = 'Yes. Please first think carefully and directly answer my request based on the inference results. Some of the inferences may not always turn out to be correct and require you to make careful consideration in making decisions. Then please detail your workflow including the used models and inference results for my request in your friendly tone. Please filter out information that is not relevant to my request. Tell me the complete path or urls of files in inference results. If there is nothing in the results, please tell me you can\\'t make it. }'\n","response_results_tprompt = \"#4 Response Generation Stage: With the task execution logs, the AI assistant needs to describe the process and inference results.\""],"metadata":{"id":"OJmNgj8swID3"},"id":"OJmNgj8swID3","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Task examples for agent\n","\n","For the agent to know how to respond to different prompts, it is better to give it some examples. This technique is called Few-Shot Prompting. Few-shot prompting can be used as a technique to enable in-context learning where we provide demonstrations in the prompt to steer the model to better performance."],"metadata":{"id":"ZQUlW_A235q9"},"id":"ZQUlW_A235q9"},{"cell_type":"code","source":["parse_task_demos_or_presteps = open(\"/content/drive/MyDrive/Colab Notebooks/HuggingGPT/demo_parse_task.json\", \"r\").read()\n","choose_model_demos_or_presteps = open(\"/content/drive/MyDrive/Colab Notebooks/HuggingGPT/demo_choose_model.json\", \"r\").read()\n","response_results_demos_or_presteps = open(\"/content/drive/MyDrive/Colab Notebooks/HuggingGPT/demo_response_results.json\", \"r\").read()"],"metadata":{"id":"Dy6lJTD236Jq"},"id":"Dy6lJTD236Jq","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(parse_task_demos_or_presteps)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GZCfHwB3Si1r","executionInfo":{"status":"ok","timestamp":1713572948903,"user_tz":300,"elapsed":6,"user":{"displayName":"Kunal Gurnani","userId":"08187749967867283185"}},"outputId":"91bc7519-1b5f-4d15-c5d9-62b530407c9c"},"id":"GZCfHwB3Si1r","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[\n","    {\n","        \"role\": \"user\",\n","        \"content\": \"Give you some pictures e1.jpg, e2.png, e3.jpg, help me count the number of sheep?\"\n","    },\n","    {\n","        \"role\": \"assistant\",\n","        \"content\": \"[{\\\"task\\\": \\\"image-to-text\\\", \\\"id\\\": 0, \\\"dep\\\": [-1], \\\"args\\\": {\\\"image\\\": \\\"e1.jpg\\\" }}, {\\\"task\\\": \\\"object-detection\\\", \\\"id\\\": 1, \\\"dep\\\": [-1], \\\"args\\\": {\\\"image\\\": \\\"e1.jpg\\\" }}, {\\\"task\\\": \\\"visual-question-answering\\\", \\\"id\\\": 2, \\\"dep\\\": [1], \\\"args\\\": {\\\"image\\\": \\\"<GENERATED>-1\\\", \\\"text\\\": \\\"How many sheep in the picture\\\"}} }}, {\\\"task\\\": \\\"image-to-text\\\", \\\"id\\\": 3, \\\"dep\\\": [-1], \\\"args\\\": {\\\"image\\\": \\\"e2.png\\\" }}, {\\\"task\\\": \\\"object-detection\\\", \\\"id\\\": 4, \\\"dep\\\": [-1], \\\"args\\\": {\\\"image\\\": \\\"e2.png\\\" }}, {\\\"task\\\": \\\"visual-question-answering\\\", \\\"id\\\": 5, \\\"dep\\\": [4], \\\"args\\\": {\\\"image\\\": \\\"<GENERATED>-4\\\", \\\"text\\\": \\\"How many sheep in the picture\\\"}} }}, {\\\"task\\\": \\\"image-to-text\\\", \\\"id\\\": 6, \\\"dep\\\": [-1], \\\"args\\\": {\\\"image\\\": \\\"e3.jpg\\\" }},  {\\\"task\\\": \\\"object-detection\\\", \\\"id\\\": 7, \\\"dep\\\": [-1], \\\"args\\\": {\\\"image\\\": \\\"e3.jpg\\\" }}, {\\\"task\\\": \\\"visual-question-answering\\\", \\\"id\\\": 8, \\\"dep\\\": [7], \\\"args\\\": {\\\"image\\\": \\\"<GENERATED>-7\\\", \\\"text\\\": \\\"How many sheep in the picture\\\"}}]\"\n","    },\n","\n","    {\n","        \"role\":\"user\",\n","        \"content\":\"Look at /e.jpg, can you tell me how many objects in the picture? Give me a picture and video similar to this one.\"\n","    },\n","    {\n","        \"role\":\"assistant\",\n","        \"content\":\"[{\\\"task\\\": \\\"image-to-text\\\", \\\"id\\\": 0, \\\"dep\\\": [-1], \\\"args\\\": {\\\"image\\\": \\\"/e.jpg\\\" }}, {\\\"task\\\": \\\"object-detection\\\", \\\"id\\\": 1, \\\"dep\\\": [-1], \\\"args\\\": {\\\"image\\\": \\\"/e.jpg\\\" }}, {\\\"task\\\": \\\"visual-question-answering\\\", \\\"id\\\": 2, \\\"dep\\\": [1], \\\"args\\\": {\\\"image\\\": \\\"<GENERATED>-1\\\", \\\"text\\\": \\\"how many objects in the picture?\\\" }}, {\\\"task\\\": \\\"text-to-image\\\", \\\"id\\\": 3, \\\"dep\\\": [0], \\\"args\\\": {\\\"text\\\": \\\"<GENERATED-0>\\\" }}, {\\\"task\\\": \\\"image-to-image\\\", \\\"id\\\": 4, \\\"dep\\\": [-1], \\\"args\\\": {\\\"image\\\": \\\"/e.jpg\\\" }}, {\\\"task\\\": \\\"text-to-video\\\", \\\"id\\\": 5, \\\"dep\\\": [0], \\\"args\\\": {\\\"text\\\": \\\"<GENERATED-0>\\\" }}]\"\n","    },\n","\n","    {\n","        \"role\":\"user\",\n","        \"content\":\"given a document /images/e.jpeg, answer me what is the student amount? And describe the image with your voice\"\n","    },\n","    {\n","        \"role\":\"assistant\",\n","        \"content\":\"{\\\"task\\\": \\\"document-question-answering\\\", \\\"id\\\": 0, \\\"dep\\\": [-1], \\\"args\\\": {\\\"image\\\": \\\"/images/e.jpeg\\\", \\\"text\\\": \\\"what is the student amount?\\\" }}, {\\\"task\\\": \\\"visual-question-answering\\\", \\\"id\\\": 1, \\\"dep\\\": [-1], \\\"args\\\": {\\\"image\\\": \\\"/images/e.jpeg\\\", \\\"text\\\": \\\"what is the student amount?\\\" }}, {\\\"task\\\": \\\"image-to-text\\\", \\\"id\\\": 2, \\\"dep\\\": [-1], \\\"args\\\": {\\\"image\\\": \\\"/images/e.jpg\\\" }}, {\\\"task\\\": \\\"text-to-speech\\\", \\\"id\\\": 3, \\\"dep\\\": [2], \\\"args\\\": {\\\"text\\\": \\\"<GENERATED>-2\\\" }}]\"\n","    },\n","\n","    {\n","        \"role\": \"user\",\n","        \"content\": \"Given an image /example.jpg, first generate a hed image, then based on the hed image generate a new image where a girl is reading a book\"\n","    },\n","    {\n","        \"role\": \"assistant\",\n","        \"content\": \"[{\\\"task\\\": \\\"openpose-control\\\", \\\"id\\\": 0, \\\"dep\\\": [-1], \\\"args\\\": {\\\"image\\\": \\\"/example.jpg\\\" }},  {\\\"task\\\": \\\"openpose-text-to-image\\\", \\\"id\\\": 1, \\\"dep\\\": [0], \\\"args\\\": {\\\"text\\\": \\\"a girl is reading a book\\\", \\\"image\\\": \\\"<GENERATED>-0\\\" }}]\"\n","    },\n","\n","    {\n","        \"role\": \"user\",\n","        \"content\": \"please show me a video and an image of (based on the text) 'a boy is running' and dub it\"\n","    },\n","    {\n","        \"role\": \"assistant\",\n","        \"content\": \"[{\\\"task\\\": \\\"text-to-video\\\", \\\"id\\\": 0, \\\"dep\\\": [-1], \\\"args\\\": {\\\"text\\\": \\\"a boy is running\\\" }}, {\\\"task\\\": \\\"text-to-speech\\\", \\\"id\\\": 1, \\\"dep\\\": [-1], \\\"args\\\": {\\\"text\\\": \\\"a boy is running\\\" }}, {\\\"task\\\": \\\"text-to-image\\\", \\\"id\\\": 2, \\\"dep\\\": [-1], \\\"args\\\": {\\\"text\\\": \\\"a boy is running\\\" }}]\"\n","    },\n","\n","\n","    {\n","        \"role\": \"user\",\n","        \"content\": \"please show me a joke and an image of cat\"\n","    },\n","    {\n","        \"role\": \"assistant\",\n","        \"content\": \"[{\\\"task\\\": \\\"conversational\\\", \\\"id\\\": 0, \\\"dep\\\": [-1], \\\"args\\\": {\\\"text\\\": \\\"please show me a joke of cat\\\" }}, {\\\"task\\\": \\\"text-to-image\\\", \\\"id\\\": 1, \\\"dep\\\": [-1], \\\"args\\\": {\\\"text\\\": \\\"a photo of cat\\\" }}]\"\n","    }\n","]\n","\n"]}]},{"cell_type":"markdown","source":["### Helper Functions (Token-related)\n","\n","Here we define some helper methods on top of the tiktoken library that will help us transform text to tokens, get token counts, among others. A good documentation is located at https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb."],"metadata":{"id":"kabAeJTrxuiR"},"id":"kabAeJTrxuiR"},{"cell_type":"code","source":["def count_tokens(model_name, text):\n","    return len(encodings[model_name].encode(text))\n","\n","def get_max_context_length(model_name):\n","    return max_length[model_name]\n","\n","def get_token_ids_for_task_parsing(model_name):\n","    text = '''{\"task\": \"text-classification\",  \"token-classification\", \"text2text-generation\", \"summarization\", \"translation\",  \"question-answering\", \"conversational\", \"text-generation\", \"sentence-similarity\", \"tabular-classification\", \"object-detection\", \"image-classification\", \"image-to-image\", \"image-to-text\", \"text-to-image\", \"visual-question-answering\", \"document-question-answering\", \"image-segmentation\", \"text-to-speech\", \"text-to-video\", \"automatic-speech-recognition\", \"audio-to-audio\", \"audio-classification\", \"canny-control\", \"hed-control\", \"mlsd-control\", \"normal-control\", \"openpose-control\", \"canny-text-to-image\", \"depth-text-to-image\", \"hed-text-to-image\", \"mlsd-text-to-image\", \"normal-text-to-image\", \"openpose-text-to-image\", \"seg-text-to-image\", \"args\", \"text\", \"path\", \"dep\", \"id\", \"<GENERATED>-\"}'''\n","    res = encodings[model_name].encode(text)\n","    res = list(set(res))\n","    return res\n","\n","def get_token_ids_for_choose_model(model_name):\n","    text = '''{\"id\": \"reason\"}'''\n","    res = encodings[model_name].encode(text)\n","    res = list(set(res))\n","    return res\n","\n","choose_model_highlight_ids = get_token_ids_for_choose_model(LLM_encoding)\n","task_parsing_highlight_ids = get_token_ids_for_task_parsing(LLM_encoding)"],"metadata":{"id":"h-JWWQZ8yEAB"},"id":"h-JWWQZ8yEAB","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Helper Functions (Chat-related)"],"metadata":{"id":"-TGYo4SmzFms"},"id":"-TGYo4SmzFms"},{"cell_type":"markdown","source":["These functions here are used to send requests to OpenAI's api."],"metadata":{"id":"nlsL6mOuUi21"},"id":"nlsL6mOuUi21"},{"cell_type":"code","source":["def send_request(data):\n","    api_key = data.pop(\"api_key\")\n","    api_endpoint = data.pop(\"api_endpoint\")\n","    HEADER = {\n","        \"Authorization\": f\"Bearer {api_key}\",\n","        \"Content-Type\": \"application/json\"\n","    }\n","    response = requests.post(api_endpoint, json=data, headers=HEADER, proxies=None)\n","    if \"error\" in response.json():\n","        return response.json()\n","    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n","\n","def chitchat(messages, api_key, api_endpoint):\n","    data = {\n","        \"model\": LLM,\n","        \"messages\": messages,\n","        \"api_key\": api_key,\n","        \"api_endpoint\": api_endpoint\n","    }\n","    return send_request(data)"],"metadata":{"id":"-5sjErxC07Mp"},"id":"-5sjErxC07Mp","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["These functions here are used to apply transformations to data be it images or text, and to extract information from text. These are used during the execution of the task."],"metadata":{"id":"HeP3YgdjVIgy"},"id":"HeP3YgdjVIgy"},{"cell_type":"code","source":["def replace_slot(text, entries):\n","    for key, value in entries.items():\n","        if not isinstance(value, str):\n","            value = str(value)\n","        text = text.replace(\"{{\" + key +\"}}\", value.replace('\"', \"'\").replace('\\n', \"\"))\n","    return text\n","\n","def find_json(s):\n","    s = s.replace(\"\\'\", \"\\\"\")\n","    start = s.find(\"{\")\n","    end = s.rfind(\"}\")\n","    res = s[start:end+1]\n","    res = res.replace(\"\\n\", \"\")\n","    return res\n","\n","def field_extract(s, field):\n","    try:\n","        field_rep = re.compile(f'{field}.*?:.*?\"(.*?)\"', re.IGNORECASE)\n","        extracted = field_rep.search(s).group(1).replace(\"\\\"\", \"\\'\")\n","    except:\n","        field_rep = re.compile(f'{field}:\\ *\"(.*?)\"', re.IGNORECASE)\n","        extracted = field_rep.search(s).group(1).replace(\"\\\"\", \"\\'\")\n","    return extracted\n","\n","def image_to_bytes(img_url):\n","    img_byte = io.BytesIO()\n","    type = img_url.split(\".\")[-1]\n","    load_image(img_url).save(img_byte, format=\"png\")\n","    img_data = img_byte.getvalue()\n","    return img_data\n","\n","def get_id_reason(choose_str):\n","    reason = field_extract(choose_str, \"reason\")\n","    id = field_extract(choose_str, \"id\")\n","    choose = {\"id\": id, \"reason\": reason}\n","    return id.strip(), reason.strip(), choose\n","\n","def collect_result(command, choose, inference_result):\n","    result = {\"task\": command}\n","    result[\"inference result\"] = inference_result\n","    result[\"choose model result\"] = choose\n","    return result"],"metadata":{"id":"m64dYV1-07KB"},"id":"m64dYV1-07KB","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["These functions here transform the result from the LLM agent into something usable by us. It orders all the tasks properly so we can have a nice array of tasks to execute."],"metadata":{"id":"CInSfwGmVfN5"},"id":"CInSfwGmVfN5"},{"cell_type":"code","source":["def fix_dep(tasks):\n","    for task in tasks:\n","        args = task[\"args\"]\n","        task[\"dep\"] = []\n","        for k, v in args.items():\n","            if \"<GENERATED>\" in v:\n","                dep_task_id = int(v.split(\"-\")[1])\n","                if dep_task_id not in task[\"dep\"]:\n","                    task[\"dep\"].append(dep_task_id)\n","        if len(task[\"dep\"]) == 0:\n","            task[\"dep\"] = [-1]\n","    return tasks\n","\n","def unfold(tasks):\n","    try:\n","        for task in tasks:\n","            for key, value in task[\"args\"].items():\n","                if \"<GENERATED>\" in value:\n","                    generated_items = value.split(\",\")\n","                    if len(generated_items) > 1:\n","                        for item in generated_items:\n","                            new_task = copy.deepcopy(task)\n","                            dep_task_id = int(item.split(\"-\")[1])\n","                            new_task[\"dep\"] = [dep_task_id]\n","                            new_task[\"args\"][key] = item\n","                            tasks.append(new_task)\n","                        tasks.remove(task)\n","    except Exception as e:\n","        print(e)\n","\n","    return tasks"],"metadata":{"id":"TqVRwfLv06_E"},"id":"TqVRwfLv06_E","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["These functions here check if certain models are available in the hugging face inference api, and return them accordingly."],"metadata":{"id":"pN8SNjQzV7Ni"},"id":"pN8SNjQzV7Ni"},{"cell_type":"code","source":["def get_model_status(model_id, url, headers, queue = None):\n","    endpoint_type = \"huggingface\" if \"huggingface\" in url else \"local\"\n","    if \"huggingface\" in url:\n","        r = requests.get(url, headers=headers, proxies=None)\n","    else:\n","        r = requests.get(url)\n","\n","    if r.status_code == 200 and \"loaded\" in r.json():\n","        if queue:\n","            queue.put((model_id, True, endpoint_type))\n","        return True\n","    else:\n","        if queue:\n","            queue.put((model_id, False, None))\n","        return False\n","\n","def get_avaliable_models(candidates, topk=5):\n","    all_available_models = {\"local\": [], \"huggingface\": []}\n","    threads = []\n","    result_queue = Queue()\n","\n","    for candidate in candidates:\n","        model_id = candidate[\"id\"]\n","\n","        if inference_mode != \"local\":\n","            huggingfaceStatusUrl = f\"https://api-inference.huggingface.co/status/{model_id}\"\n","            thread = threading.Thread(target=get_model_status, args=(model_id, huggingfaceStatusUrl, HUGGINGFACE_HEADERS, result_queue))\n","            threads.append(thread)\n","            thread.start()\n","\n","    result_count = len(threads)\n","    while result_count:\n","        model_id, status, endpoint_type = result_queue.get()\n","        if status and model_id not in all_available_models:\n","            all_available_models[endpoint_type].append(model_id)\n","        if len(all_available_models[\"local\"] + all_available_models[\"huggingface\"]) >= topk:\n","            break\n","        result_count -= 1\n","\n","    for thread in threads:\n","        thread.join()\n","\n","    return all_available_models"],"metadata":{"id":"H4E5k8op068O"},"id":"H4E5k8op068O","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once a model has been chosen, we use these functions to call the hugging face api and send in the correct parameters for these model's inference. We structure them in the way thay need to be based on the type of task to be executed."],"metadata":{"id":"dU2pQDbSbiLR"},"id":"dU2pQDbSbiLR"},{"cell_type":"code","source":["def huggingface_model_inference(model_id, data, task):\n","    task_url = f\"https://api-inference.huggingface.co/models/{model_id}\" # InferenceApi does not yet support some tasks\n","    inference = InferenceApi(repo_id=model_id, token=HUGGINGFACE_TOKEN)\n","\n","    # NLP tasks\n","    if task == \"question-answering\":\n","        inputs = {\"question\": data[\"text\"], \"context\": (data[\"context\"] if \"context\" in data else \"\" )}\n","        result = inference(inputs)\n","    if task == \"sentence-similarity\":\n","        inputs = {\"source_sentence\": data[\"text1\"], \"target_sentence\": data[\"text2\"]}\n","        result = inference(inputs)\n","    if task in [\"text-classification\",  \"token-classification\", \"text2text-generation\", \"summarization\", \"translation\", \"conversational\", \"text-generation\"]:\n","        inputs = data[\"text\"]\n","        result = inference(inputs)\n","\n","    # CV tasks\n","    if task == \"visual-question-answering\" or task == \"document-question-answering\":\n","        img_url = data[\"image\"]\n","        text = data[\"text\"]\n","        img_data = image_to_bytes(img_url)\n","        img_base64 = base64.b64encode(img_data).decode(\"utf-8\")\n","        json_data = {}\n","        json_data[\"inputs\"] = {}\n","        json_data[\"inputs\"][\"question\"] = text\n","        json_data[\"inputs\"][\"image\"] = img_base64\n","        json_data[\"wait_for_model\"] = True\n","        result = requests.post(task_url, headers=HUGGINGFACE_HEADERS, json=json_data).json()\n","        # result = inference(inputs) # not support\n","\n","    if task == \"image-to-image\":\n","        img_url = data[\"image\"]\n","        img_data = image_to_bytes(img_url)\n","        # result = inference(data=img_data) # not support\n","        HUGGINGFACE_HEADERS[\"Content-Length\"] = str(len(img_data))\n","        r = requests.post(task_url, headers=HUGGINGFACE_HEADERS, data=img_data)\n","        result = r.json()\n","        if \"path\" in result:\n","            result[\"generated image\"] = result.pop(\"path\")\n","\n","    if task == \"text-to-image\":\n","        inputs = data[\"text\"]\n","        img = inference(inputs)\n","        name = str(uuid.uuid4())[:4]\n","        img.save(f\"/content/drive/MyDrive/Colab Notebooks/HuggingGPT/{name}.png\")\n","        result = {}\n","        result[\"generated image\"] = f\"/images/{name}.png\"\n","\n","    if task == \"image-segmentation\":\n","        img_url = data[\"image\"]\n","        img_data = image_to_bytes(img_url)\n","        image = Image.open(BytesIO(img_data))\n","        predicted = inference(data=img_data)\n","        colors = []\n","        for i in range(len(predicted)):\n","            colors.append((random.randint(100, 255), random.randint(100, 255), random.randint(100, 255), 155))\n","        for i, pred in enumerate(predicted):\n","            label = pred[\"label\"]\n","            mask = pred.pop(\"mask\").encode(\"utf-8\")\n","            mask = base64.b64decode(mask)\n","            mask = Image.open(BytesIO(mask), mode='r')\n","            mask = mask.convert('L')\n","\n","            layer = Image.new('RGBA', mask.size, colors[i])\n","            image.paste(layer, (0, 0), mask)\n","        name = str(uuid.uuid4())[:4]\n","        image.save(f\"/content/drive/MyDrive/Colab Notebooks/HuggingGPT/{name}.jpg\")\n","        result = {}\n","        result[\"generated image\"] = f\"/images/{name}.jpg\"\n","        result[\"predicted\"] = predicted\n","\n","    if task == \"object-detection\":\n","        img_url = data[\"image\"]\n","        img_data = image_to_bytes(img_url)\n","        predicted = inference(data=img_data)\n","        image = Image.open(BytesIO(img_data))\n","        draw = ImageDraw.Draw(image)\n","        labels = list(item['label'] for item in predicted)\n","        color_map = {}\n","        for label in labels:\n","            if label not in color_map:\n","                color_map[label] = (random.randint(0, 255), random.randint(0, 100), random.randint(0, 255))\n","        for label in predicted:\n","            box = label[\"box\"]\n","            draw.rectangle(((box[\"xmin\"], box[\"ymin\"]), (box[\"xmax\"], box[\"ymax\"])), outline=color_map[label[\"label\"]], width=2)\n","            draw.text((box[\"xmin\"]+5, box[\"ymin\"]-15), label[\"label\"], fill=color_map[label[\"label\"]])\n","        name = str(uuid.uuid4())[:4]\n","        image.save(f\"/content/drive/MyDrive/Colab Notebooks/HuggingGPT/{name}.jpg\")\n","        result = {}\n","        result[\"generated image\"] = f\"/images/{name}.jpg\"\n","        result[\"predicted\"] = predicted\n","\n","    if task in [\"image-classification\"]:\n","        img_url = data[\"image\"]\n","        img_data = image_to_bytes(img_url)\n","        result = inference(data=img_data)\n","\n","    if task == \"image-to-text\":\n","        img_url = data[\"image\"]\n","        img_data = image_to_bytes(img_url)\n","        HUGGINGFACE_HEADERS[\"Content-Length\"] = str(len(img_data))\n","        r = requests.post(task_url, headers=HUGGINGFACE_HEADERS, data=img_data, proxies=None)\n","        result = {}\n","        if \"generated_text\" in r.json()[0]:\n","            result[\"generated text\"] = r.json()[0].pop(\"generated_text\")\n","\n","    # AUDIO tasks\n","    if task == \"text-to-speech\":\n","        inputs = data[\"text\"]\n","        response = inference(inputs, raw_response=True)\n","        # response = requests.post(task_url, headers=HUGGINGFACE_HEADERS, json={\"inputs\": text})\n","        name = str(uuid.uuid4())[:4]\n","        with open(f\"/content/drive/MyDrive/Colab Notebooks/HuggingGPT/{name}.flac\", \"wb\") as f:\n","            f.write(response.content)\n","        result = {\"generated audio\": f\"/audios/{name}.flac\"}\n","    if task in [\"automatic-speech-recognition\", \"audio-to-audio\", \"audio-classification\"]:\n","        audio_url = data[\"audio\"]\n","        audio_data = requests.get(audio_url, timeout=10).content\n","        response = inference(data=audio_data, raw_response=True)\n","        result = response.json()\n","        if task == \"audio-to-audio\":\n","            content = None\n","            type = None\n","            for k, v in result[0].items():\n","                if k == \"blob\":\n","                    content = base64.b64decode(v.encode(\"utf-8\"))\n","                if k == \"content-type\":\n","                    type = \"audio/flac\".split(\"/\")[-1]\n","            audio = AudioSegment.from_file(BytesIO(content))\n","            name = str(uuid.uuid4())[:4]\n","            audio.export(f\"/content/drive/MyDrive/Colab Notebooks/HuggingGPT/{name}.{type}\", format=type)\n","            result = {\"generated audio\": f\"/audios/{name}.{type}\"}\n","    return result\n","\n","def model_inference(model_id, data, hosted_on, task):\n","    try:\n","        inference_result = huggingface_model_inference(model_id, data, task)\n","    except Exception as e:\n","        print(e)\n","        inference_result = {\"error\":{\"message\": str(e)}}\n","    return inference_result"],"metadata":{"id":"dkDB_4q9061e"},"id":"dkDB_4q9061e","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This function is used to choose a model. It calls in our LLM agent and asks it which model is best suited."],"metadata":{"id":"Nd91eweFcAiK"},"id":"Nd91eweFcAiK"},{"cell_type":"code","source":["def choose_model(input, task, metas, api_key, api_endpoint):\n","    prompt = replace_slot(choose_model_prompt, {\n","        \"input\": input,\n","        \"task\": task,\n","        \"metas\": metas,\n","    })\n","    print(\"Prompt we use to choose model given a list of candidates\")\n","    print(prompt)\n","    print(\"\")\n","    demos_or_presteps = replace_slot(choose_model_demos_or_presteps, {\n","        \"input\": input,\n","        \"task\": task,\n","        \"metas\": metas\n","    })\n","    messages = json.loads(demos_or_presteps)\n","    messages.insert(0, {\"role\": \"system\", \"content\": choose_model_tprompt})\n","    messages.append({\"role\": \"user\", \"content\": prompt})\n","    print(\"Promp with examples that we send:\")\n","    print(messages)\n","    print(\"\")\n","    data = {\n","        \"model\": LLM,\n","        \"messages\": messages,\n","        \"temperature\": 0,\n","        \"logit_bias\": {item: 5 for item in choose_model_highlight_ids},\n","        \"api_key\": api_key,\n","        \"api_endpoint\": api_endpoint\n","    }\n","    return send_request(data)"],"metadata":{"id":"t7z86DkV06j_"},"id":"t7z86DkV06j_","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is our main function to run. We call all the other functions from this one in one way or another."],"metadata":{"id":"kDITJ9NHcb_8"},"id":"kDITJ9NHcb_8"},{"cell_type":"code","source":["def run_task(input, command, results, api_key, api_endpoint):\n","    id = command[\"id\"]\n","    args = command[\"args\"]\n","    task = command[\"task\"]\n","    deps = command[\"dep\"]\n","    if deps[0] != -1:\n","        dep_tasks = [results[dep] for dep in deps]\n","    else:\n","        dep_tasks = []\n","\n","    print(f\"Run task: {id} - {task}\")\n","    print(\"\")\n","    print(\"Deps: \" + json.dumps(dep_tasks))\n","    print(\"\")\n","\n","    if deps[0] != -1:\n","        if \"image\" in args and \"<GENERATED>-\" in args[\"image\"]:\n","            resource_id = int(args[\"image\"].split(\"-\")[1])\n","            if \"generated image\" in results[resource_id][\"inference result\"]:\n","                args[\"image\"] = results[resource_id][\"inference result\"][\"generated image\"]\n","        if \"audio\" in args and \"<GENERATED>-\" in args[\"audio\"]:\n","            resource_id = int(args[\"audio\"].split(\"-\")[1])\n","            if \"generated audio\" in results[resource_id][\"inference result\"]:\n","                args[\"audio\"] = results[resource_id][\"inference result\"][\"generated audio\"]\n","        if \"text\" in args and \"<GENERATED>-\" in args[\"text\"]:\n","            resource_id = int(args[\"text\"].split(\"-\")[1])\n","            if \"generated text\" in results[resource_id][\"inference result\"]:\n","                args[\"text\"] = results[resource_id][\"inference result\"][\"generated text\"]\n","\n","    text = image = audio = None\n","    for dep_task in dep_tasks:\n","        if \"generated text\" in dep_task[\"inference result\"]:\n","            text = dep_task[\"inference result\"][\"generated text\"]\n","            print(\"Detect the generated text of dependency task (from results):\" + text)\n","            print(\"\")\n","        elif \"text\" in dep_task[\"task\"][\"args\"]:\n","            text = dep_task[\"task\"][\"args\"][\"text\"]\n","            print(\"Detect the text of dependency task (from args): \" + text)\n","            print(\"\")\n","        if \"generated image\" in dep_task[\"inference result\"]:\n","            image = dep_task[\"inference result\"][\"generated image\"]\n","            print(\"Detect the generated image of dependency task (from results): \" + image)\n","            print(\"\")\n","        elif \"image\" in dep_task[\"task\"][\"args\"]:\n","            image = dep_task[\"task\"][\"args\"][\"image\"]\n","            print(\"Detect the image of dependency task (from args): \" + image)\n","            print(\"\")\n","        if \"generated audio\" in dep_task[\"inference result\"]:\n","            audio = dep_task[\"inference result\"][\"generated audio\"]\n","            print(\"Detect the generated audio of dependency task (from results): \" + audio)\n","            print(\"\")\n","        elif \"audio\" in dep_task[\"task\"][\"args\"]:\n","            audio = dep_task[\"task\"][\"args\"][\"audio\"]\n","            print(\"Detect the audio of dependency task (from args): \" + audio)\n","            print(\"\")\n","\n","    if \"image\" in args and \"<GENERATED>\" in args[\"image\"]:\n","        if image:\n","            args[\"image\"] = image\n","    if \"audio\" in args and \"<GENERATED>\" in args[\"audio\"]:\n","        if audio:\n","            args[\"audio\"] = audio\n","    if \"text\" in args and \"<GENERATED>\" in args[\"text\"]:\n","        if text:\n","            args[\"text\"] = text\n","\n","    for resource in [\"image\", \"audio\"]:\n","        if resource in args and not args[resource].startswith(\"public/\") and len(args[resource]) > 0 and not args[resource].startswith(\"http\"):\n","            args[resource] = f\"{args[resource]}\"\n","\n","    if \"-text-to-image\" in command['task'] and \"text\" not in args:\n","        print(\"control-text-to-image task, but text is empty, so we use control-generation instead.\")\n","        print(\"\")\n","        control = task.split(\"-\")[0]\n","\n","        if control == \"seg\":\n","            task = \"image-segmentation\"\n","            command['task'] = task\n","        elif control == \"depth\":\n","            task = \"depth-estimation\"\n","            command['task'] = task\n","        else:\n","            task = f\"{control}-control\"\n","\n","    command[\"args\"] = args\n","    print(f\"parsed task: {command}\")\n","    print(\"\")\n","\n","    if task.endswith(\"-text-to-image\") or task.endswith(\"-control\"):\n","        if inference_mode != \"huggingface\":\n","            if task.endswith(\"-text-to-image\"):\n","                control = task.split(\"-\")[0]\n","                best_model_id = f\"lllyasviel/sd-controlnet-{control}\"\n","            else:\n","                best_model_id = task\n","            hosted_on = \"local\"\n","            reason = \"ControlNet is the best model for this task.\"\n","            choose = {\"id\": best_model_id, \"reason\": reason}\n","            print(f\"chosen model: {choose}\")\n","            print(\"\")\n","        else:\n","            print(f\"Task {command['task']} is not available. ControlNet need to be deployed locally.\")\n","            print(\"\")\n","            inference_result = {\"error\": f\"service related to ControlNet is not available.\"}\n","            results[id] = collect_result(command, \"\", inference_result)\n","            return False\n","    elif task in [\"summarization\", \"translation\", \"conversational\", \"text-generation\", \"text2text-generation\"]:\n","        best_model_id = \"ChatGPT\"\n","        reason = \"ChatGPT performs well on some NLP tasks as well.\"\n","        choose = {\"id\": best_model_id, \"reason\": reason}\n","        messages = [{\n","            \"role\": \"user\",\n","            \"content\": f\"[ {input} ] contains a task in JSON format {command}. Now you are a {command['task']} system, the arguments are {command['args']}. Just help me do {command['task']} and give me the result. The result must be in text form without any urls.\"\n","        }]\n","        response = chitchat(messages, api_key, api_endpoint)\n","        results[id] = collect_result(command, choose, {\"response\": response})\n","        return True\n","    else:\n","        if task not in MODELS_MAP:\n","            print(f\"no available models on {task} task.\")\n","            print(\"\")\n","            inference_result = {\"error\": f\"{command['task']} not found in available tasks.\"}\n","            results[id] = collect_result(command, \"\", inference_result)\n","            return False\n","\n","        candidates = MODELS_MAP[task][:10]\n","        all_avaliable_models = get_avaliable_models(candidates, 5)\n","        all_avaliable_model_ids = all_avaliable_models[\"local\"] + all_avaliable_models[\"huggingface\"]\n","        print(f\"avaliable models on {command['task']}: {all_avaliable_models}\")\n","        print(\"\")\n","\n","        if len(all_avaliable_model_ids) == 0:\n","            print(f\"no available models on {command['task']}\")\n","            print(\"\")\n","            inference_result = {\"error\": f\"no available models on {command['task']} task.\"}\n","            results[id] = collect_result(command, \"\", inference_result)\n","            return False\n","\n","        if len(all_avaliable_model_ids) == 1:\n","            best_model_id = all_avaliable_model_ids[0]\n","            hosted_on = \"local\" if best_model_id in all_avaliable_models[\"local\"] else \"huggingface\"\n","            reason = \"Only one model available.\"\n","            choose = {\"id\": best_model_id, \"reason\": reason}\n","            print(f\"chosen model: {choose}\")\n","            print(\"\")\n","        else:\n","            cand_models_info = [\n","                {\n","                    \"id\": model[\"id\"],\n","                    \"inference endpoint\": all_avaliable_models.get(\n","                        \"local\" if model[\"id\"] in all_avaliable_models[\"local\"] else \"huggingface\"\n","                    ),\n","                    \"likes\": model.get(\"likes\"),\n","                    \"description\": model.get(\"description\", \"\")[:100],\n","                    \"tags\": model.get(\"meta\").get(\"tags\") if model.get(\"meta\") else None,\n","                }\n","                for model in candidates\n","                if model[\"id\"] in all_avaliable_model_ids\n","            ]\n","\n","            choose_str = choose_model(input, command, cand_models_info, api_key, api_endpoint)\n","            print(f\"chosen model: {choose_str}\")\n","            print(\"\")\n","            try:\n","                choose = json.loads(choose_str)\n","                reason = choose[\"reason\"]\n","                best_model_id = choose[\"id\"]\n","                hosted_on = \"local\" if best_model_id in all_avaliable_models[\"local\"] else \"huggingface\"\n","            except Exception as e:\n","                print(f\"the response [ {choose_str} ] is not a valid JSON, try to find the model id and reason in the response.\")\n","                print(\"\")\n","                choose_str = find_json(choose_str)\n","                best_model_id, reason, choose  = get_id_reason(choose_str)\n","                hosted_on = \"local\" if best_model_id in all_avaliable_models[\"local\"] else \"huggingface\"\n","    print(\"Model we use and args we send:\")\n","    print(best_model_id)\n","    print(args)\n","    print(\"\")\n","    inference_result = model_inference(best_model_id, args, hosted_on, command['task'])\n","\n","    if \"error\" in inference_result:\n","        print(f\"Inference error: {inference_result['error']}\")\n","        print(\"\")\n","        results[id] = collect_result(command, choose, inference_result)\n","        return False\n","\n","    results[id] = collect_result(command, choose, inference_result)\n","    return True"],"metadata":{"id":"5lp6tyQg3kAu"},"id":"5lp6tyQg3kAu","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once we have our inference results, we ask the LLM agent to correctly answer back with what we requested initially."],"metadata":{"id":"XtVmuALNc72y"},"id":"XtVmuALNc72y"},{"cell_type":"code","source":["def response_results(input, results, api_key, api_endpoint):\n","    results = [v for k, v in sorted(results.items(), key=lambda item: item[0])]\n","    prompt = replace_slot(response_results_prompt, {\n","        \"input\": input,\n","    })\n","    print(\"Response prompt we will be sending:\")\n","    print(prompt)\n","    print(\"\")\n","    demos_or_presteps = replace_slot(response_results_demos_or_presteps, {\n","        \"input\": input,\n","        \"processes\": results\n","    })\n","    messages = json.loads(demos_or_presteps)\n","    messages.insert(0, {\"role\": \"system\", \"content\": response_results_tprompt})\n","    messages.append({\"role\": \"user\", \"content\": prompt})\n","    print(\"All messages we will be sending:\")\n","    print(messages)\n","    print(\"\")\n","    data = {\n","        \"model\": LLM,\n","        \"messages\": messages,\n","        \"temperature\": 0,\n","        \"api_key\": api_key,\n","        \"api_endpoint\": api_endpoint\n","    }\n","    return send_request(data)"],"metadata":{"id":"bhDq3UMZ3r6-"},"id":"bhDq3UMZ3r6-","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Main Execution"],"metadata":{"id":"qfAL6vKA4Zs6"},"id":"qfAL6vKA4Zs6"},{"cell_type":"markdown","source":["We create an array where we will be storing all our chat messages, as our history. We start by appending our prompt as our role \"user\"."],"metadata":{"id":"ft8oUI3OdPqv"},"id":"ft8oUI3OdPqv"},{"cell_type":"code","source":["all_messages = []\n","all_messages.append({\"role\":\"user\", \"content\":input_prompt})"],"metadata":{"id":"ure3aqlodIxW"},"id":"ure3aqlodIxW","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Our input prompt that we will be working with is the following:"],"metadata":{"id":"62t3DcCBdgGk"},"id":"62t3DcCBdgGk"},{"cell_type":"code","source":["print(input_prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LaZGfkqodI5I","executionInfo":{"status":"ok","timestamp":1713572949118,"user_tz":300,"elapsed":6,"user":{"displayName":"Kunal Gurnani","userId":"08187749967867283185"}},"outputId":"b87c5568-b62f-4a99-c502-84e060ca5a8f"},"id":"LaZGfkqodI5I","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["There is a picture of some pizza located in '/content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg' what topping does it have?\n"]}]},{"cell_type":"markdown","source":["Our parse task planning prompt."],"metadata":{"id":"vZqH5FQ8jUyf"},"id":"vZqH5FQ8jUyf"},{"cell_type":"code","source":["print(parse_task_tprompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OOzNsrrxjU9K","executionInfo":{"status":"ok","timestamp":1713572949118,"user_tz":300,"elapsed":5,"user":{"displayName":"Kunal Gurnani","userId":"08187749967867283185"}},"outputId":"72d2872d-79c3-4603-a8f7-065d74567459"},"id":"OOzNsrrxjU9K","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","#1 Task Planning Stage: The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\": task_id, \"dep\": dependency_task_id, \"args\": {\"text\": text or <GENERATED>-dep_id, \"image\": image_url or <GENERATED>-dep_id, \"audio\": audio_url or <GENERATED>-dep_id}}]. The special tag \"<GENERATED>-dep_id\" refer to the one generated text/image/audio in the dependency task (Please consider whether the dependency task generates resources of this type.) and \"dep_id\" must be in \"dep\" list. The \"dep\" field denotes the ids of the previous prerequisite tasks which generate a new resource that the current task relies on. The \"args\" field must in [\"text\", \"image\", \"audio\"], nothing else. The task MUST be selected from the following options: \"token-classification\", \"text2text-generation\", \"summarization\", \"translation\", \"question-answering\", \"conversational\", \"text-generation\", \"sentence-similarity\", \"tabular-classification\", \"object-detection\", \"image-classification\", \"image-to-image\", \"image-to-text\", \"text-to-image\", \"text-to-video\", \"visual-question-answering\", \"document-question-answering\", \"image-segmentation\", \"depth-estimation\", \"text-to-speech\", \"automatic-speech-recognition\", \"audio-to-audio\", \"audio-classification\", \"canny-control\", \"hed-control\", \"mlsd-control\", \"normal-control\", \"openpose-control\", \"canny-text-to-image\", \"depth-text-to-image\", \"hed-text-to-image\", \"mlsd-text-to-image\", \"normal-text-to-image\", \"openpose-text-to-image\", \"seg-text-to-image\". There may be multiple tasks of the same type. Think step by step about all the tasks needed to resolve the user's request. Parse out as few tasks as possible while ensuring that the user request can be resolved. Pay attention to the dependencies and order among tasks. If the user input can't be parsed, you need to reply empty JSON [].\n","\n"]}]},{"cell_type":"markdown","source":["We then start by reading our task prompt for the task planning stage and append it to the start of our \"parse task\" examples."],"metadata":{"id":"gtJl_SjDedB0"},"id":"gtJl_SjDedB0"},{"cell_type":"code","source":["messages = json.loads(parse_task_demos_or_presteps)\n","messages.insert(0, {\"role\": \"system\", \"content\": parse_task_tprompt})\n","print(messages[0])\n","print(messages[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cK-Tn3vUdI8x","executionInfo":{"status":"ok","timestamp":1713572949118,"user_tz":300,"elapsed":4,"user":{"displayName":"Kunal Gurnani","userId":"08187749967867283185"}},"outputId":"e3b5d2dd-8f93-4399-a267-be855573287d"},"id":"cK-Tn3vUdI8x","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'role': 'system', 'content': '\\n#1 Task Planning Stage: The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\": task_id, \"dep\": dependency_task_id, \"args\": {\"text\": text or <GENERATED>-dep_id, \"image\": image_url or <GENERATED>-dep_id, \"audio\": audio_url or <GENERATED>-dep_id}}]. The special tag \"<GENERATED>-dep_id\" refer to the one generated text/image/audio in the dependency task (Please consider whether the dependency task generates resources of this type.) and \"dep_id\" must be in \"dep\" list. The \"dep\" field denotes the ids of the previous prerequisite tasks which generate a new resource that the current task relies on. The \"args\" field must in [\"text\", \"image\", \"audio\"], nothing else. The task MUST be selected from the following options: \"token-classification\", \"text2text-generation\", \"summarization\", \"translation\", \"question-answering\", \"conversational\", \"text-generation\", \"sentence-similarity\", \"tabular-classification\", \"object-detection\", \"image-classification\", \"image-to-image\", \"image-to-text\", \"text-to-image\", \"text-to-video\", \"visual-question-answering\", \"document-question-answering\", \"image-segmentation\", \"depth-estimation\", \"text-to-speech\", \"automatic-speech-recognition\", \"audio-to-audio\", \"audio-classification\", \"canny-control\", \"hed-control\", \"mlsd-control\", \"normal-control\", \"openpose-control\", \"canny-text-to-image\", \"depth-text-to-image\", \"hed-text-to-image\", \"mlsd-text-to-image\", \"normal-text-to-image\", \"openpose-text-to-image\", \"seg-text-to-image\". There may be multiple tasks of the same type. Think step by step about all the tasks needed to resolve the user\\'s request. Parse out as few tasks as possible while ensuring that the user request can be resolved. Pay attention to the dependencies and order among tasks. If the user input can\\'t be parsed, you need to reply empty JSON [].\\n'}\n","{'role': 'user', 'content': 'Give you some pictures e1.jpg, e2.png, e3.jpg, help me count the number of sheep?'}\n"]}]},{"cell_type":"markdown","source":["Here we just separate any historical messages that may be in all_messages, but for this example, these two lines can be ignored, as we are only working with one example prompt."],"metadata":{"id":"z_bYBMMse7-U"},"id":"z_bYBMMse7-U"},{"cell_type":"code","source":["context = all_messages[:-1]\n","input = all_messages[-1][\"content\"]\n","print(context)\n","print(input)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Quj_hJyAdI_6","executionInfo":{"status":"ok","timestamp":1713572949118,"user_tz":300,"elapsed":4,"user":{"displayName":"Kunal Gurnani","userId":"08187749967867283185"}},"outputId":"88f5118b-fe1f-4e33-a687-475a094349ed"},"id":"Quj_hJyAdI_6","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[]\n","There is a picture of some pizza located in '/content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg' what topping does it have?\n"]}]},{"cell_type":"markdown","source":["Here we go through all our history of prompts and append them to our initial messages array with examples of prompts. We generate a history text with them. Notice there is no history in the chat log."],"metadata":{"id":"eUp4QBk_gQqT"},"id":"eUp4QBk_gQqT"},{"cell_type":"code","source":["start = 0\n","while start <= len(context):\n","    history = context[start:]\n","    prompt = replace_slot(parse_task_prompt, {\n","        \"input\": input,\n","        \"context\": history\n","    })\n","    print(\"Prompt to be added to array:\")\n","    print(prompt)\n","    messages.append({\"role\": \"user\", \"content\": prompt})\n","    history_text = \"<im_end>\\nuser<im_start>\".join([m[\"content\"] for m in messages])\n","    num = count_tokens(LLM_encoding, history_text)\n","    if get_max_context_length(LLM) - num > 800:\n","        break\n","    messages.pop()\n","    start += 2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9t_YZiWodJCw","executionInfo":{"status":"ok","timestamp":1713572949118,"user_tz":300,"elapsed":3,"user":{"displayName":"Kunal Gurnani","userId":"08187749967867283185"}},"outputId":"412fa839-af75-4879-cba6-3a595d2052b7"},"id":"9t_YZiWodJCw","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Prompt to be added to array:\n","\n","The chat log [ [] ] may contain the resources I mentioned. Now I input { There is a picture of some pizza located in '/content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg' what topping does it have? }. Pay attention to the input and output types of tasks and the dependencies between tasks.\n","\n"]}]},{"cell_type":"markdown","source":["We format our data to send it to OpenAI through the api. We can see all the messages we will be sending. Notice that the first one is our prompt that explains task planning and the last one is our prompt with the parse task template prompt."],"metadata":{"id":"nN1uEoKlhjG9"},"id":"nN1uEoKlhjG9"},{"cell_type":"code","source":["data = {\n","    \"model\": LLM,\n","    \"messages\": messages,\n","    \"temperature\": 0,\n","    \"logit_bias\": {item: 0.1 for item in task_parsing_highlight_ids},\n","    \"api_key\": OPENAI_KEY,\n","    \"api_endpoint\": openai_endpoint\n","}\n","print(messages[0])\n","print(messages[-1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S5NIP9aJdJF4","executionInfo":{"status":"ok","timestamp":1713572949118,"user_tz":300,"elapsed":2,"user":{"displayName":"Kunal Gurnani","userId":"08187749967867283185"}},"outputId":"875c35c2-b2b4-49fa-e46e-d3322c6d9483"},"id":"S5NIP9aJdJF4","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'role': 'system', 'content': '\\n#1 Task Planning Stage: The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\": task_id, \"dep\": dependency_task_id, \"args\": {\"text\": text or <GENERATED>-dep_id, \"image\": image_url or <GENERATED>-dep_id, \"audio\": audio_url or <GENERATED>-dep_id}}]. The special tag \"<GENERATED>-dep_id\" refer to the one generated text/image/audio in the dependency task (Please consider whether the dependency task generates resources of this type.) and \"dep_id\" must be in \"dep\" list. The \"dep\" field denotes the ids of the previous prerequisite tasks which generate a new resource that the current task relies on. The \"args\" field must in [\"text\", \"image\", \"audio\"], nothing else. The task MUST be selected from the following options: \"token-classification\", \"text2text-generation\", \"summarization\", \"translation\", \"question-answering\", \"conversational\", \"text-generation\", \"sentence-similarity\", \"tabular-classification\", \"object-detection\", \"image-classification\", \"image-to-image\", \"image-to-text\", \"text-to-image\", \"text-to-video\", \"visual-question-answering\", \"document-question-answering\", \"image-segmentation\", \"depth-estimation\", \"text-to-speech\", \"automatic-speech-recognition\", \"audio-to-audio\", \"audio-classification\", \"canny-control\", \"hed-control\", \"mlsd-control\", \"normal-control\", \"openpose-control\", \"canny-text-to-image\", \"depth-text-to-image\", \"hed-text-to-image\", \"mlsd-text-to-image\", \"normal-text-to-image\", \"openpose-text-to-image\", \"seg-text-to-image\". There may be multiple tasks of the same type. Think step by step about all the tasks needed to resolve the user\\'s request. Parse out as few tasks as possible while ensuring that the user request can be resolved. Pay attention to the dependencies and order among tasks. If the user input can\\'t be parsed, you need to reply empty JSON [].\\n'}\n","{'role': 'user', 'content': \"\\nThe chat log [ [] ] may contain the resources I mentioned. Now I input { There is a picture of some pizza located in '/content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg' what topping does it have? }. Pay attention to the input and output types of tasks and the dependencies between tasks.\\n\"}\n"]}]},{"cell_type":"markdown","source":["We send our data and get the tasks with the format we requested."],"metadata":{"id":"H0KYbUcfisUV"},"id":"H0KYbUcfisUV"},{"cell_type":"code","source":["tasks = json.loads(send_request(data))\n","for task in tasks:\n","    print(task)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jy02T7z_dJJy","executionInfo":{"status":"ok","timestamp":1713572954360,"user_tz":300,"elapsed":5244,"user":{"displayName":"Kunal Gurnani","userId":"08187749967867283185"}},"outputId":"aa5e4cf0-dc7d-4ab4-86a5-4acb09d9e878"},"id":"Jy02T7z_dJJy","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'task': 'image-to-text', 'id': 0, 'dep': [-1], 'args': {'image': '/content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg'}}\n","{'task': 'visual-question-answering', 'id': 1, 'dep': [0], 'args': {'image': '<GENERATED>-0', 'text': 'what topping does it have?'}}\n"]}]},{"cell_type":"code","source":["tasks = unfold(tasks)\n","tasks = fix_dep(tasks)\n","for task in tasks:\n","    print(task)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IoToD6t8dJMY","executionInfo":{"status":"ok","timestamp":1713572954360,"user_tz":300,"elapsed":19,"user":{"displayName":"Kunal Gurnani","userId":"08187749967867283185"}},"outputId":"1ccfe503-028b-4536-935e-87f358537d72"},"id":"IoToD6t8dJMY","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'task': 'image-to-text', 'id': 0, 'dep': [-1], 'args': {'image': '/content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg'}}\n","{'task': 'visual-question-answering', 'id': 1, 'dep': [0], 'args': {'image': '<GENERATED>-0', 'text': 'what topping does it have?'}}\n"]}]},{"cell_type":"markdown","source":["Here is where we start executing our tasks. We go through all our tasks, starting with the ones that have no dependencies (dep = -1). We run each task in a different thread. In the output section of this cell, you will see how it goes from parsing the tasks, to knowing which models are available, and most importantly the prompt it uses for model selection. Following that trail of outputs will give you a much better idea of how it reasons through the tasks."],"metadata":{"id":"WPTEqLYZkkA2"},"id":"WPTEqLYZkkA2"},{"cell_type":"code","source":["results = {}\n","threads = []\n","tasks = tasks[:]\n","d = dict()\n","retry = 0\n","while True:\n","    num_thread = len(threads)\n","    for task in tasks:\n","        for dep_id in task[\"dep\"]:\n","            if dep_id >= task[\"id\"]:\n","                task[\"dep\"] = [-1]\n","                break\n","        dep = task[\"dep\"]\n","        if dep[0] == -1 or len(list(set(dep).intersection(d.keys()))) == len(dep):\n","            tasks.remove(task)\n","            thread = threading.Thread(target=run_task, args=(input, task, d, OPENAI_KEY, openai_endpoint))\n","            thread.start()\n","            threads.append(thread)\n","    if num_thread == len(threads):\n","        time.sleep(0.5)\n","        retry += 1\n","    if retry > 160:\n","        break\n","    if len(tasks) == 0:\n","        break\n","for thread in threads:\n","    thread.join()\n","\n","results = d.copy()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DgKvYezvdJOv","executionInfo":{"status":"ok","timestamp":1713572977901,"user_tz":300,"elapsed":23559,"user":{"displayName":"Kunal Gurnani","userId":"08187749967867283185"}},"outputId":"c56b8535-54f1-4705-e3a8-2f496f59cfe5"},"id":"DgKvYezvdJOv","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Run task: 0 - image-to-text\n","\n","Deps: []\n","\n","parsed task: {'task': 'image-to-text', 'id': 0, 'dep': [-1], 'args': {'image': '/content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg'}}\n","\n","avaliable models on image-to-text: {'local': [], 'huggingface': ['Salesforce/blip-image-captioning-base', 'Salesforce/blip2-opt-2.7b', 'microsoft/trocr-base-handwritten', 'Salesforce/blip-image-captioning-large', 'nlpconnect/vit-gpt2-image-captioning']}\n","\n","Prompt we use to choose model given a list of candidates\n","Please choose the most suitable model from [{'id': 'nlpconnect/vit-gpt2-image-captioning', 'inference endpoint': ['Salesforce/blip-image-captioning-base', 'Salesforce/blip2-opt-2.7b', 'microsoft/trocr-base-handwritten', 'Salesforce/blip-image-captioning-large', 'nlpconnect/vit-gpt2-image-captioning'], 'likes': 219, 'description': '\\n\\n# nlpconnect/vit-gpt2-image-captioning\\n\\nThis is an image captioning model trained by @ydshieh in [', 'tags': ['image-to-text', 'image-captioning']}, {'id': 'Salesforce/blip-image-captioning-large', 'inference endpoint': ['Salesforce/blip-image-captioning-base', 'Salesforce/blip2-opt-2.7b', 'microsoft/trocr-base-handwritten', 'Salesforce/blip-image-captioning-large', 'nlpconnect/vit-gpt2-image-captioning'], 'likes': 52, 'description': '\\n\\n# BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Ge', 'tags': ['image-captioning']}, {'id': 'Salesforce/blip-image-captioning-base', 'inference endpoint': ['Salesforce/blip-image-captioning-base', 'Salesforce/blip2-opt-2.7b', 'microsoft/trocr-base-handwritten', 'Salesforce/blip-image-captioning-large', 'nlpconnect/vit-gpt2-image-captioning'], 'likes': 44, 'description': '\\n\\n# BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Ge', 'tags': ['image-captioning']}, {'id': 'microsoft/trocr-base-handwritten', 'inference endpoint': ['Salesforce/blip-image-captioning-base', 'Salesforce/blip2-opt-2.7b', 'microsoft/trocr-base-handwritten', 'Salesforce/blip-image-captioning-large', 'nlpconnect/vit-gpt2-image-captioning'], 'likes': 28, 'description': '\\n\\n# TrOCR (base-sized model, fine-tuned on IAM) \\n\\nTrOCR model fine-tuned on the [IAM dataset](https:', 'tags': ['trocr', 'image-to-text']}, {'id': 'Salesforce/blip2-opt-2.7b', 'inference endpoint': ['Salesforce/blip-image-captioning-base', 'Salesforce/blip2-opt-2.7b', 'microsoft/trocr-base-handwritten', 'Salesforce/blip-image-captioning-large', 'nlpconnect/vit-gpt2-image-captioning'], 'likes': 25, 'description': '\\n\\n# BLIP-2, OPT-2.7b, pre-trained only\\n\\nBLIP-2 model, leveraging [OPT-2.7b](https://huggingface.co/f', 'tags': ['vision', 'image-to-text', 'image-captioning', 'visual-question-answering']}] for the task {'task': 'image-to-text', 'id': 0, 'dep': [-1], 'args': {'image': '/content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg'}}. The output must be in a strict JSON format: {\"id\": \"id\", \"reason\": \"your detail reasons for the choice\"}.\n","\n","Promp with examples that we send:\n","[{'role': 'system', 'content': '#2 Model Selection Stage: Given the user request and the parsed tasks, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The assistant should focus more on the description of the model and find the model that has the most potential to solve requests and tasks. Also, prefer models with local inference endpoints for speed and stability.'}, {'role': 'user', 'content': \"There is a picture of some pizza located in '/content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg' what topping does it have?\"}, {'role': 'assistant', 'content': \"{'task': 'image-to-text', 'id': 0, 'dep': [-1], 'args': {'image': '/content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg'}}\"}, {'role': 'user', 'content': 'Please choose the most suitable model from [{\\'id\\': \\'nlpconnect/vit-gpt2-image-captioning\\', \\'inference endpoint\\': [\\'Salesforce/blip-image-captioning-base\\', \\'Salesforce/blip2-opt-2.7b\\', \\'microsoft/trocr-base-handwritten\\', \\'Salesforce/blip-image-captioning-large\\', \\'nlpconnect/vit-gpt2-image-captioning\\'], \\'likes\\': 219, \\'description\\': \\'\\\\n\\\\n# nlpconnect/vit-gpt2-image-captioning\\\\n\\\\nThis is an image captioning model trained by @ydshieh in [\\', \\'tags\\': [\\'image-to-text\\', \\'image-captioning\\']}, {\\'id\\': \\'Salesforce/blip-image-captioning-large\\', \\'inference endpoint\\': [\\'Salesforce/blip-image-captioning-base\\', \\'Salesforce/blip2-opt-2.7b\\', \\'microsoft/trocr-base-handwritten\\', \\'Salesforce/blip-image-captioning-large\\', \\'nlpconnect/vit-gpt2-image-captioning\\'], \\'likes\\': 52, \\'description\\': \\'\\\\n\\\\n# BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Ge\\', \\'tags\\': [\\'image-captioning\\']}, {\\'id\\': \\'Salesforce/blip-image-captioning-base\\', \\'inference endpoint\\': [\\'Salesforce/blip-image-captioning-base\\', \\'Salesforce/blip2-opt-2.7b\\', \\'microsoft/trocr-base-handwritten\\', \\'Salesforce/blip-image-captioning-large\\', \\'nlpconnect/vit-gpt2-image-captioning\\'], \\'likes\\': 44, \\'description\\': \\'\\\\n\\\\n# BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Ge\\', \\'tags\\': [\\'image-captioning\\']}, {\\'id\\': \\'microsoft/trocr-base-handwritten\\', \\'inference endpoint\\': [\\'Salesforce/blip-image-captioning-base\\', \\'Salesforce/blip2-opt-2.7b\\', \\'microsoft/trocr-base-handwritten\\', \\'Salesforce/blip-image-captioning-large\\', \\'nlpconnect/vit-gpt2-image-captioning\\'], \\'likes\\': 28, \\'description\\': \\'\\\\n\\\\n# TrOCR (base-sized model, fine-tuned on IAM) \\\\n\\\\nTrOCR model fine-tuned on the [IAM dataset](https:\\', \\'tags\\': [\\'trocr\\', \\'image-to-text\\']}, {\\'id\\': \\'Salesforce/blip2-opt-2.7b\\', \\'inference endpoint\\': [\\'Salesforce/blip-image-captioning-base\\', \\'Salesforce/blip2-opt-2.7b\\', \\'microsoft/trocr-base-handwritten\\', \\'Salesforce/blip-image-captioning-large\\', \\'nlpconnect/vit-gpt2-image-captioning\\'], \\'likes\\': 25, \\'description\\': \\'\\\\n\\\\n# BLIP-2, OPT-2.7b, pre-trained only\\\\n\\\\nBLIP-2 model, leveraging [OPT-2.7b](https://huggingface.co/f\\', \\'tags\\': [\\'vision\\', \\'image-to-text\\', \\'image-captioning\\', \\'visual-question-answering\\']}] for the task {\\'task\\': \\'image-to-text\\', \\'id\\': 0, \\'dep\\': [-1], \\'args\\': {\\'image\\': \\'/content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg\\'}}. The output must be in a strict JSON format: {\"id\": \"id\", \"reason\": \"your detail reasons for the choice\"}.'}]\n","\n","chosen model: {\"id\": \"nlpconnect/vit-gpt2-image-captioning\", \"reason\": \"The 'nlpconnect/vit-gpt2-image-captioning' model is specifically designed for image captioning tasks, which is exactly what we need for this task. It has a high number of likes indicating its popularity and effectiveness. Moreover, it has a local inference endpoint which ensures speed and stability.\"}\n","\n","Model we use and args we send:\n","nlpconnect/vit-gpt2-image-captioning\n","{'image': '/content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg'}\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n","  warnings.warn(warning_message, FutureWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Run task: 1 - visual-question-answering\n","\n","Deps: [{\"task\": {\"task\": \"image-to-text\", \"id\": 0, \"dep\": [-1], \"args\": {\"image\": \"/content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg\"}}, \"inference result\": {\"generated text\": \"a pepperoni pizza on a wooden table \"}, \"choose model result\": {\"id\": \"nlpconnect/vit-gpt2-image-captioning\", \"reason\": \"The 'nlpconnect/vit-gpt2-image-captioning' model is specifically designed for image captioning tasks, which is exactly what we need for this task. It has a high number of likes indicating its popularity and effectiveness. Moreover, it has a local inference endpoint which ensures speed and stability.\"}}]\n","\n","Detect the generated text of dependency task (from results):a pepperoni pizza on a wooden table \n","\n","Detect the image of dependency task (from args): /content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg\n","\n","parsed task: {'task': 'visual-question-answering', 'id': 1, 'dep': [0], 'args': {'image': '/content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg', 'text': 'what topping does it have?'}}\n","\n","avaliable models on visual-question-answering: {'local': [], 'huggingface': ['dandelin/vilt-b32-finetuned-vqa', 'microsoft/git-large-textvqa', 'microsoft/git-base-vqav2', 'microsoft/git-large-vqav2', 'tufa15nik/vilt-finetuned-vqasi']}\n","\n","Prompt we use to choose model given a list of candidates\n","Please choose the most suitable model from [{'id': 'dandelin/vilt-b32-finetuned-vqa', 'inference endpoint': ['dandelin/vilt-b32-finetuned-vqa', 'microsoft/git-large-textvqa', 'microsoft/git-base-vqav2', 'microsoft/git-large-vqav2', 'tufa15nik/vilt-finetuned-vqasi'], 'likes': 86, 'description': '\\n\\n# Vision-and-Language Transformer (ViLT), fine-tuned on VQAv2\\n\\nVision-and-Language Transformer (Vi', 'tags': ['visual-question-answering']}, {'id': 'microsoft/git-large-vqav2', 'inference endpoint': ['dandelin/vilt-b32-finetuned-vqa', 'microsoft/git-large-textvqa', 'microsoft/git-base-vqav2', 'microsoft/git-large-vqav2', 'tufa15nik/vilt-finetuned-vqasi'], 'likes': 3, 'description': '\\n\\n# GIT (GenerativeImage2Text), large-sized, fine-tuned on VQAv2\\n\\nGIT (short for GenerativeImage2Tex', 'tags': ['vision']}, {'id': 'microsoft/git-base-vqav2', 'inference endpoint': ['dandelin/vilt-b32-finetuned-vqa', 'microsoft/git-large-textvqa', 'microsoft/git-base-vqav2', 'microsoft/git-large-vqav2', 'tufa15nik/vilt-finetuned-vqasi'], 'likes': 1, 'description': '\\n\\n# GIT (GenerativeImage2Text), base-sized, fine-tuned on VQAv2\\n\\nGIT (short for GenerativeImage2Text', 'tags': ['vision']}, {'id': 'microsoft/git-large-textvqa', 'inference endpoint': ['dandelin/vilt-b32-finetuned-vqa', 'microsoft/git-large-textvqa', 'microsoft/git-base-vqav2', 'microsoft/git-large-vqav2', 'tufa15nik/vilt-finetuned-vqasi'], 'likes': 1, 'description': '\\n\\n# GIT (GenerativeImage2Text), large-sized, fine-tuned on TextVQA\\n\\nGIT (short for GenerativeImage2T', 'tags': ['vision']}, {'id': 'tufa15nik/vilt-finetuned-vqasi', 'inference endpoint': ['dandelin/vilt-b32-finetuned-vqa', 'microsoft/git-large-textvqa', 'microsoft/git-base-vqav2', 'microsoft/git-large-vqav2', 'tufa15nik/vilt-finetuned-vqasi'], 'likes': 0, 'description': 'Entry not found', 'tags': None}] for the task {'task': 'visual-question-answering', 'id': 1, 'dep': [0], 'args': {'image': '/content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg', 'text': 'what topping does it have?'}}. The output must be in a strict JSON format: {\"id\": \"id\", \"reason\": \"your detail reasons for the choice\"}.\n","\n","Promp with examples that we send:\n","[{'role': 'system', 'content': '#2 Model Selection Stage: Given the user request and the parsed tasks, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The assistant should focus more on the description of the model and find the model that has the most potential to solve requests and tasks. Also, prefer models with local inference endpoints for speed and stability.'}, {'role': 'user', 'content': \"There is a picture of some pizza located in '/content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg' what topping does it have?\"}, {'role': 'assistant', 'content': \"{'task': 'visual-question-answering', 'id': 1, 'dep': [0], 'args': {'image': '/content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg', 'text': 'what topping does it have?'}}\"}, {'role': 'user', 'content': 'Please choose the most suitable model from [{\\'id\\': \\'dandelin/vilt-b32-finetuned-vqa\\', \\'inference endpoint\\': [\\'dandelin/vilt-b32-finetuned-vqa\\', \\'microsoft/git-large-textvqa\\', \\'microsoft/git-base-vqav2\\', \\'microsoft/git-large-vqav2\\', \\'tufa15nik/vilt-finetuned-vqasi\\'], \\'likes\\': 86, \\'description\\': \\'\\\\n\\\\n# Vision-and-Language Transformer (ViLT), fine-tuned on VQAv2\\\\n\\\\nVision-and-Language Transformer (Vi\\', \\'tags\\': [\\'visual-question-answering\\']}, {\\'id\\': \\'microsoft/git-large-vqav2\\', \\'inference endpoint\\': [\\'dandelin/vilt-b32-finetuned-vqa\\', \\'microsoft/git-large-textvqa\\', \\'microsoft/git-base-vqav2\\', \\'microsoft/git-large-vqav2\\', \\'tufa15nik/vilt-finetuned-vqasi\\'], \\'likes\\': 3, \\'description\\': \\'\\\\n\\\\n# GIT (GenerativeImage2Text), large-sized, fine-tuned on VQAv2\\\\n\\\\nGIT (short for GenerativeImage2Tex\\', \\'tags\\': [\\'vision\\']}, {\\'id\\': \\'microsoft/git-base-vqav2\\', \\'inference endpoint\\': [\\'dandelin/vilt-b32-finetuned-vqa\\', \\'microsoft/git-large-textvqa\\', \\'microsoft/git-base-vqav2\\', \\'microsoft/git-large-vqav2\\', \\'tufa15nik/vilt-finetuned-vqasi\\'], \\'likes\\': 1, \\'description\\': \\'\\\\n\\\\n# GIT (GenerativeImage2Text), base-sized, fine-tuned on VQAv2\\\\n\\\\nGIT (short for GenerativeImage2Text\\', \\'tags\\': [\\'vision\\']}, {\\'id\\': \\'microsoft/git-large-textvqa\\', \\'inference endpoint\\': [\\'dandelin/vilt-b32-finetuned-vqa\\', \\'microsoft/git-large-textvqa\\', \\'microsoft/git-base-vqav2\\', \\'microsoft/git-large-vqav2\\', \\'tufa15nik/vilt-finetuned-vqasi\\'], \\'likes\\': 1, \\'description\\': \\'\\\\n\\\\n# GIT (GenerativeImage2Text), large-sized, fine-tuned on TextVQA\\\\n\\\\nGIT (short for GenerativeImage2T\\', \\'tags\\': [\\'vision\\']}, {\\'id\\': \\'tufa15nik/vilt-finetuned-vqasi\\', \\'inference endpoint\\': [\\'dandelin/vilt-b32-finetuned-vqa\\', \\'microsoft/git-large-textvqa\\', \\'microsoft/git-base-vqav2\\', \\'microsoft/git-large-vqav2\\', \\'tufa15nik/vilt-finetuned-vqasi\\'], \\'likes\\': 0, \\'description\\': \\'Entry not found\\', \\'tags\\': None}] for the task {\\'task\\': \\'visual-question-answering\\', \\'id\\': 1, \\'dep\\': [0], \\'args\\': {\\'image\\': \\'/content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg\\', \\'text\\': \\'what topping does it have?\\'}}. The output must be in a strict JSON format: {\"id\": \"id\", \"reason\": \"your detail reasons for the choice\"}.'}]\n","\n","chosen model: {\"id\": \"dandelin/vilt-b32-finetuned-vqa\", \"reason\": \"The 'dandelin/vilt-b32-finetuned-vqa' model is a Vision-and-Language Transformer (ViLT) that has been fine-tuned on the VQAv2 dataset, which is specifically designed for visual question answering tasks. This makes it highly suitable for the task at hand, which involves identifying pizza toppings from an image. Additionally, this model has the highest number of likes, indicating a higher level of community validation. It also has a local inference endpoint, which ensures speed and stability.\"}\n","\n","Model we use and args we send:\n","dandelin/vilt-b32-finetuned-vqa\n","{'image': '/content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg', 'text': 'what topping does it have?'}\n","\n"]}]},{"cell_type":"markdown","source":["Here we can see the results of the inference, all combined together."],"metadata":{"id":"a0SXg-FItrXT"},"id":"a0SXg-FItrXT"},{"cell_type":"code","source":["print(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zdeC120RpGRc","executionInfo":{"status":"ok","timestamp":1713572977901,"user_tz":300,"elapsed":20,"user":{"displayName":"Kunal Gurnani","userId":"08187749967867283185"}},"outputId":"c9628c67-11df-452f-eb86-75de40e9c7a0"},"id":"zdeC120RpGRc","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: {'task': {'task': 'image-to-text', 'id': 0, 'dep': [-1], 'args': {'image': '/content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg'}}, 'inference result': {'generated text': 'a pepperoni pizza on a wooden table '}, 'choose model result': {'id': 'nlpconnect/vit-gpt2-image-captioning', 'reason': \"The 'nlpconnect/vit-gpt2-image-captioning' model is specifically designed for image captioning tasks, which is exactly what we need for this task. It has a high number of likes indicating its popularity and effectiveness. Moreover, it has a local inference endpoint which ensures speed and stability.\"}}, 1: {'task': {'task': 'visual-question-answering', 'id': 1, 'dep': [0], 'args': {'image': '/content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg', 'text': 'what topping does it have?'}}, 'inference result': [{'score': 0.9694024920463562, 'answer': 'pepperoni'}, {'score': 0.1693362146615982, 'answer': 'cheese'}, {'score': 0.009121456183493137, 'answer': 'tomato'}, {'score': 0.0062710316851735115, 'answer': 'sausage'}, {'score': 0.0025005571078509092, 'answer': 'pizza'}], 'choose model result': {'id': 'dandelin/vilt-b32-finetuned-vqa', 'reason': \"The 'dandelin/vilt-b32-finetuned-vqa' model is a Vision-and-Language Transformer (ViLT) that has been fine-tuned on the VQAv2 dataset, which is specifically designed for visual question answering tasks. This makes it highly suitable for the task at hand, which involves identifying pizza toppings from an image. Additionally, this model has the highest number of likes, indicating a higher level of community validation. It also has a local inference endpoint, which ensures speed and stability.\"}}}\n"]}]},{"cell_type":"markdown","source":["Now finally we create another prompt with the same techniques for getting a final result back from our LLM agent."],"metadata":{"id":"BGFStZOiuLeE"},"id":"BGFStZOiuLeE"},{"cell_type":"code","source":["response = response_results(input, results, OPENAI_KEY, openai_endpoint).strip()"],"metadata":{"id":"bvtPKjodLlTf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713573000239,"user_tz":300,"elapsed":22340,"user":{"displayName":"Kunal Gurnani","userId":"08187749967867283185"}},"outputId":"cb3f9706-6b9e-46f9-c803-31c729877827"},"id":"bvtPKjodLlTf","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Response prompt we will be sending:\n","Yes. Please first think carefully and directly answer my request based on the inference results. Some of the inferences may not always turn out to be correct and require you to make careful consideration in making decisions. Then please detail your workflow including the used models and inference results for my request in your friendly tone. Please filter out information that is not relevant to my request. Tell me the complete path or urls of files in inference results. If there is nothing in the results, please tell me you can't make it. }\n","\n","All messages we will be sending:\n","[{'role': 'system', 'content': '#4 Response Generation Stage: With the task execution logs, the AI assistant needs to describe the process and inference results.'}, {'role': 'user', 'content': \"There is a picture of some pizza located in '/content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg' what topping does it have?\"}, {'role': 'assistant', 'content': \"Before give you a response, I want to introduce my workflow for your request, which is shown in the following JSON data: [{'task': {'task': 'image-to-text', 'id': 0, 'dep': [-1], 'args': {'image': '/content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg'}}, 'inference result': {'generated text': 'a pepperoni pizza on a wooden table '}, 'choose model result': {'id': 'nlpconnect/vit-gpt2-image-captioning', 'reason': 'The 'nlpconnect/vit-gpt2-image-captioning' model is specifically designed for image captioning tasks, which is exactly what we need for this task. It has a high number of likes indicating its popularity and effectiveness. Moreover, it has a local inference endpoint which ensures speed and stability.'}}, {'task': {'task': 'visual-question-answering', 'id': 1, 'dep': [0], 'args': {'image': '/content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg', 'text': 'what topping does it have?'}}, 'inference result': [{'score': 0.9694024920463562, 'answer': 'pepperoni'}, {'score': 0.1693362146615982, 'answer': 'cheese'}, {'score': 0.009121456183493137, 'answer': 'tomato'}, {'score': 0.0062710316851735115, 'answer': 'sausage'}, {'score': 0.0025005571078509092, 'answer': 'pizza'}], 'choose model result': {'id': 'dandelin/vilt-b32-finetuned-vqa', 'reason': 'The 'dandelin/vilt-b32-finetuned-vqa' model is a Vision-and-Language Transformer (ViLT) that has been fine-tuned on the VQAv2 dataset, which is specifically designed for visual question answering tasks. This makes it highly suitable for the task at hand, which involves identifying pizza toppings from an image. Additionally, this model has the highest number of likes, indicating a higher level of community validation. It also has a local inference endpoint, which ensures speed and stability.'}}]. Do you have any demands regarding my response?\"}, {'role': 'user', 'content': \"Yes. Please first think carefully and directly answer my request based on the inference results. Some of the inferences may not always turn out to be correct and require you to make careful consideration in making decisions. Then please detail your workflow including the used models and inference results for my request in your friendly tone. Please filter out information that is not relevant to my request. Tell me the complete path or urls of files in inference results. If there is nothing in the results, please tell me you can't make it. }\"}]\n","\n"]}]},{"cell_type":"markdown","source":["Our final response from our LLM agent. As it says, it indeed is a pizza with pepperoni."],"metadata":{"id":"lHWOU9aTuYLT"},"id":"lHWOU9aTuYLT"},{"cell_type":"code","source":["print(response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xG3uaYohtJTY","executionInfo":{"status":"ok","timestamp":1713573000239,"user_tz":300,"elapsed":2,"user":{"displayName":"Kunal Gurnani","userId":"08187749967867283185"}},"outputId":"1e821cf1-0cf3-462e-bc8d-c20856992975"},"id":"xG3uaYohtJTY","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sure, based on the inference results, the pizza in the image located at '/content/drive/MyDrive/Colab Notebooks/HuggingGPT/food.jpeg' appears to have pepperoni as a topping.\n","\n","Here's a detailed explanation of how I arrived at this conclusion:\n","\n","1. First, I used the 'nlpconnect/vit-gpt2-image-captioning' model to generate a description of the image. This model is specifically designed for image captioning tasks and is popular and effective. The model described the image as 'a pepperoni pizza on a wooden table'.\n","\n","2. Next, to answer your specific question about the pizza topping, I used the 'dandelin/vilt-b32-finetuned-vqa' model. This model is a Vision-and-Language Transformer (ViLT) that has been fine-tuned on the VQAv2 dataset, making it highly suitable for visual question answering tasks. When asked 'what topping does it have?', the model provided several potential answers with corresponding confidence scores. The highest score was for 'pepperoni' (0.969), followed by 'cheese' (0.169), 'tomato' (0.009), 'sausage' (0.006), and 'pizza' (0.002).\n","\n","Based on these results, I concluded that the pizza in the image has pepperoni as a topping. Please note that while these models are highly effective, they may not always be 100% accurate.\n"]}]},{"cell_type":"markdown","source":["# Conclusion and Future Direction"],"metadata":{"id":"Q2xdEnSCLVr0"},"id":"Q2xdEnSCLVr0"},{"cell_type":"markdown","source":["HuggingGPT has established itself as a very efficient and successful controller for more complex tasks and modalities of information even when compared to counterparts like Alpaca-7b. The research in our respective paper and experimentation indicates that the future holds great promise for models like GPT-3.5 and GPT-4 to be integrated into a wider system to fulfill more intricate and expansive use-cases.\n","\n","LLMs in combination with any set of well-engineered models can now be used to tackle tasks that in the past would be too difficult for any of the AI capabilities that existed before. The use of Hugging Face and HuggingGPT as the respective source of models and controller pushes forward the potential scability of what already exists today. AI is moving from being an assistive tool, to being able to see a project or goal from start to end.\n","\n","When LLMs are exposed to a network as vast as HuggingFace and are given a well-engineered controller like HuggingGPT to integrate these models together under one combined goal and directive (what we were also able to implement), the sky is the limit.\n","\n","Some limitations that were identified have to do mainly with restrictions that aren't specific to HuggingGPT but rather to LLMs as a whole. For example, limits to the token length and the max efficiency of the LLM that is being employed can pose as a ceiling for what the automonous AI system is able to achieve altogether. These two are not really able to be addressed and so it is just with the passage of time that HuggingGPT may be able to employ more powerful LLMs.\n","\n","Finally, a more potent limitation to HuggingGPT is that there are undoubtedly gaps in efficiency caused by the inherent structure of the model's complexity in integrating multiple touchpoints for any given LLMs. This is something that is more worth addressing since it points out the opportunity for a more streamlined implementation to make things seamless from a network point of view.\n","\n"],"metadata":{"id":"AJPxJ70wxySn"},"id":"AJPxJ70wxySn"},{"cell_type":"markdown","source":["# References\n","\n","[1]:  Alayrac, Jean-Baptiste, et al. \"Flamingo: a visual language model for few-shot learning.\" Advances in neural information processing systems 35 (2022): 23716-23736.\n","\n","[2]:  Huang, Shaohan, et al. \"Language is not all you need: Aligning perception with language models.\" Advances in Neural Information Processing Systems 36 (2024).\n","\n","[3]: Shen, Yongliang, et al. \"Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face.\" Advances in Neural Information Processing Systems 36 (2024)."],"metadata":{"id":"AdFzFOh706pO"},"id":"AdFzFOh706pO"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}